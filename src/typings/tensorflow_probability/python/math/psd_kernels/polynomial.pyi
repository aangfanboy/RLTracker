"""
This type stub file was generated by pyright.
"""

from tensorflow_probability.python.math.psd_kernels import positive_semidefinite_kernel as psd_kernel

"""Polynomial and Linear kernel."""
__all__ = ['Constant', 'Linear', 'Polynomial']
class Polynomial(psd_kernel.AutoCompositeTensorPsdKernel):
  """Polynomial Kernel.

    Is based on the dot product covariance function and can be obtained
    from polynomial regression. This kernel, when parameterizing a
    Gaussian Process, results in random polynomial functions.
    A linear kernel can be created from this by setting the exponent to 1
    or None.

    ```none
    k(x, y) = bias_amplitude**2 + slope_amplitude**2 *
              ((x - shift) dot (y - shift))**exponent
    ```

    #### References

    [1]: Carl Edward Rasmussen and Christopher K. I. Williams. Gaussian
         Processes for Machine Learning. Section 4.4.2. 2006.
         http://www.gaussianprocess.org/gpml/chapters/RW4.pdf
    [2]: David Duvenaud. The Kernel Cookbook.
         https://www.cs.toronto.edu/~duvenaud/cookbook/

  """
  def __init__(self, bias_amplitude=..., slope_amplitude=..., shift=..., exponent=..., feature_ndims=..., validate_args=..., parameters=..., name=...) -> None:
    """Construct a Polynomial kernel instance.

    Args:
      bias_amplitude: Non-negative floating point `Tensor` that controls the
        stddev from the origin. If bias = 0, there is no stddev and the
        fitted function goes through the origin.  Must be broadcastable with
        `slope_amplitude`, `shift`, `exponent`, and inputs to `apply` and
        `matrix` methods. A value of `None` is treated like 0.
        Default Value: `None`
      slope_amplitude: Non-negative floating point `Tensor` that controls the
        stddev of the regression line slope that is the basis for the
        polynomial. Must be broadcastable with `bias_amplitude`, `shift`,
        `exponent`, and inputs to `apply` and `matrix` methods. A value of
        `None` is treated like 1.
        Default Value: `None`
      shift: Floating point `Tensor` that contols the intercept with the x-axis
        of the linear function to be exponentiated to get this polynomial. Must
        be broadcastable with `bias_amplitude`, `slope_amplitude`, `exponent`
        and inputs to `apply` and `matrix` methods. A value of `None` is treated
        like 0, which results in having the intercept at the origin.
        Default Value: `None`
      exponent: Positive floating point `Tensor` that controls the exponent
        (also known as the degree) of the polynomial function, and must be an
        integer.
        Must be broadcastable with `bias_amplitude`, `slope_amplitude`, `shift`,
        and inputs to `apply` and `matrix` methods. A value of `None` is treated
        like 1, which results in a linear kernel.
        Default Value: `None`
      feature_ndims: Python `int` number of rightmost dims to include in kernel
        computation.
        Default Value: 1
      validate_args: If `True`, parameters are checked for validity despite
        possibly degrading runtime performance.
        Default Value: `False`
      parameters: For subclasses, a dict of constructor arguments.
      name: Python `str` name prefixed to Ops created by this class.
        Default Value: `'Polynomial'`
    """
    ...
  
  @property
  def bias_amplitude(self):
    """Stddev on bias parameter."""
    ...
  
  @property
  def slope_amplitude(self):
    """Amplitude on slope parameter."""
    ...
  
  @property
  def shift(self):
    """Shift of linear function that is exponentiated."""
    ...
  
  @property
  def exponent(self):
    """Exponent of the polynomial term."""
    ...
  


class Linear(Polynomial):
  """Linear Kernel.

    Is based on the dot product covariance function and can be obtained
    from linear regression. This kernel, when parameterizing a
    Gaussian Process, results in random linear functions.
    The Linear kernel is based on the Polynomial kernel without the
    exponent.

    ```none
    k(x, y) = bias_amplitude**2 + slope_amplitude**2 *
              ((x - shift) dot (y - shift))
    ```
  """
  def __init__(self, bias_amplitude=..., slope_amplitude=..., shift=..., feature_ndims=..., validate_args=..., parameters=..., name=...) -> None:
    """Construct a Linear kernel instance.

    Args:
      bias_amplitude: Non-negative floating point `Tensor` that controls the
        stddev from the origin. If bias = 0, there is no stddev and the
        fitted function goes through the origin.  Must be broadcastable with
        `slope_amplitude`, `shift`, `exponent`, and inputs to `apply` and
        `matrix` methods. A value of `None` is treated like 0.
        Default Value: `None`
      slope_amplitude: Non-negative floating point `Tensor` that controls the
        stddev of the regression line slope that is the basis for the
        polynomial. Must be broadcastable with `bias_amplitude`, `shift`,
        `exponent`, and inputs to `apply` and `matrix` methods. A value of
        `None` is treated like 1.
        Default Value: `None`
      shift: Floating point `Tensor` that controls the intercept with the x-axis
        of the linear interpolation. Must be broadcastable with
        `bias_amplitude`, `slope_amplitude`, and inputs to `apply` and `matrix`
        methods. A value of `None` is treated like 0, which results in having
        the intercept at the origin.
      feature_ndims: Python `int` number of rightmost dims to include in kernel
        computation.
        Default Value: 1
      validate_args: If `True`, parameters are checked for validity despite
        possibly degrading runtime performance.
        Default Value: `False`
      parameters: For subclasses, a dict of constructor arguments.
      name: Python `str` name prefixed to Ops created by this class.
        Default Value: `'Linear'`
    """
    ...
  


class Constant(Linear):
  """Kernel that just outputs positive constant values.

  Useful class for multiplying / adding constants with other kernels.

  Warning: This can potentially lead to poorly conditioned matrices, since
  if the constant is large, adding it to each entry of another matrix will
  make it closer to an ill-conditioned matrix. If using a `Constant` kernel
  as a summand for a composite kernel in a `GaussianProcess`, it's instead
  recommended to use a trainable mean instead.
  """
  def __init__(self, constant, feature_ndims=..., validate_args=..., name=...) -> None:
    """Construct a constant kernel instance.

    Args:
      constant: Positive floating point `Tensor` (or convertible) that is used
        for all kernel entries.
      feature_ndims: Python `int` number of rightmost dims to include in kernel
        computation.
      validate_args: If `True`, parameters are checked for validity despite
        possibly degrading runtime performance
      name: Python `str` name prefixed to Ops created by this class.
    """
    ...
  
  @property
  def constant(self):
    ...
  


