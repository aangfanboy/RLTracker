"""
This type stub file was generated by pyright.
"""

"""Functions for common linear algebra operations.

Note: Many of these functions will eventually be migrated to core TensorFlow.
"""
__all__ = ['cholesky_concat', 'cholesky_update', 'fill_triangular', 'fill_triangular_inverse', 'hpsd_logdet', 'hpsd_quadratic_form_solve', 'hpsd_quadratic_form_solvevec', 'hpsd_solve', 'hpsd_solvevec', 'lu_matrix_inverse', 'lu_reconstruct', 'lu_reconstruct_assertions', 'lu_solve', 'low_rank_cholesky', 'pivoted_cholesky', 'sparse_or_dense_matmul', 'sparse_or_dense_matvecmul']
def cholesky_concat(chol, cols, name=...):
  """Concatenates `chol @ chol.T` with additional rows and columns.

  This operation is conceptually identical to:
  ```python
  def cholesky_concat_slow(chol, cols):  # cols shaped (n + m) x m = z x m
    mat = tf.matmul(chol, chol, adjoint_b=True)  # shape of n x n
    # Concat columns.
    mat = tf.concat([mat, cols[..., :tf.shape(mat)[-2], :]], axis=-1)  # n x z
    # Concat rows.
    mat = tf.concat([mat, tf.linalg.matrix_transpose(cols)], axis=-2)  # z x z
    return tf.linalg.cholesky(mat)
  ```
  but whereas `cholesky_concat_slow` would cost `O(z**3)` work,
  `cholesky_concat` only costs `O(z**2 + m**3)` work.

  The resulting (implicit) matrix must be symmetric and positive definite.
  Thus, the bottom right `m x m` must be self-adjoint, and we do not require a
  separate `rows` argument (which can be inferred from `conj(cols.T)`).

  Args:
    chol: Cholesky decomposition of `mat = chol @ chol.T`.
    cols: The new columns whose first `n` rows we would like concatenated to the
      right of `mat = chol @ chol.T`, and whose conjugate transpose we would
      like concatenated to the bottom of `concat(mat, cols[:n,:])`. A `Tensor`
      with final dims `(n+m, m)`. The first `n` rows are the top right rectangle
      (their conjugate transpose forms the bottom left), and the bottom `m x m`
      is self-adjoint.
    name: Optional name for this op.

  Returns:
    chol_concat: The Cholesky decomposition of:
      ```
      [ [ mat  cols[:n, :] ]
        [   conj(cols.T)   ] ]
      ```
  """
  ...

def cholesky_update(chol, update_vector, multiplier=..., name=...):
  """Returns cholesky of chol @ chol.T + multiplier * u @ u.T.

  Given a (batch of) lower triangular cholesky factor(s) `chol`, along with a
  (batch of) vector(s) `update_vector`, compute the lower triangular cholesky
  factor of the rank-1 update `chol @ chol.T + multiplier * u @ u.T`, where
  `multiplier` is a (batch of) scalar(s).

  If `chol` has shape `[L, L]`, this has complexity `O(L^2)` compared to the
  naive algorithm which has complexity `O(L^3)`.

  Args:
    chol: Floating-point `Tensor` with shape `[B1, ..., Bn, L, L]`.
      Cholesky decomposition of `mat = chol @ chol.T`. Batch dimensions
      must be broadcastable with `update_vector` and `multiplier`.
    update_vector: Floating-point `Tensor` with shape `[B1, ... Bn, L]`. Vector
      defining rank-one update. Batch dimensions must be broadcastable with
      `chol` and `multiplier`.
    multiplier: Floating-point `Tensor` with shape `[B1, ..., Bn]. Scalar
      multiplier to rank-one update. Batch dimensions must be broadcastable
      with `chol` and `update_vector`. Note that updates where `multiplier` is
      positive are numerically stable, while when `multiplier` is negative
      (downdating), the update will only work if the new resulting matrix is
      still positive definite.
    name: Optional name for this op.

  #### References
  [1] Oswin Krause. Christian Igel. A More Efficient Rank-one Covariance
      Matrix Update for Evolution Strategies. 2015 ACM Conference.
      https://www.researchgate.net/publication/300581419_A_More_Efficient_Rank-one_Covariance_Matrix_Update_for_Evolution_Strategies
  """
  ...

def pivoted_cholesky(matrix, max_rank, diag_rtol=..., return_pivoting_order=..., name=...): # -> tuple[Any, Any]:
  """Computes the (partial) pivoted cholesky decomposition of `matrix`.

  The pivoted Cholesky is a low rank approximation of the Cholesky decomposition
  of `matrix`, i.e. as described in [(Harbrecht et al., 2012)][1]. The
  currently-worst-approximated diagonal element is selected as the pivot at each
  iteration. This yields from a `[B1...Bn, N, N]` shaped `matrix` a `[B1...Bn,
  N, K]` shaped rank-`K` approximation `lr` such that `lr @ lr.T ~= matrix`.
  Note that, unlike the Cholesky decomposition, `lr` is not triangular even in
  a rectangular-matrix sense. However, under a permutation it could be made
  triangular (it has one more zero in each column as you move to the right).

  Such a matrix can be useful as a preconditioner for conjugate gradient
  optimization, i.e. as in [(Wang et al. 2019)][2], as matmuls and solves can be
  cheaply done via the Woodbury matrix identity, as implemented by
  `tf.linalg.LinearOperatorLowRankUpdate`.

  Args:
    matrix: Floating point `Tensor` batch of symmetric, positive definite
      matrices.
    max_rank: Scalar `int` `Tensor`, the rank at which to truncate the
      approximation.
    diag_rtol: Scalar floating point `Tensor` (same dtype as `matrix`). If the
      errors of all diagonal elements of `lr @ lr.T` are each lower than
      `element * diag_rtol`, iteration is permitted to terminate early.
    return_pivoting_order: If `True`, return an `int` `Tensor` indicating the 
      pivoting order used to produce `lr` (in addition to `lr`).
    name: Optional name for the op.

  Returns:
    lr: Low rank pivoted Cholesky approximation of `matrix`.
    perm: (Optional) pivoting order used to produce `lr`.

  #### References

  [1]: H Harbrecht, M Peters, R Schneider. On the low-rank approximation by the
       pivoted Cholesky decomposition. _Applied numerical mathematics_,
       62(4):428-440, 2012.

  [2]: K. A. Wang et al. Exact Gaussian Processes on a Million Data Points.
       _arXiv preprint arXiv:1903.08114_, 2019. https://arxiv.org/abs/1903.08114
  """
  ...

def low_rank_cholesky(matrix, max_rank, trace_atol=..., trace_rtol=..., name=...): # -> tuple[Any, Any, Any]:
  """Computes a low-rank approximation to the Cholesky decomposition.

  This routine is similar to pivoted_cholesky, but works under JAX, at
  the cost of being slightly less numerically stable.

  Args:
    matrix: Floating point `Tensor` batch of symmetric, positive definite
      matrices, or a tf.linalg.LinearOperator.
    max_rank: Scalar `int` `Tensor`, the rank at which to truncate the
      approximation.
    trace_atol: Scalar floating point `Tensor` (same dtype as `matrix`). If
      trace_atol > 0 and trace(matrix - LR * LR^t) < trace_atol, the output
      LR matrix is allowed to be of rank less than max_rank.
    trace_rtol: Scalar floating point `Tensor` (same dtype as `matrix`). If
      trace_rtol > 0 and trace(matrix - LR * LR^t) < trace_rtol * trace(matrix),
      the output LR matrix is allowed to be of rank less than max_rank.
    name: Optional name for the op.

  Returns:
    A triplet (LR, r, residual_diag) of
    LR: a matrix such that LR * LR^t is approximately the input matrix.
      If matrix is of shape (b1, ..., bn, m, m), then LR will be of shape
      (b1, ..., bn, m, r) where r <= max_rank.
    r: the rank of LR.  If r is < max_rank, then
      trace(matrix - LR * LR^t) < trace_atol, and
    residual_diag: The diagonal entries of matrix - LR * LR^t.  This is
      returned because together with LR, it is useful for preconditioning
      the input matrix.
  """
  ...

def lu_solve(lower_upper, perm, rhs, validate_args=..., name=...):
  """Solves systems of linear eqns `A X = RHS`, given LU factorizations.

  Note: this function does not verify the implied matrix is actually invertible
  nor is this condition checked even when `validate_args=True`.

  Args:
    lower_upper: `lu` as returned by `tf.linalg.lu`, i.e., if
      `matmul(P, matmul(L, U)) = X` then `lower_upper = L + U - eye`.
    perm: `p` as returned by `tf.linag.lu`, i.e., if
      `matmul(P, matmul(L, U)) = X` then `perm = argmax(P)`.
    rhs: Matrix-shaped float `Tensor` representing targets for which to solve;
      `A X = RHS`. To handle vector cases, use:
      `lu_solve(..., rhs[..., tf.newaxis])[..., 0]`.
    validate_args: Python `bool` indicating whether arguments should be checked
      for correctness. Note: this function does not verify the implied matrix is
      actually invertible, even when `validate_args=True`.
      Default value: `False` (i.e., don't validate arguments).
    name: Python `str` name given to ops managed by this object.
      Default value: `None` (i.e., 'lu_solve').

  Returns:
    x: The `X` in `A @ X = RHS`.

  #### Examples

  ```python
  import numpy as np
  import tensorflow as tf
  import tensorflow_probability as tfp

  x = [[[1., 2],
        [3, 4]],
       [[7, 8],
        [3, 4]]]
  inv_x = tfp.math.lu_solve(*tf.linalg.lu(x), rhs=tf.eye(2))
  tf.assert_near(tf.matrix_inverse(x), inv_x)
  # ==> True
  ```

  """
  ...

def lu_matrix_inverse(lower_upper, perm, validate_args=..., name=...):
  """Computes a matrix inverse given the matrix's LU decomposition.

  This op is conceptually identical to,

  ```python
  inv_X = tf.lu_matrix_inverse(*tf.linalg.lu(X))
  tf.assert_near(tf.matrix_inverse(X), inv_X)
  # ==> True
  ```

  Note: this function does not verify the implied matrix is actually invertible
  nor is this condition checked even when `validate_args=True`.

  Args:
    lower_upper: `lu` as returned by `tf.linalg.lu`, i.e., if
      `matmul(P, matmul(L, U)) = X` then `lower_upper = L + U - eye`.
    perm: `p` as returned by `tf.linag.lu`, i.e., if
      `matmul(P, matmul(L, U)) = X` then `perm = argmax(P)`.
    validate_args: Python `bool` indicating whether arguments should be checked
      for correctness. Note: this function does not verify the implied matrix is
      actually invertible, even when `validate_args=True`.
      Default value: `False` (i.e., don't validate arguments).
    name: Python `str` name given to ops managed by this object.
      Default value: `None` (i.e., 'lu_matrix_inverse').

  Returns:
    inv_x: The matrix_inv, i.e.,
      `tf.matrix_inverse(tfp.math.lu_reconstruct(lu, perm))`.

  #### Examples

  ```python
  import numpy as np
  import tensorflow as tf
  import tensorflow_probability as tfp

  x = [[[3., 4], [1, 2]],
       [[7., 8], [3, 4]]]
  inv_x = tfp.math.lu_matrix_inverse(*tf.linalg.lu(x))
  tf.assert_near(tf.matrix_inverse(x), inv_x)
  # ==> True
  ```

  """
  ...

def lu_reconstruct(lower_upper, perm, validate_args=..., name=...):
  """The inverse LU decomposition, `X == lu_reconstruct(*tf.linalg.lu(X))`.

  Args:
    lower_upper: `lu` as returned by `tf.linalg.lu`, i.e., if
      `matmul(P, matmul(L, U)) = X` then `lower_upper = L + U - eye`.
    perm: `p` as returned by `tf.linag.lu`, i.e., if
      `matmul(P, matmul(L, U)) = X` then `perm = argmax(P)`.
    validate_args: Python `bool` indicating whether arguments should be checked
      for correctness.
      Default value: `False` (i.e., don't validate arguments).
    name: Python `str` name given to ops managed by this object.
      Default value: `None` (i.e., 'lu_reconstruct').

  Returns:
    x: The original input to `tf.linalg.lu`, i.e., `x` as in,
      `lu_reconstruct(*tf.linalg.lu(x))`.

  #### Examples

  ```python
  import numpy as np
  import tensorflow as tf
  import tensorflow_probability as tfp

  x = [[[3., 4], [1, 2]],
       [[7., 8], [3, 4]]]
  x_reconstructed = tfp.math.lu_reconstruct(*tf.linalg.lu(x))
  tf.assert_near(x, x_reconstructed)
  # ==> True
  ```

  """
  ...

def lu_reconstruct_assertions(lower_upper, perm, validate_args): # -> list[Any]:
  """Returns list of assertions related to `lu_reconstruct` assumptions."""
  ...

def sparse_or_dense_matmul(sparse_or_dense_a, dense_b, validate_args=..., name=..., **kwargs):
  """Returns (batched) matmul of a SparseTensor (or Tensor) with a Tensor.

  Args:
    sparse_or_dense_a: `SparseTensor` or `Tensor` representing a (batch of)
      matrices.
    dense_b: `Tensor` representing a (batch of) matrices, with the same batch
      shape as `sparse_or_dense_a`. The shape must be compatible with the shape
      of `sparse_or_dense_a` and kwargs.
    validate_args: When `True`, additional assertions might be embedded in the
      graph.
      Default value: `False` (i.e., no graph assertions are added).
    name: Python `str` prefixed to ops created by this function.
      Default value: 'sparse_or_dense_matmul'.
    **kwargs: Keyword arguments to `tf.sparse_tensor_dense_matmul` or
      `tf.matmul`.

  Returns:
    product: A dense (batch of) matrix-shaped Tensor of the same batch shape and
    dtype as `sparse_or_dense_a` and `dense_b`. If `sparse_or_dense_a` or
    `dense_b` is adjointed through `kwargs` then the shape is adjusted
    accordingly.
  """
  ...

def sparse_or_dense_matvecmul(sparse_or_dense_matrix, dense_vector, validate_args=..., name=..., **kwargs):
  """Returns (batched) matmul of a (sparse) matrix with a column vector.

  Args:
    sparse_or_dense_matrix: `SparseTensor` or `Tensor` representing a (batch of)
      matrices.
    dense_vector: `Tensor` representing a (batch of) vectors, with the same
      batch shape as `sparse_or_dense_matrix`. The shape must be compatible with
      the shape of `sparse_or_dense_matrix` and kwargs.
    validate_args: When `True`, additional assertions might be embedded in the
      graph.
      Default value: `False` (i.e., no graph assertions are added).
    name: Python `str` prefixed to ops created by this function.
      Default value: 'sparse_or_dense_matvecmul'.
    **kwargs: Keyword arguments to `tf.sparse_tensor_dense_matmul` or
      `tf.matmul`.

  Returns:
    product: A dense (batch of) vector-shaped Tensor of the same batch shape and
    dtype as `sparse_or_dense_matrix` and `dense_vector`.
  """
  ...

def fill_triangular(x, upper=..., name=...):
  """Creates a (batch of) triangular matrix from a vector of inputs.

  Created matrix can be lower- or upper-triangular. (It is more efficient to
  create the matrix as upper or lower, rather than transpose.)

  Triangular matrix elements are filled in a clockwise spiral. See example,
  below.

  If `x.shape` is `[b1, b2, ..., bB, d]` then the output shape is
  `[b1, b2, ..., bB, n, n]` where `n` is such that `d = n(n+1)/2`, i.e.,
  `n = int(np.sqrt(0.25 + 2. * m) - 0.5)`.

  Example:

  ```python
  fill_triangular([1, 2, 3, 4, 5, 6])
  # ==> [[4, 0, 0],
  #      [6, 5, 0],
  #      [3, 2, 1]]

  fill_triangular([1, 2, 3, 4, 5, 6], upper=True)
  # ==> [[1, 2, 3],
  #      [0, 5, 6],
  #      [0, 0, 4]]
  ```

  The key trick is to create an upper triangular matrix by concatenating `x`
  and a tail of itself, then reshaping.

  Suppose that we are filling the upper triangle of an `n`-by-`n` matrix `M`
  from a vector `x`. The matrix `M` contains n**2 entries total. The vector `x`
  contains `n * (n+1) / 2` entries. For concreteness, we'll consider `n = 5`
  (so `x` has `15` entries and `M` has `25`). We'll concatenate `x` and `x` with
  the first (`n = 5`) elements removed and reversed:

  ```python
  x = np.arange(15) + 1
  xc = np.concatenate([x, x[5:][::-1]])
  # ==> array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 15, 14, 13,
  #            12, 11, 10, 9, 8, 7, 6])

  # (We add one to the arange result to disambiguate the zeros below the
  # diagonal of our upper-triangular matrix from the first entry in `x`.)

  # Now, when reshapedlay this out as a matrix:
  y = np.reshape(xc, [5, 5])
  # ==> array([[ 1,  2,  3,  4,  5],
  #            [ 6,  7,  8,  9, 10],
  #            [11, 12, 13, 14, 15],
  #            [15, 14, 13, 12, 11],
  #            [10,  9,  8,  7,  6]])

  # Finally, zero the elements below the diagonal:
  y = np.triu(y, k=0)
  # ==> array([[ 1,  2,  3,  4,  5],
  #            [ 0,  7,  8,  9, 10],
  #            [ 0,  0, 13, 14, 15],
  #            [ 0,  0,  0, 12, 11],
  #            [ 0,  0,  0,  0,  6]])
  ```

  From this example we see that the resuting matrix is upper-triangular, and
  contains all the entries of x, as desired. The rest is details:

  - If `n` is even, `x` doesn't exactly fill an even number of rows (it fills
    `n / 2` rows and half of an additional row), but the whole scheme still
    works.
  - If we want a lower triangular matrix instead of an upper triangular,
    we remove the first `n` elements from `x` rather than from the reversed
    `x`.

  For additional comparisons, a pure numpy version of this function can be found
  in `distribution_util_test.py`, function `_fill_triangular`.

  Args:
    x: `Tensor` representing lower (or upper) triangular elements.
    upper: Python `bool` representing whether output matrix should be upper
      triangular (`True`) or lower triangular (`False`, default).
    name: Python `str`. The name to give this op.

  Returns:
    tril: `Tensor` with lower (or upper) triangular elements filled from `x`.

  Raises:
    ValueError: if `x` cannot be mapped to a triangular matrix.
  """
  ...

def fill_triangular_inverse(x, upper=..., name=...):
  """Creates a vector from a (batch of) triangular matrix.

  The vector is created from the lower-triangular or upper-triangular portion
  depending on the value of the parameter `upper`.

  If `x.shape` is `[b1, b2, ..., bB, n, n]` then the output shape is
  `[b1, b2, ..., bB, d]` where `d = n (n + 1) / 2`.

  Example:

  ```python
  fill_triangular_inverse(
    [[4, 0, 0],
     [6, 5, 0],
     [3, 2, 1]])

  # ==> [1, 2, 3, 4, 5, 6]

  fill_triangular_inverse(
    [[1, 2, 3],
     [0, 5, 6],
     [0, 0, 4]], upper=True)

  # ==> [1, 2, 3, 4, 5, 6]
  ```

  Args:
    x: `Tensor` representing lower (or upper) triangular elements.
    upper: Python `bool` representing whether output matrix should be upper
      triangular (`True`) or lower triangular (`False`, default).
    name: Python `str`. The name to give this op.

  Returns:
    flat_tril: (Batch of) vector-shaped `Tensor` representing vectorized lower
      (or upper) triangular elements from `x`.
  """
  ...

def hpsd_logdet(matrix, cholesky_matrix=...):
  """Computes `log|det(matrix)|`, where `matrix` is a HPSD matrix.

  Given `matrix` computes `log|det(matrix)|`, where `matrix` is Hermitian
  positive Semi-definite matrix.

  Args:
    matrix: Floating-point `Tensor` of shape `[..., N, N]`. Represents
      a Hermitian positive semi-definite matrix.
    cholesky_matrix: (Optional) Floating-point `Tensor` of shape `[..., N, N]`
      that represents a Cholesky factor of `matrix`.
  Returns:
    hpsd_logdet: Scalar `Tensor`, retaining the batch shape of `matrix`.
  """
  ...

def hpsd_solve(matrix, rhs, cholesky_matrix=...):
  """Computes `matrix^-1 rhs`, where `matrix` is HPSD.

  Given `matrix` and `rhs`, computes `matrix^-1 rhs`, where
  `matrix` is a Hermitian positive semi-definite matrix.

  Args:
    matrix: Floating-point `Tensor` of shape `[..., N, N]`. Represents
      a Hermitian positive semi-definite matrix.
    rhs: Floating-point `Tensor` of shape `[..., N, K]`.
    cholesky_matrix: (Optional) Floating-point `Tensor` of shape `[..., N, N]`
      that represents a Cholesky factor of `matrix`.
  Returns:
    hpsd_solve: `Tensor` of shape `[..., N, K]`.
  """
  ...

def hpsd_solvevec(matrix, rhs, cholesky_matrix=...):
  """Computes `matrix^-1 rhs`, where `matrix` is HPSD.

  Given `matrix` and `rhs`, computes `matrix^-1 rhs`, where
  `matrix` is a Hermitian positive semi-definite matrix.

  Args:
    matrix: Floating-point `Tensor` of shape `[..., N, N]`. Represents
      a Hermitian positive semi-definite matrix.
    rhs: Floating-point `Tensor` of shape `[..., N]`.
    cholesky_matrix: (Optional) Floating-point `Tensor` of shape `[..., N, N]`
      that represents a Cholesky factor of `matrix`.
  Returns:
    hpsd_solvevec: `Tensor` of shape `[..., N]`.
  """
  ...

def hpsd_quadratic_form_solve(matrix, rhs, cholesky_matrix=...):
  """Computes `rhs^T matrix^-1 rhs`, where `matrix` is HPSD.

  Given `matrix` and `rhs`, computes `rhs^T @ matrix^-1 rhs`, where
  `matrix` is a Hermitian positive semi-definite matrix.

  Args:
    matrix: Floating-point `Tensor` of shape `[..., N, N]`. Represents
      a Hermitian positive semi-definite matrix.
    rhs: Floating-point `Tensor` of shape `[..., N, K]`.
    cholesky_matrix: (Optional) Floating-point `Tensor` of shape `[..., N, N]`
      that represents a Cholesky factor of `matrix`.
  Returns:
    hpsd_quadratic_form_solve: `Tensor` of shape `[..., K, K]`.
  """
  ...

def hpsd_quadratic_form_solvevec(matrix, rhs, cholesky_matrix=...):
  """Computes `rhs^T matrix^-1 rhs`, where `matrix` is HPSD.

  Given `matrix` and `rhs`, computes `rhs^T @ matrix^-1 rhs`, where
  `matrix` is a Hermitian positive semi-definite matrix.

  Args:
    matrix: Floating-point `Tensor` of shape `[..., N, N]`. Represents
      a Hermitian positive semi-definite matrix.
    rhs: Floating-point `Tensor` of shape `[..., N]`.
    cholesky_matrix: (Optional) Floating-point `Tensor` of shape `[..., N, N]`
      that represents a Cholesky factor of `matrix`.
  Returns:
    hpsd_quadratic_form_solvevec: Scalar `Tensor`.
  """
  ...

