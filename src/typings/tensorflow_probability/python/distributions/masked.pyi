"""
This type stub file was generated by pyright.
"""

from tensorflow_probability.python.bijectors import bijector as bijector_lib
from tensorflow_probability.python.distributions import distribution as distribution_lib

"""The MaskedIndependent distribution class."""
class _Masked(distribution_lib.Distribution):
  """A distribution that masks invalid underlying distributions.

  Sometimes we may want a way of masking out a subset of distributions. Perhaps
  we have labels for only a subset of batch members and want to evaluate a
  log_prob. Or we may want to encode a sparse random variable as a dense
  random variable with a mask applied. In single-program/multiple-data regimes,
  it can be necessary to pad Distributions and the samples thereof to a given
  size in order to achieve the "single-program" desideratum.

  When computing a probability density in this regime, we would like to mask out
  the contributions of invalid batch members. We may also want to ensure that
  the values being sampled are valid parameters for descendant distributions in
  a hierarchical model, even if they are ultimately masked out. This
  distribution answers those requirements. Specifically, for invalid batch
  elements:
  - `log_prob(x) == 0.` for all `x`, with no gradients back to `x`, nor any
    gradients to the parameters of `distribution`.
  - `sample() == tf.stop_gradient(safe_value_fn(distribution))`, with no
    gradients back to the parameters of `distribution`.

  The distribution accepts a mask specified by `validity_mask`, a boolean tensor
  broadcastable with the underlying distribution's batch shape which specifies
  for each batch element whether or not it is valid.

  Entries in `validity_mask` which are `False` denote missing distributions,
  which means that the corresponding entries in the measures (e.g. `prob`)
  and statistics (e.g. `mean`) must not be treated as coming from some real
  distribution. Whenever doing a reduction across those quantites, make sure to
  either mask out the invalid entries or make sure the returned value
  corresponds to the identity element of the reduction. For a couple examples:
  - OK: `reduce_sum(masked_dist.log_prob(x))`
  - OK: `tfd.Independent(masked_dist, ...)`
  - Not OK: `reduce_var(masked_dist.mean())` will underestimate the variance
    because it uses too large an `N`.
  - Not OK: `tf.linalg.cholesky(masked_dist.covariance())` will fail for invalid
    batch elements.

  The default `safe_value_fn` is to draw a fixed-seeded sample from the
  underlying `distribution`.  Since this may be expensive, it is suggested to
  specify a computationally cheaper method. Some options might include:
  - `tfd.Distribution.mode`
  - `tfd.Distribution.mean`
  - `lambda d: d.quantile(.5)` (median)
  - `lambda _: 0.` (if zero is always in the support of d)
  - `lambda d: d.experimental_default_event_space_bijector()(0.)`
  Besides the output of `sample`, results from `safe_value_fn` may also appear
  in (invalid batch members of) `masked.default_event_space_bijector().forward`.

  #### Examples

  ```
  # Use tf.sequence_mask for `range(n) < num_valid`.
  num_valid = 3
  num_entries = 4
  d = tfd.Masked(
      tfd.MultivariateNormalDiag(tf.zeros([2, num_entries, 5]), tf.ones([5])),
      tf.sequence_mask(num_valid, num_entries))
  d.batch_shape  # [2, 4]
  d.event_shape  # [5]
  d.log_prob(tf.zeros([5]))  # shape [2, 4]
  # => [[nonzero, nonzero, nonzero, 0.],
  #     [nonzero, nonzero, nonzero, 0.]]

  # Explicitly denote which elements are valid, adding a new batch dim of 2.
  d = tfd.Masked(tfd.MultivariateNormalDiag(tf.zeros([4, 5]), tf.ones([5])),
                 [[False], [True]])
  d.batch_shape  # [2, 4]
  d.event_shape  # [5]
  d.log_prob(tf.zeros([5]))  # shape [2, 4]
  # => [[0., 0., 0., 0.],
  #     [nonzero, nonzero, nonzero, nonzero]]

  # Use `BatchBroadcast` and `Independent` to achieve the equivalent of adding
  # positional mask functionality to `tfd.Sample`.
  # Suppose we wanted to achieve this:
  # `tfd.Sample(tfd.Normal(tf.zeros(2), 1), [3, 4], validity_mask=mask)`
  # We can write:
  d = tfd.Independent(
      tfd.Masked(tfd.BatchBroadcast(tfd.Normal(0, 1), [2, 3, 4]), mask),
      reinterpreted_batch_ndims=2)
  d.batch_shape  # [2]
  d.event_shape  # [3, 4]
  d.log_prob(tf.ones([3, 4]))  # shape [2]
  ```

  """
  def __init__(self, distribution, validity_mask, safe_sample_fn=..., validate_args=..., allow_nan_stats=..., name=...) -> None:
    """Constructs a Masked distribution.

    Args:
      distribution: The underlying distribution, which will be masked.
      validity_mask: Boolean mask where `True` indicates an element is valid.
        `validity_mask` must broadcast with the batch shape of the underlying
        distribution. Invalid batch elements are masked so that sampling returns
        `safe_sample_fn(dist)` in invalid positions and `log_prob(x)` returns
        `0.` for invalid positions.
      safe_sample_fn: A callable which takes a distribution (namely,
        the `distribution` argument) and returns a determinstic, safe sample
        value. This helps to avoid `nan` gradients and allows downstream usage
        of samples from a `Masked` distribution to assume a "safe" even if
        invalid value. (Be careful to ensure that such downstream usages are
        themselves masked!) Note that the result of this function will be
        wrapped in a `tf.stop_gradient` call.
      validate_args: Boolean indicating whether argument assertions should be
        run. May impose performance penalties.
      allow_nan_stats: Boolean indicating whether statistical functions may
        return `nan`, or should instead use asserts where possible.
      name: Optional name for operation scoping.
    """
    ...
  
  @property
  def distribution(self): # -> Any:
    ...
  
  @property
  def validity_mask(self):
    ...
  
  @property
  def safe_sample_fn(self):
    ...
  
  @property
  def experimental_is_sharded(self):
    ...
  
  _log_prob = ...
  _prob = ...
  _log_cdf = ...
  _cdf = ...
  _log_survival_function = ...
  _survival_function = ...
  _entropy = ...
  _mode = ...
  _mean = ...
  _variance = ...
  _stddev = ...
  _covariance = ...
  _quantile = ...


class Masked(_Masked, distribution_lib.AutoCompositeTensorDistribution):
  def __new__(cls, *args, **kwargs): # -> _Masked:
    """Maybe return a non-`CompositeTensor` `_Masked`."""
    ...
  


class _NonCompositeTensorMaskedBijector(bijector_lib.Bijector):
  """Event space bijector for Masked distributions."""
  def __init__(self, masked, underlying_bijector) -> None:
    ...
  


class _MaskedBijector(_NonCompositeTensorMaskedBijector, bijector_lib.AutoCompositeTensorBijector):
  """Event space bijector for Masked distributions."""
  def __new__(cls, *args, **kwargs): # -> _NonCompositeTensorMaskedBijector:
    """Maybe return a `_NonCompositeTensorMaskedBijector`."""
    ...
  


