"""
This type stub file was generated by pyright.
"""

import abc
import six
import tensorflow.compat.v2 as tf
from tensorflow_probability.python.internal import auto_composite_tensor
from tensorflow.python.util import deprecation

"""Base classes for probability distributions."""
__all__ = ['Distribution']
_DISTRIBUTION_PUBLIC_METHOD_WRAPPERS = ...
_ALWAYS_COPY_PUBLIC_METHOD_WRAPPERS = ...
UNSET_VALUE = ...
JAX_MODE = ...
@six.add_metaclass(abc.ABCMeta)
class _BaseDistribution(tf.Module):
  """Abstract base class needed for resolving subclass hierarchy."""
  ...


class _DistributionMeta(abc.ABCMeta):
  """Helper metaclass for tfp.Distribution."""
  def __new__(mcs, classname, baseclasses, attrs): # -> Self:
    """Control the creation of subclasses of the Distribution class.

    The main purpose of this method is to properly propagate docstrings
    from private Distribution methods, like `_log_prob`, into their
    public wrappers as inherited by the Distribution base class
    (e.g. `log_prob`).

    Args:
      classname: The name of the subclass being created.
      baseclasses: A tuple of parent classes.
      attrs: A dict mapping new attributes to their values.

    Returns:
      The class object.

    Raises:
      TypeError: If `Distribution` is not a subclass of `BaseDistribution`, or
        the new class is derived via multiple inheritance and the first
        parent class is not a subclass of `BaseDistribution`.
      AttributeError:  If `Distribution` does not implement e.g. `log_prob`.
      ValueError:  If a `Distribution` public method lacks a docstring.
    """
    ...
  


@six.add_metaclass(_DistributionMeta)
class Distribution(_BaseDistribution):
  """A generic probability distribution base class.

  `Distribution` is a base class for constructing and organizing properties
  (e.g., mean, variance) of random variables (e.g, Bernoulli, Gaussian).

  #### Subclassing

  Subclasses are expected to implement a leading-underscore version of the
  same-named function. The argument signature should be identical except for
  the omission of `name='...'`. For example, to enable `log_prob(value,
  name='log_prob')` a subclass should implement `_log_prob(value)`.

  Subclasses can append to public-level docstrings by providing
  docstrings for their method specializations. For example:

  ```python
  @distribution_util.AppendDocstring('Some other details.')
  def _log_prob(self, value):
    ...
  ```

  would add the string "Some other details." to the `log_prob` function
  docstring. This is implemented as a simple decorator to avoid python
  linter complaining about missing Args/Returns/Raises sections in the
  partial docstrings.

  TFP methods generally assume that Distribution subclasses implement at least
  the following methods:
  - `_sample_n`.
  - `_log_prob` or `_prob`.
  - `_event_shape` and `_event_shape_tensor`.
  - `_parameter_properties` OR `_batch_shape` and `_batch_shape_tensor`.

  Batch shape methods can be automatically derived from `parameter_properties`
  in most cases, so it's usually not necessary to implement them directly.
  Exceptions include Distributions that accept non-Tensor parameters (for
  example, a distribution parameterized by a callable), or that have nonstandard
  batch semantics (for example, `BatchReshape`).

  Some functionality may depend on implementing additional methods. It is common
  for Distribution subclasses to implement:

  - Relevant statistics, such as `_mean`, `_mode`, `_variance` and/or `_stddev`.
  - At least one of `_log_cdf`, `_cdf`, `_survival_function`, or
    `_log_survival_function`.
  - `_quantile`.
  - `_entropy`.
  - `_default_event_space_bijector`.
  - `_parameter_properties` (to support automatic batch shape derivation,
    batch slicing and other features).
  - `_sample_and_log_prob`,
  - `_maximum_likelihood_parameters`.

  Note that subclasses of existing Distributions that redefine `__init__` do
  *not* automatically inherit
  `_parameter_properties` annotations from their parent: the subclass must
  explicitly implement its own `_parameter_properties` method to support the
  features, such as batch slicing, that this enables.

  #### Broadcasting, batching, and shapes

  All distributions support batches of independent distributions of that type.
  The batch shape is determined by broadcasting together the parameters.

  The shape of arguments to `__init__`, `cdf`, `log_cdf`, `prob`, and
  `log_prob` reflect this broadcasting, as does the return value of `sample`.

  `sample_n_shape = [n] + batch_shape + event_shape`, where `sample_n_shape` is
  the shape of the `Tensor` returned from `sample(n)`, `n` is the number of
  samples, `batch_shape` defines how many independent distributions there are,
  and `event_shape` defines the shape of samples from each of those independent
  distributions. Samples are independent along the `batch_shape` dimensions, but
  not necessarily so along the `event_shape` dimensions (depending on the
  particulars of the underlying distribution).

  Using the `Uniform` distribution as an example:

  ```python
  minval = 3.0
  maxval = [[4.0, 6.0],
            [10.0, 12.0]]

  # Broadcasting:
  # This instance represents 4 Uniform distributions. Each has a lower bound at
  # 3.0 as the `minval` parameter was broadcasted to match `maxval`'s shape.
  u = Uniform(minval, maxval)

  # `event_shape` is `TensorShape([])`.
  event_shape = u.event_shape
  # `event_shape_t` is a `Tensor` which will evaluate to [].
  event_shape_t = u.event_shape_tensor()

  # Sampling returns a sample per distribution. `samples` has shape
  # [5, 2, 2], which is [n] + batch_shape + event_shape, where n=5,
  # batch_shape=[2, 2], and event_shape=[].
  samples = u.sample(5)

  # The broadcasting holds across methods. Here we use `cdf` as an example. The
  # same holds for `log_cdf` and the likelihood functions.

  # `cum_prob` has shape [2, 2] as the `value` argument was broadcasted to the
  # shape of the `Uniform` instance.
  cum_prob_broadcast = u.cdf(4.0)

  # `cum_prob`'s shape is [2, 2], one per distribution. No broadcasting
  # occurred.
  cum_prob_per_dist = u.cdf([[4.0, 5.0],
                             [6.0, 7.0]])

  # INVALID as the `value` argument is not broadcastable to the distribution's
  # shape.
  cum_prob_invalid = u.cdf([4.0, 5.0, 6.0])
  ```

  #### Shapes

  There are three important concepts associated with TensorFlow Distributions
  shapes:

  - Event shape describes the shape of a single draw from the distribution;
    it may be dependent across dimensions. For scalar distributions, the event
    shape is `[]`. For a 5-dimensional MultivariateNormal, the event shape is
    `[5]`.
  - Batch shape describes independent, not identically distributed draws, aka a
    "collection" or "bunch" of distributions.
  - Sample shape describes independent, identically distributed draws of batches
    from the distribution family.

  The event shape and the batch shape are properties of a Distribution object,
  whereas the sample shape is associated with a specific call to `sample` or
  `log_prob`.

  For detailed usage examples of TensorFlow Distributions shapes, see
  [this tutorial](
  https://github.com/tensorflow/probability/blob/main/tensorflow_probability/examples/jupyter_notebooks/Understanding_TensorFlow_Distributions_Shapes.ipynb)

  #### Parameter values leading to undefined statistics or distributions.

  Some distributions do not have well-defined statistics for all initialization
  parameter values. For example, the beta distribution is parameterized by
  positive real numbers `concentration1` and `concentration0`, and does not have
  well-defined mode if `concentration1 < 1` or `concentration0 < 1`.

  The user is given the option of raising an exception or returning `NaN`.

  ```python
  a = tf.exp(tf.matmul(logits, weights_a))
  b = tf.exp(tf.matmul(logits, weights_b))

  # Will raise exception if ANY batch member has a < 1 or b < 1.
  dist = distributions.beta(a, b, allow_nan_stats=False)
  mode = dist.mode()

  # Will return NaN for batch members with either a < 1 or b < 1.
  dist = distributions.beta(a, b, allow_nan_stats=True)  # Default behavior
  mode = dist.mode()
  ```

  In all cases, an exception is raised if *invalid* parameters are passed, e.g.

  ```python
  # Will raise an exception if any Op is run.
  negative_a = -1.0 * a  # beta distribution by definition has a > 0.
  dist = distributions.beta(negative_a, b, allow_nan_stats=True)
  dist.mean()
  ```

  """
  def __init__(self, dtype, reparameterization_type, validate_args, allow_nan_stats, parameters=..., graph_parents=..., name=...) -> None:
    """Constructs the `Distribution`.

    **This is a private method for subclass use.**

    Args:
      dtype: The type of the event samples. `None` implies no type-enforcement.
      reparameterization_type: Instance of `ReparameterizationType`.
        If `tfd.FULLY_REPARAMETERIZED`, then samples from the distribution are
        fully reparameterized, and straight-through gradients are supported.
        If `tfd.NOT_REPARAMETERIZED`, then samples from the distribution are not
        fully reparameterized, and straight-through gradients are either
        partially unsupported or are not supported at all.
      validate_args: Python `bool`, default `False`. When `True` distribution
        parameters are checked for validity despite possibly degrading runtime
        performance. When `False` invalid inputs may silently render incorrect
        outputs.
      allow_nan_stats: Python `bool`, default `True`. When `True`, statistics
        (e.g., mean, mode, variance) use the value "`NaN`" to indicate the
        result is undefined. When `False`, an exception is raised if one or
        more of the statistic's batch members are undefined.
      parameters: Python `dict` of parameters used to instantiate this
        `Distribution`.
      graph_parents: Python `list` of graph prerequisites of this
        `Distribution`.
      name: Python `str` name prefixed to Ops created by this class. Default:
        subclass name.

    Raises:
      ValueError: if any member of graph_parents is `None` or not a `Tensor`.
    """
    ...
  
  @classmethod
  def parameter_properties(cls, dtype=..., num_classes=...):
    """Returns a dict mapping constructor arg names to property annotations.

    This dict should include an entry for each of the distribution's
    `Tensor`-valued constructor arguments.

    Distribution subclasses are not required to implement
    `_parameter_properties`, so this method may raise `NotImplementedError`.
    Providing a `_parameter_properties` implementation enables several advanced
    features, including:
      - Distribution batch slicing (`sliced_distribution = distribution[i:j]`).
      - Automatic inference of `_batch_shape` and
        `_batch_shape_tensor`, which must otherwise be computed explicitly.
      - Automatic instantiation of the distribution within TFP's internal
        property tests.
      - Automatic construction of 'trainable' instances of the distribution
        using appropriate bijectors to avoid violating parameter constraints.
        This enables the distribution family to be used easily as a
        surrogate posterior in variational inference.

    In the future, parameter property annotations may enable additional
    functionality; for example, returning Distribution instances from
    `tf.vectorized_map`.

    Args:
      dtype: Optional float `dtype` to assume for continuous-valued parameters.
        Some constraining bijectors require advance knowledge of the dtype
        because certain constants (e.g., `tfb.Softplus.low`) must be
        instantiated with the same dtype as the values to be transformed.
      num_classes: Optional `int` `Tensor` number of classes to assume when
        inferring the shape of parameters for categorical-like distributions.
        Otherwise ignored.

    Returns:
      parameter_properties: A
        `str -> `tfp.python.internal.parameter_properties.ParameterProperties`
        dict mapping constructor argument names to `ParameterProperties`
        instances.
    Raises:
      NotImplementedError: if the distribution class does not implement
        `_parameter_properties`.
    """
    ...
  
  @classmethod
  @deprecation.deprecated('2021-03-01', 'The `param_shapes` method of `tfd.Distribution` is ' 'deprecated; use `parameter_properties` instead.')
  def param_shapes(cls, sample_shape, name=...): # -> dict[Any, Any]:
    """Shapes of parameters given the desired shape of a call to `sample()`.

    This is a class method that describes what key/value arguments are required
    to instantiate the given `Distribution` so that a particular shape is
    returned for that instance's call to `sample()`.

    Subclasses should override class method `_param_shapes`.

    Args:
      sample_shape: `Tensor` or python list/tuple. Desired shape of a call to
        `sample()`.
      name: name to prepend ops with.

    Returns:
      `dict` of parameter name to `Tensor` shapes.
    """
    ...
  
  @classmethod
  @deprecation.deprecated('2021-03-01', 'The `param_static_shapes` method of `tfd.Distribution` is ' 'deprecated; use `parameter_properties` instead.')
  def param_static_shapes(cls, sample_shape): # -> dict[Any, Any]:
    """param_shapes with static (i.e. `TensorShape`) shapes.

    This is a class method that describes what key/value arguments are required
    to instantiate the given `Distribution` so that a particular shape is
    returned for that instance's call to `sample()`. Assumes that the sample's
    shape is known statically.

    Subclasses should override class method `_param_shapes` to return
    constant-valued tensors when constant values are fed.

    Args:
      sample_shape: `TensorShape` or python list/tuple. Desired shape of a call
        to `sample()`.

    Returns:
      `dict` of parameter name to `TensorShape`.

    Raises:
      ValueError: if `sample_shape` is a `TensorShape` and is not fully defined.
    """
    ...
  
  @property
  def name(self): # -> str | None:
    """Name prepended to all ops created by this `Distribution`."""
    ...
  
  @property
  def dtype(self): # -> None:
    """The `DType` of `Tensor`s handled by this `Distribution`."""
    ...
  
  @property
  def parameters(self): # -> dict[bytes, bytes]:
    """Dictionary of parameters used to instantiate this `Distribution`."""
    ...
  
  def __getitem__(self, slices): # -> Any:
    """Slices the batch axes of this distribution, returning a new instance.

    ```python
    b = tfd.Bernoulli(logits=tf.zeros([3, 5, 7, 9]))
    b.batch_shape  # => [3, 5, 7, 9]
    b2 = b[:, tf.newaxis, ..., -2:, 1::2]
    b2.batch_shape  # => [3, 1, 5, 2, 4]

    x = tf.random.normal([5, 3, 2, 2])
    cov = tf.matmul(x, x, transpose_b=True)
    chol = tf.linalg.cholesky(cov)
    loc = tf.random.normal([4, 1, 3, 1])
    mvn = tfd.MultivariateNormalTriL(loc, chol)
    mvn.batch_shape  # => [4, 5, 3]
    mvn.event_shape  # => [2]
    mvn2 = mvn[:, 3:, ..., ::-1, tf.newaxis]
    mvn2.batch_shape  # => [4, 2, 3, 1]
    mvn2.event_shape  # => [2]
    ```

    Args:
      slices: slices from the [] operator

    Returns:
      dist: A new `tfd.Distribution` instance with sliced parameters.
    """
    ...
  
  def __iter__(self):
    ...
  
  @property
  def reparameterization_type(self): # -> Any:
    """Describes how samples from the distribution are reparameterized.

    Currently this is one of the static instances
    `tfd.FULLY_REPARAMETERIZED` or `tfd.NOT_REPARAMETERIZED`.

    Returns:
      An instance of `ReparameterizationType`.
    """
    ...
  
  @property
  def allow_nan_stats(self): # -> Any:
    """Python `bool` describing behavior when a stat is undefined.

    Stats return +/- infinity when it makes sense. E.g., the variance of a
    Cauchy distribution is infinity. However, sometimes the statistic is
    undefined, e.g., if a distribution's pdf does not achieve a maximum within
    the support of the distribution, the mode is undefined. If the mean is
    undefined, then by definition the variance is undefined. E.g. the mean for
    Student's T for df = 1 is undefined (no clear way to say it is either + or -
    infinity), so the variance = E[(X - mean)**2] is also undefined.

    Returns:
      allow_nan_stats: Python `bool`.
    """
    ...
  
  @property
  def validate_args(self): # -> Any:
    """Python `bool` indicating possibly expensive checks are enabled."""
    ...
  
  @property
  def experimental_shard_axis_names(self): # -> list[Any]:
    """The list or structure of lists of active shard axis names."""
    ...
  
  def copy(self, **override_parameters_kwargs): # -> Any | Self:
    """Creates a deep copy of the distribution.

    Note: the copy distribution may continue to depend on the original
    initialization arguments.

    Args:
      **override_parameters_kwargs: String/value dictionary of initialization
        arguments to override with new values.

    Returns:
      distribution: A new instance of `type(self)` initialized from the union
        of self.parameters and override_parameters_kwargs, i.e.,
        `dict(self.parameters, **override_parameters_kwargs)`.
    """
    ...
  
  def batch_shape_tensor(self, name=...): # -> defaultdict[Any, Any] | Any | list[Any] | None:
    """Shape of a single sample from a single event index as a 1-D `Tensor`.

    The batch dimensions are indexes into independent, non-identical
    parameterizations of this distribution.

    Args:
      name: name to give to the op

    Returns:
      batch_shape: `Tensor`.
    """
    ...
  
  @property
  def batch_shape(self):
    """Shape of a single sample from a single event index as a `TensorShape`.

    May be partially defined or unknown.

    The batch dimensions are indexes into independent, non-identical
    parameterizations of this distribution.

    Returns:
      batch_shape: `TensorShape`, possibly unknown.
    """
    ...
  
  def event_shape_tensor(self, name=...): # -> defaultdict[Any, Any] | Any | list[Any] | None:
    """Shape of a single sample from a single batch as a 1-D int32 `Tensor`.

    Args:
      name: name to give to the op

    Returns:
      event_shape: `Tensor`.
    """
    ...
  
  @property
  def event_shape(self): # -> defaultdict[Any, Any] | Any | list[Any] | None:
    """Shape of a single sample from a single batch as a `TensorShape`.

    May be partially defined or unknown.

    Returns:
      event_shape: `TensorShape`, possibly unknown.
    """
    ...
  
  def is_scalar_event(self, name=...):
    """Indicates that `event_shape == []`.

    Args:
      name: Python `str` prepended to names of ops created by this function.

    Returns:
      is_scalar_event: `bool` scalar `Tensor`.
    """
    ...
  
  def is_scalar_batch(self, name=...):
    """Indicates that `batch_shape == []`.

    Args:
      name: Python `str` prepended to names of ops created by this function.

    Returns:
      is_scalar_batch: `bool` scalar `Tensor`.
    """
    ...
  
  def sample(self, sample_shape=..., seed=..., name=..., **kwargs):
    """Generate samples of the specified shape.

    Note that a call to `sample()` without arguments will generate a single
    sample.

    Args:
      sample_shape: 0D or 1D `int32` `Tensor`. Shape of the generated samples.
      seed: PRNG seed; see `tfp.random.sanitize_seed` for details.
      name: name to give to the op.
      **kwargs: Named arguments forwarded to subclass implementation.

    Returns:
      samples: a `Tensor` with prepended dimensions `sample_shape`.
    """
    ...
  
  def experimental_sample_and_log_prob(self, sample_shape=..., seed=..., name=..., **kwargs): # -> tuple[Any, Any]:
    """Samples from this distribution and returns the log density of the sample.

    The default implementation simply calls `sample` and `log_prob`:

    ```
    def _sample_and_log_prob(self, sample_shape, seed, **kwargs):
      x = self.sample(sample_shape=sample_shape, seed=seed, **kwargs)
      return x, self.log_prob(x, **kwargs)
    ```

    However, some subclasses may provide more efficient and/or numerically
    stable implementations.

    Args:
      sample_shape: integer `Tensor` desired shape of samples to draw.
        Default value: `()`.
      seed: PRNG seed; see `tfp.random.sanitize_seed` for details.
        Default value: `None`.
      name: name to give to the op.
        Default value: `'sample_and_log_prob'`.
      **kwargs: Named arguments forwarded to subclass implementation.
    Returns:
      samples: a `Tensor`, or structure of `Tensor`s, with prepended dimensions
        `sample_shape`.
      log_prob: a `Tensor` of shape `sample_shape(x) + self.batch_shape` with
        values of type `self.dtype`.
    """
    ...
  
  def log_prob(self, value, name=..., **kwargs):
    """Log probability density/mass function.

    Args:
      value: `float` or `double` `Tensor`.
      name: Python `str` prepended to names of ops created by this function.
      **kwargs: Named arguments forwarded to subclass implementation.

    Returns:
      log_prob: a `Tensor` of shape `sample_shape(x) + self.batch_shape` with
        values of type `self.dtype`.
    """
    ...
  
  def prob(self, value, name=..., **kwargs):
    """Probability density/mass function.

    Args:
      value: `float` or `double` `Tensor`.
      name: Python `str` prepended to names of ops created by this function.
      **kwargs: Named arguments forwarded to subclass implementation.

    Returns:
      prob: a `Tensor` of shape `sample_shape(x) + self.batch_shape` with
        values of type `self.dtype`.
    """
    ...
  
  def unnormalized_log_prob(self, value, name=..., **kwargs):
    """Potentially unnormalized log probability density/mass function.

    This function is similar to `log_prob`, but does not require that the
    return value be normalized.  (Normalization here refers to the total
    integral of probability being one, as it should be by definition for any
    probability distribution.)  This is useful, for example, for distributions
    where the normalization constant is difficult or expensive to compute.  By
    default, this simply calls `log_prob`.

    Args:
      value: `float` or `double` `Tensor`.
      name: Python `str` prepended to names of ops created by this function.
      **kwargs: Named arguments forwarded to subclass implementation.

    Returns:
      unnormalized_log_prob: a `Tensor` of shape
        `sample_shape(x) + self.batch_shape` with values of type `self.dtype`.
    """
    ...
  
  def log_cdf(self, value, name=..., **kwargs):
    """Log cumulative distribution function.

    Given random variable `X`, the cumulative distribution function `cdf` is:

    ```none
    log_cdf(x) := Log[ P[X <= x] ]
    ```

    Often, a numerical approximation can be used for `log_cdf(x)` that yields
    a more accurate answer than simply taking the logarithm of the `cdf` when
    `x << -1`.

    Args:
      value: `float` or `double` `Tensor`.
      name: Python `str` prepended to names of ops created by this function.
      **kwargs: Named arguments forwarded to subclass implementation.

    Returns:
      logcdf: a `Tensor` of shape `sample_shape(x) + self.batch_shape` with
        values of type `self.dtype`.
    """
    ...
  
  def cdf(self, value, name=..., **kwargs):
    """Cumulative distribution function.

    Given random variable `X`, the cumulative distribution function `cdf` is:

    ```none
    cdf(x) := P[X <= x]
    ```

    Args:
      value: `float` or `double` `Tensor`.
      name: Python `str` prepended to names of ops created by this function.
      **kwargs: Named arguments forwarded to subclass implementation.

    Returns:
      cdf: a `Tensor` of shape `sample_shape(x) + self.batch_shape` with
        values of type `self.dtype`.
    """
    ...
  
  def log_survival_function(self, value, name=..., **kwargs):
    """Log survival function.

    Given random variable `X`, the survival function is defined:

    ```none
    log_survival_function(x) = Log[ P[X > x] ]
                             = Log[ 1 - P[X <= x] ]
                             = Log[ 1 - cdf(x) ]
    ```

    Typically, different numerical approximations can be used for the log
    survival function, which are more accurate than `1 - cdf(x)` when `x >> 1`.

    Args:
      value: `float` or `double` `Tensor`.
      name: Python `str` prepended to names of ops created by this function.
      **kwargs: Named arguments forwarded to subclass implementation.

    Returns:
      `Tensor` of shape `sample_shape(x) + self.batch_shape` with values of type
        `self.dtype`.
    """
    ...
  
  def survival_function(self, value, name=..., **kwargs):
    """Survival function.

    Given random variable `X`, the survival function is defined:

    ```none
    survival_function(x) = P[X > x]
                         = 1 - P[X <= x]
                         = 1 - cdf(x).
    ```

    Args:
      value: `float` or `double` `Tensor`.
      name: Python `str` prepended to names of ops created by this function.
      **kwargs: Named arguments forwarded to subclass implementation.

    Returns:
      `Tensor` of shape `sample_shape(x) + self.batch_shape` with values of type
        `self.dtype`.
    """
    ...
  
  def entropy(self, name=..., **kwargs):
    """Shannon entropy in nats."""
    ...
  
  def mean(self, name=..., **kwargs):
    """Mean."""
    ...
  
  def quantile(self, value, name=..., **kwargs):
    """Quantile function. Aka 'inverse cdf' or 'percent point function'.

    Given random variable `X` and `p in [0, 1]`, the `quantile` is:

    ```none
    quantile(p) := x such that P[X <= x] == p
    ```

    Args:
      value: `float` or `double` `Tensor`.
      name: Python `str` prepended to names of ops created by this function.
      **kwargs: Named arguments forwarded to subclass implementation.

    Returns:
      quantile: a `Tensor` of shape `sample_shape(x) + self.batch_shape` with
        values of type `self.dtype`.
    """
    ...
  
  def variance(self, name=..., **kwargs):
    """Variance.

    Variance is defined as,

    ```none
    Var = E[(X - E[X])**2]
    ```

    where `X` is the random variable associated with this distribution, `E`
    denotes expectation, and `Var.shape = batch_shape + event_shape`.

    Args:
      name: Python `str` prepended to names of ops created by this function.
      **kwargs: Named arguments forwarded to subclass implementation.

    Returns:
      variance: Floating-point `Tensor` with shape identical to
        `batch_shape + event_shape`, i.e., the same shape as `self.mean()`.
    """
    ...
  
  def stddev(self, name=..., **kwargs):
    """Standard deviation.

    Standard deviation is defined as,

    ```none
    stddev = E[(X - E[X])**2]**0.5
    ```

    where `X` is the random variable associated with this distribution, `E`
    denotes expectation, and `stddev.shape = batch_shape + event_shape`.

    Args:
      name: Python `str` prepended to names of ops created by this function.
      **kwargs: Named arguments forwarded to subclass implementation.

    Returns:
      stddev: Floating-point `Tensor` with shape identical to
        `batch_shape + event_shape`, i.e., the same shape as `self.mean()`.
    """
    ...
  
  def covariance(self, name=..., **kwargs):
    """Covariance.

    Covariance is (possibly) defined only for non-scalar-event distributions.

    For example, for a length-`k`, vector-valued distribution, it is calculated
    as,

    ```none
    Cov[i, j] = Covariance(X_i, X_j) = E[(X_i - E[X_i]) (X_j - E[X_j])]
    ```

    where `Cov` is a (batch of) `k x k` matrix, `0 <= (i, j) < k`, and `E`
    denotes expectation.

    Alternatively, for non-vector, multivariate distributions (e.g.,
    matrix-valued, Wishart), `Covariance` shall return a (batch of) matrices
    under some vectorization of the events, i.e.,

    ```none
    Cov[i, j] = Covariance(Vec(X)_i, Vec(X)_j) = [as above]
    ```

    where `Cov` is a (batch of) `k' x k'` matrices,
    `0 <= (i, j) < k' = reduce_prod(event_shape)`, and `Vec` is some function
    mapping indices of this distribution's event dimensions to indices of a
    length-`k'` vector.

    Args:
      name: Python `str` prepended to names of ops created by this function.
      **kwargs: Named arguments forwarded to subclass implementation.

    Returns:
      covariance: Floating-point `Tensor` with shape `[B1, ..., Bn, k', k']`
        where the first `n` dimensions are batch coordinates and
        `k' = reduce_prod(self.event_shape)`.
    """
    ...
  
  def mode(self, name=..., **kwargs):
    """Mode."""
    ...
  
  def cross_entropy(self, other, name=...):
    """Computes the (Shannon) cross entropy.

    Denote this distribution (`self`) by `P` and the `other` distribution by
    `Q`. Assuming `P, Q` are absolutely continuous with respect to
    one another and permit densities `p(x) dr(x)` and `q(x) dr(x)`, (Shannon)
    cross entropy is defined as:

    ```none
    H[P, Q] = E_p[-log q(X)] = -int_F p(x) log q(x) dr(x)
    ```

    where `F` denotes the support of the random variable `X ~ P`.

    Args:
      other: `tfp.distributions.Distribution` instance.
      name: Python `str` prepended to names of ops created by this function.

    Returns:
      cross_entropy: `self.dtype` `Tensor` with shape `[B1, ..., Bn]`
        representing `n` different calculations of (Shannon) cross entropy.
    """
    ...
  
  def kl_divergence(self, other, name=...):
    """Computes the Kullback--Leibler divergence.

    Denote this distribution (`self`) by `p` and the `other` distribution by
    `q`. Assuming `p, q` are absolutely continuous with respect to reference
    measure `r`, the KL divergence is defined as:

    ```none
    KL[p, q] = E_p[log(p(X)/q(X))]
             = -int_F p(x) log q(x) dr(x) + int_F p(x) log p(x) dr(x)
             = H[p, q] - H[p]
    ```

    where `F` denotes the support of the random variable `X ~ p`, `H[., .]`
    denotes (Shannon) cross entropy, and `H[.]` denotes (Shannon) entropy.

    Args:
      other: `tfp.distributions.Distribution` instance.
      name: Python `str` prepended to names of ops created by this function.

    Returns:
      kl_divergence: `self.dtype` `Tensor` with shape `[B1, ..., Bn]`
        representing `n` different calculations of the Kullback-Leibler
        divergence.
    """
    ...
  
  def experimental_default_event_space_bijector(self, *args, **kwargs):
    """Bijector mapping the reals (R**n) to the event space of the distribution.

    Distributions with continuous support may implement
    `_default_event_space_bijector` which returns a subclass of
    `tfp.bijectors.Bijector` that maps R**n to the distribution's event space.
    For example, the default bijector for the `Beta` distribution
    is `tfp.bijectors.Sigmoid()`, which maps the real line to `[0, 1]`, the
    support of the `Beta` distribution. The default bijector for the
    `CholeskyLKJ` distribution is `tfp.bijectors.CorrelationCholesky`, which
    maps R^(k * (k-1) // 2) to the submanifold of k x k lower triangular
    matrices with ones along the diagonal.

    The purpose of `experimental_default_event_space_bijector` is
    to enable gradient descent in an unconstrained space for Variational
    Inference and Hamiltonian Monte Carlo methods. Some effort has been made to
    choose bijectors such that the tails of the distribution in the
    unconstrained space are between Gaussian and Exponential.

    For distributions with discrete event space, or for which TFP currently
    lacks a suitable bijector, this function returns `None`.

    Args:
      *args: Passed to implementation `_default_event_space_bijector`.
      **kwargs: Passed to implementation `_default_event_space_bijector`.

    Returns:
      event_space_bijector: `Bijector` instance or `None`.
    """
    ...
  
  @classmethod
  def experimental_fit(cls, value, sample_ndims=..., validate_args=..., **init_kwargs): # -> Self:
    """Instantiates a distribution that maximizes the likelihood of `x`.

    Args:
      value: a `Tensor` valid sample from this distribution family.
      sample_ndims: Positive `int` Tensor number of leftmost dimensions of
        `value` that index i.i.d. samples.
        Default value: `1`.
      validate_args: Python `bool`, default `False`. When `True`, distribution
        parameters are checked for validity despite possibly degrading runtime
        performance. When `False`, invalid inputs may silently render incorrect
        outputs.
        Default value: `False`.
      **init_kwargs: Additional keyword arguments passed through to
        `cls.__init__`. These take precedence in case of collision with the
        fitted parameters; for example,
        `tfd.Normal.experimental_fit([1., 1.], scale=20.)` returns a Normal
        distribution with `scale=20.` rather than the maximum likelihood
        parameter `scale=0.`.
    Returns:
      maximum_likelihood_instance: instance of `cls` with parameters that
        maximize the likelihood of `value`.
    """
    ...
  
  def experimental_local_measure(self, value, backward_compat=..., **kwargs): # -> tuple[Any, Any | FullSpace]:
    """Returns a log probability density together with a `TangentSpace`.

    A `TangentSpace` allows us to calculate the correct push-forward
    density when we apply a transformation to a `Distribution` on
    a strict submanifold of R^n (typically via a `Bijector` in the
    `TransformedDistribution` subclass). The density correction uses
    the basis of the tangent space.

    Args:
      value: `float` or `double` `Tensor`.
      backward_compat: `bool` specifying whether to fall back to returning
        `FullSpace` as the tangent space, and representing R^n with the standard
         basis.
      **kwargs: Named arguments forwarded to subclass implementation.

    Returns:
      log_prob: a `Tensor` representing the log probability density, of shape
        `sample_shape(x) + self.batch_shape` with values of type `self.dtype`.
      tangent_space: a `TangentSpace` object (by default `FullSpace`)
        representing the tangent space to the manifold at `value`.

    Raises:
      UnspecifiedTangentSpaceError if `backward_compat` is False and
        the `_experimental_tangent_space` attribute has not been defined.
    """
    ...
  
  def __str__(self) -> str:
    ...
  
  def __repr__(self): # -> str:
    ...
  


class _AutoCompositeTensorDistributionMeta(_DistributionMeta):
  """Metaclass for `AutoCompositeTensorBijector`."""
  def __new__(mcs, classname, baseclasses, attrs): # -> partial[Any] | type[CompositeTensor] | type[_AutoCompositeTensor]:
    """Give subclasses their own type_spec, not an inherited one."""
    ...
  


class AutoCompositeTensorDistribution(Distribution, auto_composite_tensor.AutoCompositeTensor, metaclass=_AutoCompositeTensorDistributionMeta):
  r"""Base for `CompositeTensor` bijectors with auto-generated `TypeSpec`s.

  `CompositeTensor` objects are able to pass in and out of `tf.function` and
  `tf.while_loop`, or serve as part of the signature of a TF saved model.
  `Distribution` subclasses that follow the contract of
  `tfp.experimental.auto_composite_tensor` may be defined as `CompositeTensor`s
  by inheriting from `AutoCompositeTensorDistribution`:

  ```python
  class MyDistribution(tfb.AutoCompositeTensorDistribution):

    # The remainder of the subclass implementation is unchanged.
  ```
  """
  ...


class DiscreteDistributionMixin:
  """Mixin for Distributions over discrete spaces.

  This mixin identifies a `Distribution` as a discrete distribution, which in
  turn ensures that it is transformed properly under `TransformedDistribution`.

  Normally, for a continuous distribution `dist` by a bijector `bij`, we have
  the following formula for the `log_prob`:
      `dist.log_prob(bij.inverse(y)) + bij.inverse_log_det_jacobian(y)`.
  For a discrete distribution, we don't apply the `inverse_log_det_jacobian`
  correction (hence just `dist.log_prob(bij.inverse(y))`). This difference
  comes from transforming a probability density vs. probabilities.

  As an example, we could take a Bernoulli distribution (
  whose samples are `0` or `1`) and square it via `tfb.Square`. Samples from
  this new distribution are still `0` or `1` and one would expect that the
  probabilities for `0` and `1` are unchanged after this transformation.

  ```python
  dist = tfp.distributions.Bernoulli(probs=0.5)
  dist.prob(1.)  # expect 0.5
  transformed_dist = tfp.bijectors.Square()(dist)
  transformed_dist.prob(1.) # expect 0.5
  ```

  If we apply the jacobian correction, we would instead get the wrong answer

  ```python
  # If we compute with the jacobian correction explicitly, we get the wrong
  # answer.
  bij = tfp.bijectors.Square()
  prob_at_1 = dist.log_prob(bij.inverse(1.)) + bij.inverse_log_det_jacobian(1.)
  prob_at_1 = tf.math.exp(prob_at_1) # This is 0.25
  ```
  """
  ...


class _PrettyDict(dict):
  """`dict` with stable `repr`, `str`."""
  def __str__(self) -> str:
    ...
  
  def __repr__(self): # -> str:
    ...
  


