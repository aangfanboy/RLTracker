"""
This type stub file was generated by pyright.
"""

from tensorflow_probability.python.distributions import distribution, transformed_distribution

"""The ExpGamma distribution class."""
__all__ = ['ExpGamma', 'ExpInverseGamma']
class ExpGamma(distribution.AutoCompositeTensorDistribution):
  """ExpGamma distribution.

  The ExpGamma distribution is defined over the real line using
  parameters `concentration` (aka "alpha") and `rate` (aka "beta").

  This distribution is a transformation of the Gamma distribution such that
  X ~ ExpGamma(..) => exp(X) ~ Gamma(..).

  #### Mathematical Details

  The probability density function (pdf) can be derived from the change of
  variables rule (since the distribution is logically equivalent to
  `tfb.Log()(tfd.Gamma(..))`):

  ```none
  pdf(x; alpha, beta > 0) = exp(x)**(alpha) exp(-exp(x) beta) / Z
  Z = Gamma(alpha) beta**(-alpha)
  ```

  where:

  * `concentration = alpha`, `alpha > 0`,
  * `rate = beta`, `beta > 0`,
  * `Z` is the normalizing constant of the corresponding Gamma distribution, and
  * `Gamma` is the [gamma function](
    https://en.wikipedia.org/wiki/Gamma_function).

  The cumulative density function (cdf) is,

  ```none
  cdf(x; alpha, beta, x) = GammaInc(alpha, beta exp(x)) / Gamma(alpha)
  ```

  where `GammaInc` is the [lower incomplete Gamma function](
  https://en.wikipedia.org/wiki/Incomplete_gamma_function).

  Distribution parameters are automatically broadcast in all functions; see
  examples for details.

  Samples of this distribution are reparameterized (pathwise differentiable).
  The derivatives are computed using the approach described in
  [(Figurnov et al., 2018)][1].

  #### Examples

  ```python
  tfd = tfp.distributions

  dist = tfd.ExpGamma(concentration=3.0, rate=2.0)
  dist2 = tfd.ExpGamma(concentration=[3.0, 4.0], rate=[2.0, 3.0])
  ```

  Compute the gradients of samples w.r.t. the parameters:

  ```python
  concentration = tf.constant(3.0)
  rate = tf.constant(2.0)
  dist = tfd.ExpGamma(concentration, rate)
  with tf.GradientTape() as t:
    t.watch([concentration, rate])
    samples = dist.sample(5)  # Shape [5]
    loss = tf.reduce_mean(tf.square(samples))  # Arbitrary loss function
  # Unbiased stochastic gradients of the loss function
  grads = t.gradient(loss, [concentration, rate])
  ```

  #### References

  [1]: Michael Figurnov, Shakir Mohamed, Andriy Mnih.
       Implicit Reparameterization Gradients. _arXiv preprint arXiv:1805.08498_,
       2018. https://arxiv.org/abs/1805.08498

  """
  def __init__(self, concentration, rate=..., log_rate=..., validate_args=..., allow_nan_stats=..., name=...) -> None:
    """Construct ExpGamma with `concentration` and `rate` parameters.

    The parameters `concentration` and `rate` must be shaped in a way that
    supports broadcasting (e.g. `concentration + rate` is a valid operation).

    Args:
      concentration: Floating point tensor, the concentration params of the
        distribution(s). Must contain only positive values.
      rate: Floating point tensor, the inverse scale params of the
        distribution(s). Must contain only positive values. Mutually exclusive
        with `log_rate`.
      log_rate: Floating point tensor, natural logarithm of the inverse scale
        params of the distribution(s). Mutually exclusive with `rate`.
      validate_args: Python `bool`, default `False`. When `True` distribution
        parameters are checked for validity despite possibly degrading runtime
        performance. When `False` invalid inputs may silently render incorrect
        outputs.
      allow_nan_stats: Python `bool`, default `True`. When `True`, statistics
        (e.g., mean, mode, variance) use the value "`NaN`" to indicate the
        result is undefined. When `False`, an exception is raised if one or
        more of the statistic's batch members are undefined.
      name: Python `str` name prefixed to Ops created by this class.

    Raises:
      TypeError: if `concentration` and `rate` are different dtypes.
    """
    ...
  
  @property
  def concentration(self):
    """Concentration parameter."""
    ...
  
  @property
  def rate(self):
    """Rate parameter."""
    ...
  
  @property
  def log_rate(self):
    """Log-rate parameter."""
    ...
  


class ExpInverseGamma(transformed_distribution.TransformedDistribution):
  """ExpInverseGamma distribution.

  The `ExpInverseGamma` distribution is defined over the real numbers such that
  X ~ ExpInverseGamma(..) => exp(X) ~ InverseGamma(..).

  The distribution is logically equivalent to `tfb.Log()(tfd.InverseGamma(..))`,
  but can be sampled with much better precision.

  #### Mathematical Details

  The probability density function (pdf) is very similar to ExpGamma,

  ```none
  pdf(x; alpha, beta > 0) = exp(-x)**(alpha) exp(-exp(-x) beta) / Z
  Z = Gamma(alpha) beta**(-alpha)
  ```

  where:

  * `concentration = alpha`,
  * `scale = beta`,
  * `Z` is the normalizing constant, and,
  * `Gamma` is the [gamma function](
    https://en.wikipedia.org/wiki/Gamma_function).

  The cumulative density function (cdf) is,

  ```none
  cdf(x; alpha, beta, x) = 1 - GammaInc(alpha, beta exp(-x)) / Gamma(alpha)
  ```

  where `GammaInc` is the [upper incomplete Gamma function](
  https://en.wikipedia.org/wiki/Incomplete_gamma_function).

  Distribution parameters are automatically broadcast in all functions; see
  examples for details.

  Samples of this distribution are reparameterized (pathwise differentiable).
  The derivatives are computed using the approach described in [1].

  #### Examples

  ```python
  tfd = tfp.distributions
  dist = tfd.ExpInverseGamma(concentration=3.0, scale=2.0)
  dist2 = tfd.ExpInverseGamma(concentration=[3.0, 4.0], log_scale=[0.5, -1.])
  ```

  Compute the gradients of samples w.r.t. the parameters:

  ```python
  concentration = tf.constant(3.0)
  log_scale = tf.constant(.5)
  dist = tfd.ExpInverseGamma(concentration, scale)
  with tf.GradientTape() as tape:
    tape.watch([concentration, log_scale])
    samples = dist.sample(5)  # Shape [5]
    loss = tf.reduce_mean(tf.square(samples))  # Arbitrary loss function
  # Unbiased stochastic gradients of the loss function
  grads = tape.gradient(loss, [concentration, log_scale])
  ```

  #### References

  [1]: Michael Figurnov, Shakir Mohamed, Andriy Mnih.
       Implicit Reparameterization Gradients. _arXiv preprint arXiv:1805.08498_,
       2018. https://arxiv.org/abs/1805.08498

  """
  def __init__(self, concentration, scale=..., log_scale=..., validate_args=..., allow_nan_stats=..., name=...) -> None:
    """Construct ExpInverseGamma with `concentration` and `scale` parameters.

    The parameters `concentration` and `scale` (or `log_scale`) must be shaped
    in a way that supports broadcasting (e.g. `concentration + scale` is a valid
    operation).

    Args:
      concentration: Floating point tensor, the concentration params of the
        distribution(s). Must contain only positive values.
      scale: Floating point tensor, the scale params of the distribution(s).
        Must contain only positive values. Mutually exclusive with `log_scale`.
      log_scale: Floating point tensor, the natural logarithm of the scale
        params of the distribution(s). Mutually exclusive with `scale`.
      validate_args: Python `bool`, default `False`. When `True` distribution
        parameters are checked for validity despite possibly degrading runtime
        performance. When `False` invalid inputs may silently render incorrect
        outputs.
      allow_nan_stats: Python `bool`, default `True`. When `True`, statistics
        (e.g., mean, mode, variance) use the value "`NaN`" to indicate the
        result is undefined. When `False`, an exception is raised if one or
        more of the statistic's batch members are undefined.
      name: Python `str` name prefixed to Ops created by this class.


    Raises:
      TypeError: if `concentration`, `scale`, or `log_scale` are different
        dtypes.
    """
    ...
  
  @property
  def concentration(self):
    """Concentration parameter."""
    ...
  
  @property
  def scale(self):
    """Scale parameter."""
    ...
  
  @property
  def log_scale(self):
    """Log of scale parameter."""
    ...
  
  experimental_is_sharded = ...


