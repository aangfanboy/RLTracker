"""
This type stub file was generated by pyright.
"""

"""Functions for computing statistics of samples."""
__all__ = ['auto_correlation', 'cholesky_covariance', 'correlation', 'covariance', 'cumulative_variance', 'log_average_probs', 'stddev', 'variance', 'windowed_mean', 'windowed_variance']
def auto_correlation(x, axis=..., max_lags=..., center=..., normalize=..., name=...):
  """Auto correlation along one axis.

  Given a `1-D` wide sense stationary (WSS) sequence `X`, the auto correlation
  `RXX` may be defined as  (with `E` expectation and `Conj` complex conjugate)

  ```
  RXX[m] := E{ W[m] Conj(W[0]) } = E{ W[0] Conj(W[-m]) },
  W[n]   := (X[n] - MU) / S,
  MU     := E{ X[0] },
  S**2   := E{ (X[0] - MU) Conj(X[0] - MU) }.
  ```

  This function takes the viewpoint that `x` is (along one axis) a finite
  sub-sequence of a realization of (WSS) `X`, and then uses `x` to produce an
  estimate of `RXX[m]` as follows:

  After extending `x` from length `L` to `inf` by zero padding, the auto
  correlation estimate `rxx[m]` is computed for `m = 0, 1, ..., max_lags` as

  ```
  rxx[m] := (L - m)**-1 sum_n w[n + m] Conj(w[n]),
  w[n]   := (x[n] - mu) / s,
  mu     := L**-1 sum_n x[n],
  s**2   := L**-1 sum_n (x[n] - mu) Conj(x[n] - mu)
  ```

  The error in this estimate is proportional to `1 / sqrt(len(x) - m)`, so users
  often set `max_lags` small enough so that the entire output is meaningful.

  Note that since `mu` is an imperfect estimate of `E{ X[0] }`, and we divide by
  `len(x) - m` rather than `len(x) - m - 1`, our estimate of auto correlation
  contains a slight bias, which goes to zero as `len(x) - m --> infinity`.

  Args:
    x:  `float32` or `complex64` `Tensor`.
    axis:  Python `int`. The axis number along which to compute correlation.
      Other dimensions index different batch members.
    max_lags:  Positive `int` tensor.  The maximum value of `m` to consider (in
      equation above).  If `max_lags >= x.shape[axis]`, we effectively re-set
      `max_lags` to `x.shape[axis] - 1`.
    center:  Python `bool`.  If `False`, do not subtract the mean estimate `mu`
      from `x[n]` when forming `w[n]`.
    normalize:  Python `bool`.  If `False`, do not divide by the variance
      estimate `s**2` when forming `w[n]`.
    name:  `String` name to prepend to created ops.

  Returns:
    `rxx`: `Tensor` of same `dtype` as `x`.  `rxx.shape[i] = x.shape[i]` for
      `i != axis`, and `rxx.shape[axis] = max_lags + 1`.

  Raises:
    TypeError:  If `x` is not a supported type.
  """
  ...

def cholesky_covariance(x, sample_axis=..., keepdims=..., name=...):
  """Cholesky factor of the covariance matrix of vector-variate random samples.

  This function can be use to fit a multivariate normal to data.

  ```python
  tf.enable_eager_execution()
  import tensorflow_probability as tfp
  tfd = tfp.distributions

  # Assume data.shape = (1000, 2).  1000 samples of a random variable in R^2.
  observed_data = read_data_samples(...)

  # The mean is easy
  mu = tf.reduce_mean(observed_data, axis=0)

  # Get the scale matrix
  L = tfp.stats.cholesky_covariance(observed_data)

  # Make the best fit multivariate normal (under maximum likelihood condition).
  mvn = tfd.MultivariateNormalTriL(loc=mu, scale_tril=L)

  # Plot contours of the pdf.
  xs, ys = tf.meshgrid(
      tf.linspace(-5., 5., 50), tf.linspace(-5., 5., 50), indexing='ij')
  xy = tf.stack((tf.reshape(xs, [-1]), tf.reshape(ys, [-1])), axis=-1)
  pdf = tf.reshape(mvn.prob(xy), (50, 50))
  CS = plt.contour(xs, ys, pdf, 10)
  plt.clabel(CS, inline=1, fontsize=10)
  ```

  Why does this work?
  Given vector-variate random variables `X = (X1, ..., Xd)`, one may obtain the
  sample covariance matrix in `R^{d x d}` (see `tfp.stats.covariance`).

  The [Cholesky factor](https://en.wikipedia.org/wiki/Cholesky_decomposition)
  of this matrix is analogous to standard deviation for scalar random variables:
  Suppose `X` has covariance matrix `C`, with Cholesky factorization `C = L L^T`
  Then multiplying a vector of iid random variables which have unit variance by
  `L` produces a vector with covariance `L L^T`, which is the same as `X`.

  ```python
  observed_data = read_data_samples(...)
  L = tfp.stats.cholesky_covariance(observed_data, sample_axis=0)

  # Make fake_data with the same covariance as observed_data.
  uncorrelated_normal = tf.random.normal(shape=(500, 10))
  fake_data = tf.linalg.matvec(L, uncorrelated_normal)
  ```

  Args:
    x:  Numeric `Tensor`.  The rightmost dimension of `x` indexes events. E.g.
      dimensions of a random vector.
    sample_axis: Scalar or vector `Tensor` designating axis holding samples.
      Default value: `0` (leftmost dimension). Cannot be the rightmost dimension
        (since this indexes events).
    keepdims:  Boolean.  Whether to keep the sample axis as singletons.
    name: Python `str` name prefixed to Ops created by this function.
          Default value: `None` (i.e., `'covariance'`).

  Returns:
    chol:  `Tensor` of same `dtype` as `x`.  The last two dimensions hold
      lower triangular matrices (the Cholesky factors).
  """
  ...

def covariance(x, y=..., sample_axis=..., event_axis=..., keepdims=..., name=...):
  """Sample covariance between observations indexed by `event_axis`.

  Given `N` samples of scalar random variables `X` and `Y`, covariance may be
  estimated as

  ```none
  Cov[X, Y] := N^{-1} sum_{n=1}^N (X_n - Xbar) Conj{(Y_n - Ybar)}
  Xbar := N^{-1} sum_{n=1}^N X_n
  Ybar := N^{-1} sum_{n=1}^N Y_n
  ```

  For vector-variate random variables `X = (X1, ..., Xd)`, `Y = (Y1, ..., Yd)`,
  one is often interested in the covariance matrix, `C_{ij} := Cov[Xi, Yj]`.

  ```python
  x = tf.random.normal(shape=(100, 2, 3))
  y = tf.random.normal(shape=(100, 2, 3))

  # cov[i, j] is the sample covariance between x[:, i, j] and y[:, i, j].
  cov = tfp.stats.covariance(x, y, sample_axis=0, event_axis=None)

  # cov_matrix[i, m, n] is the sample covariance of x[:, i, m] and y[:, i, n]
  cov_matrix = tfp.stats.covariance(x, y, sample_axis=0, event_axis=-1)
  ```

  Notice we divide by `N`, which does not create `NaN` when `N = 1`, but is
  slightly biased.

  Args:
    x:  A numeric `Tensor` holding samples.
    y:  Optional `Tensor` with same `dtype` and `shape` as `x`.
      Default value: `None` (`y` is effectively set to `x`).
    sample_axis: Scalar or vector `Tensor` designating axis holding samples, or
      `None` (meaning all axis hold samples).
      Default value: `0` (leftmost dimension).
    event_axis:  Scalar or vector `Tensor`, or `None` (scalar events).
      Axis indexing random events, whose covariance we are interested in.
      If a vector, entries must form a contiguous block of dims. `sample_axis`
      and `event_axis` should not intersect.
      Default value: `-1` (rightmost axis holds events).
    keepdims:  Boolean.  Whether to keep the sample axis as singletons.
    name: Python `str` name prefixed to Ops created by this function.
          Default value: `None` (i.e., `'covariance'`).

  Returns:
    cov: A `Tensor` of same `dtype` as the `x`, and rank equal to
      `rank(x) - len(sample_axis) + 2 * len(event_axis)`.

  Raises:
    AssertionError:  If `x` and `y` are found to have different shape.
    ValueError:  If `sample_axis` and `event_axis` are found to overlap.
    ValueError:  If `event_axis` is found to not be contiguous.
  """
  ...

def correlation(x, y=..., sample_axis=..., event_axis=..., keepdims=..., name=...):
  """Sample correlation (Pearson) between observations indexed by `event_axis`.

  Given `N` samples of scalar random variables `X` and `Y`, correlation may be
  estimated as

  ```none
  Corr[X, Y] := Cov[X, Y] / Sqrt(Cov[X, X] * Cov[Y, Y]),
  where
  Cov[X, Y] := N^{-1} sum_{n=1}^N (X_n - Xbar) Conj{(Y_n - Ybar)}
  Xbar := N^{-1} sum_{n=1}^N X_n
  Ybar := N^{-1} sum_{n=1}^N Y_n
  ```

  Correlation is always in the interval `[-1, 1]`, and `Corr[X, X] == 1`.

  For vector-variate random variables `X = (X1, ..., Xd)`, `Y = (Y1, ..., Yd)`,
  one is often interested in the correlation matrix, `C_{ij} := Corr[Xi, Yj]`.

  ```python
  x = tf.random.normal(shape=(100, 2, 3))
  y = tf.random.normal(shape=(100, 2, 3))

  # corr[i, j] is the sample correlation between x[:, i, j] and y[:, i, j].
  corr = tfp.stats.correlation(x, y, sample_axis=0, event_axis=None)

  # corr_matrix[i, m, n] is the sample correlation of x[:, i, m] and y[:, i, n]
  corr_matrix = tfp.stats.correlation(x, y, sample_axis=0, event_axis=-1)
  ```

  Notice we divide by `N` (the numpy default), which does not create `NaN`
  when `N = 1`, but is slightly biased.

  Args:
    x:  A numeric `Tensor` holding samples.
    y:  Optional `Tensor` with same `dtype` and `shape` as `x`.
      Default value: `None` (`y` is effectively set to `x`).
    sample_axis: Scalar or vector `Tensor` designating axis holding samples, or
      `None` (meaning all axis hold samples).
      Default value: `0` (leftmost dimension).
    event_axis:  Scalar or vector `Tensor`, or `None` (scalar events).
      Axis indexing random events, whose correlation we are interested in.
      If a vector, entries must form a contiguous block of dims. `sample_axis`
      and `event_axis` should not intersect.
      Default value: `-1` (rightmost axis holds events).
    keepdims:  Boolean.  Whether to keep the sample axis as singletons.
    name: Python `str` name prefixed to Ops created by this function.
          Default value: `None` (i.e., `'correlation'`).

  Returns:
    corr: A `Tensor` of same `dtype` as the `x`, and rank equal to
      `rank(x) - len(sample_axis) + 2 * len(event_axis)`.

  Raises:
    AssertionError:  If `x` and `y` are found to have different shape.
    ValueError:  If `sample_axis` and `event_axis` are found to overlap.
    ValueError:  If `event_axis` is found to not be contiguous.
  """
  ...

def stddev(x, sample_axis=..., keepdims=..., name=...):
  """Estimate standard deviation using samples.

  Given `N` samples of scalar valued random variable `X`, standard deviation may
  be estimated as

  ```none
  Stddev[X] := Sqrt[Var[X]],
  Var[X] := N^{-1} sum_{n=1}^N (X_n - Xbar) Conj{(X_n - Xbar)},
  Xbar := N^{-1} sum_{n=1}^N X_n
  ```

  ```python
  x = tf.random.normal(shape=(100, 2, 3))

  # stddev[i, j] is the sample standard deviation of the (i, j) batch member.
  stddev = tfp.stats.stddev(x, sample_axis=0)
  ```

  Scaling a unit normal by a standard deviation produces normal samples
  with that standard deviation.

  ```python
  observed_data = read_data_samples(...)
  stddev = tfp.stats.stddev(observed_data)

  # Make fake_data with the same standard deviation as observed_data.
  fake_data = stddev * tf.random.normal(shape=(100,))
  ```

  Notice we divide by `N` (the numpy default), which does not create `NaN`
  when `N = 1`, but is slightly biased.

  Args:
    x:  A numeric `Tensor` holding samples.
    sample_axis: Scalar or vector `Tensor` designating axis holding samples, or
      `None` (meaning all axis hold samples).
      Default value: `0` (leftmost dimension).
    keepdims:  Boolean.  Whether to keep the sample axis as singletons.
    name: Python `str` name prefixed to Ops created by this function.
          Default value: `None` (i.e., `'stddev'`).

  Returns:
    stddev: A `Tensor` of same `dtype` as the `x`, and rank equal to
      `rank(x) - len(sample_axis)`
  """
  ...

def variance(x, sample_axis=..., keepdims=..., name=...):
  """Estimate variance using samples.

  Given `N` samples of scalar valued random variable `X`, variance may
  be estimated as

  ```none
  Var[X] := N^{-1} sum_{n=1}^N (X_n - Xbar) Conj{(X_n - Xbar)}
  Xbar := N^{-1} sum_{n=1}^N X_n
  ```

  ```python
  x = tf.random.normal(shape=(100, 2, 3))

  # var[i, j] is the sample variance of the (i, j) batch member of x.
  var = tfp.stats.variance(x, sample_axis=0)
  ```

  Notice we divide by `N` (the numpy default), which does not create `NaN`
  when `N = 1`, but is slightly biased.

  Args:
    x:  A numeric `Tensor` holding samples.
    sample_axis: Scalar or vector `Tensor` designating axis holding samples, or
      `None` (meaning all axis hold samples).
      Default value: `0` (leftmost dimension).
    keepdims:  Boolean.  Whether to keep the sample axis as singletons.
    name: Python `str` name prefixed to Ops created by this function.
          Default value: `None` (i.e., `'variance'`).

  Returns:
    var: A `Tensor` of same `dtype` as the `x`, and rank equal to
      `rank(x) - len(sample_axis)`
  """
  ...

def cumulative_variance(x, sample_axis=..., name=...):
  """Cumulative estimates of variance.

  Given `N` samples of a scalar-valued random variable `X`, we can compute
  cumulative variance estimates

    result[i] = variance(x[0:i+1])

  in O(N) work and O(log(N)) depth (the length of the longest series
  of operations that are performed sequentially), with O(1) TF kernel
  invocations.  This implementation also arranges to do so in a
  numerically accurate manner, i.e., without incurring a subtraction
  of floating-point numbers of size quadratic in the data `x`.  The
  underlying algorithm is from [1].

  Args:
    x: A numeric `Tensor` holding samples.
    sample_axis: Scalar `Tensor` designating the axis holding samples.
      Other axes are treated in batch.  Default value: `0` (leftmost
      dimension).
    name: Python `str` name prefixed to Ops created by this function.
          Default value: `None` (i.e., `'cumulative_variance'`).

  Returns:
    cum_var: A `Tensor` of same shape and dtype as `x` giving
      cumulative variance estimates.  The zeroth element is the
      variance of a size-1 set of samples, so 0.

  #### References
  [1]: Philippe Pebay. Formulas for Robust, One-Pass Parallel Computation of
       Covariances and Arbitrary-Order Statistical Moments. _Technical Report
       SAND2008-6212_, 2008.
       https://prod-ng.sandia.gov/techlib-noauth/access-control.cgi/2008/086212.pdf

  """
  ...

def windowed_variance(x, low_indices=..., high_indices=..., axis=..., name=...):
  """Windowed estimates of variance.

  Computes variances among data in the Tensor `x` along the given windows:

    result[i] = variance(x[low_indices[i]:high_indices[i]+1])

  accurately and efficiently.  To wit, if K is the size of
  `low_indices` and `high_indices`, and `N` is the size of `x` along
  the given `axis`, the computation takes O(K + N) work, O(log(N))
  depth (the length of the longest series of operations that are
  performed sequentially), and only uses O(1) TensorFlow kernel
  invocations.  The underlying algorithm is an adaptation of the
  streaming reduction for accurate variance computations given in [1].

  This function can be useful for assessing the behavior over time of
  trailing-window estimators from some iterative process, such as the
  last half of an MCMC chain.

  Suppose `x` has shape `Bx + [N] + E`, where the `Bx` component has
  rank `axis`, and `low_indices` and `high_indices` broadcast to shape
  `[M]`.  Then each element of `low_indices` and `high_indices`
  must be between 0 and N+1, and the shape of the output will be
  `Bx + [M] + E`.  Batch shape in the indices is not currently supported.

  The default windows are
  `[0, 1), [1, 2), [1, 3), [2, 4), [2, 5), ...`
  This corresponds to analyzing `x` as though it were streaming, for
  example successive states of an MCMC sampler, and we were interested
  in the variance of the last half of the data at each point.

  Args:
    x: A numeric `Tensor` holding `N` samples along the given `axis`,
      whose windowed variances are desired.
    low_indices: An integer `Tensor` defining the lower boundary
      (inclusive) of each window.  Default: elementwise half of
      `high_indices`.
    high_indices: An integer `Tensor` defining the upper boundary
      (exclusive) of each window.  Must be broadcast-compatible with
      `low_indices`.  Default: `tf.range(1, N+1)`, i.e., N windows
      that each end in the corresponding datum from `x` (inclusive)`.
    axis: Scalar `Tensor` designating the axis holding samples.  This
      is the axis of `x` along which we take windows, and therefore
      the axis that `low_indices` and `high_indices` index into.
      Other axes are treated in batch.  Default value: `0` (leftmost
      dimension).
    name: Python `str` name prefixed to Ops created by this function.
      Default value: `None` (i.e., `'windowed_variance'`).

  Returns:
    variances: A numeric `Tensor` holding the windowed variances of
      `x` along the `axis` dimension.

  #### References
  [1]: Philippe Pebay. Formulas for Robust, One-Pass Parallel Computation of
       Covariances and Arbitrary-Order Statistical Moments. _Technical Report
       SAND2008-6212_, 2008.
       https://prod-ng.sandia.gov/techlib-noauth/access-control.cgi/2008/086212.pdf

  """
  ...

def windowed_mean(x, low_indices=..., high_indices=..., axis=..., name=...):
  """Windowed estimates of mean.

  Computes means among data in the Tensor `x` along the given windows:

    result[i] = mean(x[low_indices[i]:high_indices[i]+1])

  efficiently.  To wit, if K is the size of `low_indices` and
  `high_indices`, and `N` is the size of `x` along the given `axis`,
  the computation takes O(K + N) work, O(log(N)) depth (the length of
  the longest series of operations that are performed sequentially),
  and only uses O(1) TensorFlow kernel invocations.

  This function can be useful for assessing the behavior over time of
  trailing-window estimators from some iterative process, such as the
  last half of an MCMC chain.

  Suppose `x` has shape `Bx + [N] + E`, where the `Bx` component has
  rank `axis`, and `low_indices` and `high_indices` broadcast to shape
  `[M]`.  Then each element of `low_indices` and `high_indices`
  must be between 0 and N+1, and the shape of the output will be
  `Bx + [M] + E`.  Batch shape in the indices is not currently supported.

  The default windows are
  `[0, 1), [1, 2), [1, 3), [2, 4), [2, 5), ...`
  This corresponds to analyzing `x` as though it were streaming, for
  example successive states of an MCMC sampler, and we were interested
  in the variance of the last half of the data at each point.

  Args:
    x: A numeric `Tensor` holding `N` samples along the given `axis`,
      whose windowed means are desired.
    low_indices: An integer `Tensor` defining the lower boundary
      (inclusive) of each window.  Default: elementwise half of
      `high_indices`.
    high_indices: An integer `Tensor` defining the upper boundary
      (exclusive) of each window.  Must be broadcast-compatible with
      `low_indices`.  Default: `tf.range(1, N+1)`, i.e., N windows
      that each end in the corresponding datum from `x` (inclusive).
    axis: Scalar `Tensor` designating the axis holding samples.  This
      is the axis of `x` along which we take windows, and therefore
      the axis that `low_indices` and `high_indices` index into.
      Other axes are treated in batch.  Default value: `0` (leftmost
      dimension).
    name: Python `str` name prefixed to Ops created by this function.
      Default value: `None` (i.e., `'windowed_mean'`).

  Returns:
    means: A numeric `Tensor` holding the windowed means of `x` along
      the `axis` dimension.

  """
  ...

def log_average_probs(logits, sample_axis=..., event_axis=..., keepdims=..., validate_args=..., name=...):
  """Computes `log(average(to_probs(logits)))` in a numerically stable manner.

  The meaning of `to_probs` is controlled by the `event_axis` argument. When
  `event_axis` is `None`, `to_probs = tf.math.sigmoid` and otherwise
  `to_probs = lambda x: tf.math.log_softmax(x, axis=event_axis)`.

  `sample_axis` and `event_axis` should have a null intersection. This
  requirement is always verified when `validate_args` is `True`.

  Args:
    logits: A `float` `Tensor` representing logits.
    sample_axis: Scalar or vector `Tensor` designating axis holding samples, or
      `None` (meaning all axis hold samples).
      Default value: `0` (leftmost dimension).
    event_axis: Scalar or vector `Tensor` designating the axis representing
      categorical logits.
      Default value: `None` (i.e., Bernoulli logits).
    keepdims:  Boolean.  Whether to keep the sample axis as singletons.
      Default value: `False` (i.e., squeeze the reduced dimensions).
    validate_args: Python `bool`, default `False`. When `True` distribution
      parameters are checked for validity despite possibly degrading runtime
      performance. When `False` invalid inputs may silently render incorrect
      outputs.
      Default value: `False` (i.e., do not validate args).
    name: Python `str` name prefixed to Ops created by this function.
      Default value: `None` (i.e., `'log_average_probs'`).

  Returns:
    log_avg_probs: The natural log of the average of probs computed from logits.
  """
  ...

