"""
This type stub file was generated by pyright.
"""

import contextlib

"""Manually implemented special functions.

Normally you'd just use functions coming from the array library you're using,
but on some platforms (like TPU) the default implementations are insufficiently
precise for certain tasks when running under 32 bits (64 bit implementations are
typically okay).

This file provides manual implementations of some special functions.

You can either use these functions directly, or monkey-patch them in via
`patch_manual_special_functions`.
"""
JAX_MODE = ...
__all__ = ['exp_pade_4_4', 'expm1_pade_4_4', 'log1p_pade_4_4', 'log_pade_4_4', 'patch_manual_special_functions', 'reduce_logsumexp', 'softplus']
if JAX_MODE:
  ...
_real_log = ...
def reduce_logsumexp(a, axis=..., keepdims=..., name=...):
  """Like tf.math.reduce_logsumexp.

  This has no functional difference from the regular version, except that it's
  implemented inline here, allowing monkey-patching of the special functions it
  uses (e.g. exp).

  Args:
    a: A tensor.
    axis: Dimensions to reduce. If `None`, reduces all dimensions.
    keepdims: If `True`, retains the reduced dimensions with length 1.
    name: Name for the op.

  Returns:
    y: The reduced tensor.
  """
  ...

def softplus(x, name=...):
  """Like tf.math.reduce_logsumexp.

  This has no functional difference from the regular version, except that it's
  implemented inline here, allowing monkey-patching of the special functions it
  uses (e.g. exp).

  Args:
    x: A Tensor.
    name: Name for the op.

  Returns:
    y: softplus(x)
  """
  ...

def exp_pade_4_4(x, name=...):
  """exp using the Pade(4,4) approximant."""
  ...

def log_pade_4_4(x, name=...):
  """log using the Pade(4,4) approximant."""
  ...

def expm1_pade_4_4(x, name=...):
  """expm1 using the Pade(4,4) approximant."""
  ...

def log1p_pade_4_4(x, name=...):
  """log1p using the Pade(4,4) approximant."""
  ...

@contextlib.contextmanager
def patch_manual_special_functions(): # -> Generator[None, Any, None]:
  """Patches in the manually implemented special functions.

  Normally you'd just use functions coming from the array library you're using,
  but on some platforms (like TPU) the default implementations are
  insufficiently precise for certain tasks when running under 32 bits (64 bit
  implementations are typically okay).

  This patches in manual implementations of those functions which are provide
  higher precision at the cost of speed. The list of affected functions is:

  - `exp`
  - `log`
  - `expm1`
  - `log1p`
  - `logsumexp` (aka `reduce_logsumexp`)
  - `softplus`

  Yields:
    Nothing.
  """
  ...

