"""
This type stub file was generated by pyright.
"""

from tensorflow_probability.python.distributions import joint_distribution
from tensorflow_probability.python.internal import docstring_util

"""A partially pinned, unnormalized JointDistribution-like object."""
CALLING_CONVENTION_DESCRIPTION = ...
UnnormalizedLogProbParts = ...
class JointDistributionPinned:
  """A wrapper class for `JointDistribution` which pins, e.g., the evidence.

  This object is experimental; the API may change without warning.

  Think of this object as `functools.partial` for joint distributions. Sampling
  trims off pinned values (after specifying them as `jd.sample(value=pins)` to
  the underlying distribution). Log-density evaluates the joint probability of
  the given event and the pinned values.

  Mathematically speaking, the object represents a joint probability density,
  `p(x, y)` where the `x` are pinned and the `y` are unpinned. Accordingly, it
  is also proportional to `p(y | x)`, up to a (generally) intractable
  normalizing constant `p(x)`, i.e. `p(x, y) = p(y | x) p(x)`.

  The measure on the unpinned values `y` that this object represents
  is no longer a probability measure, in that the integral of
  `log_prob` over the space of `y` is `p(x)`, which in general is not
  1.  As such, this is not a `tfp.distributions.Distribution`, and
  lacks a `sample` method. In its place, it provides:

  * `sample_unpinned`, `sample_and_log_weight`

  A common use-case with probabilistic inference is writing out a generative
  model to explain some observed data:

  ```python
  jd = tfd.JointDistributionNamed(dict(
    loc = tfd.Normal(0., 1.),
    scale = tfd.Gamma(1., 1.),
    obs = lambda loc, scale: tfd.Normal(loc, scale),
  ))
  ```

  Later, when we want to infer 'typical' values of `loc` and `scale` conditioned
  on some given `data`, we will often write:

  ```python
  def target_log_prob_fn(loc, scale):
    return jd.log_prob(loc=loc, scale=scale, obs=data)
  ```

  This class enables one to write instead:

  ```python
  partial = tfde.JointDistributionPinned(jd, obs=data)
  target_log_prob_fn = partial.log_prob
  ```

  Or, even more concisely `partial = jd.experimental_pin(obs=data)`.

  This is nice, but it wasn't too hard to write out the `target_log_prob_fn`
  function explicitly.

  Now, let's consider that for many inference and optimization methods, we may
  want to use a smooth change of variables to perform inference in the
  unconstrained space of real numbers. In some cases this transformation can be
  parameter-dependent. For example, if we want to unconstrain the support of
  `tfp.distributions.Uniform(-3., 2.)` to the real line, we might use
  `tfp.bijectors.Sigmoid(low=-3., high=2.)`. In support of such use cases,
  most distributions (including the `JointDistribution*` classes) provide a
  `experimental_default_event_space_bijector()` method.

  When these transformations may be dependent on ancestral parts of a joint
  distribution, and some of those parameters may be pinned, it is helpful to
  have a utility class to bridge the gap and provide the multi-part bijective
  transform. This is the "raison d'Ãªtre" of this class.

  The model below is somewhat contrived, but demonstrates the use-case.

  ```python
  tfd = tfp.distributions
  tfde = tfp.experimental.distributions

  n = 75
  dim = 3
  joint = tfd.JointDistributionNamed(dict(
    upper = tfd.Uniform(.4, 1.5),
    concentration = tfd.Gamma(1., .5),
    corr = lambda concentration: tfd.CholeskyLKJ(
        dim, concentration=concentration),
    stddev = lambda upper: tfd.Sample(tfd.Uniform(.2, upper), dim),
    obs = lambda corr, stddev: tfd.Sample(
        tfd.MultivariateNormalTriL(
            loc=tf.zeros([dim]), scale_tril=corr * stddev[..., tf.newaxis]),
        n)
  ))
  fixed_upper = 1.3
  data = joint.sample(upper=fixed_upper)['obs']

  pinned = tfde.JointDistributionPinned(joint, upper=fixed_upper, obs=data)
  bij = pinned.experimental_default_event_space_bijector()
  pulled_back_shape = bij.inverse_event_shape(pinned.event_shape)

  # Fit an ensemble using SGD.
  batch = 16
  uniform_init = tf.nest.map_structure(
      lambda s: tf.random.uniform(tf.concat([[batch], s], axis=0), -2., 2.),
      pulled_back_shape)
  vars = tf.nest.map_structure(tf.Variable, uniform_init)

  opt = tf_keras.optimizers.Adam(.01)

  @tf.function(autograph=False)
  def one_step():
    with tf.GradientTape() as tape:
      lp = pinned.log_prob(bij.forward(vars))
    gradients = tape.gradient(lp, vars)
    opt.apply_gradients(zip(gradients.values(), vars.values()))

  for _ in range(100):
    one_step()

  # Alternatively, sample using MCMC (currently aspirational):
  initial_state = bij.forward(uniform_init)

  kernel = tfp.mcmc.HamiltonianMonteCarlo(
      target_log_prob_fn=pinned.log_prob,
      step_size=.5, num_leapfrog_steps=4)
  # **This line is currently aspirational**, to demonstrate the use-case.
  kernel = tfp.mcmc.TransformedTransitionKernel(kernel, bij)
  tfp.mcmc.sample_chain(10, kernel=kernel, current_state=initial_state)
  ```
  """
  def __init__(self, distribution, *pins, name=..., **named_pins) -> None:
    """Constructs a `JointDistributionPinned`.

    ### Examples:

    ```
    JointDistributionPinned(
       tfd.JointDistributionSequential([
           tfd.Gamma(1., 1.), lambda scale: tfd.Normal(0., scale, name='x')]),
       1.)  # Pins the `scale` part.

    JointDistributionPinned(
       tfd.JointDistributionSequential([
           tfd.Gamma(1., 1.), lambda scale: tfd.Normal(0., scale, name='x')]),
       [1.])  # Pins the `scale` part to scalar `1.`.

    JointDistributionPinned(
       tfd.JointDistributionSequential([
           tfd.Gamma(1., 1.), lambda scale: tfd.Normal(0., scale, name='x')]),
       None, 1.)  # Pins the `x` part.

    JointDistributionPinned(
       tfd.JointDistributionSequential([
           tfd.Gamma(1., 1.), lambda scale: tfd.Normal(0., scale, name='x')]),
       [None, 1.])  # Pins the `x` part.

    JointDistributionPinned(
       tfd.JointDistributionSequential([
           tfd.Gamma(1., 1.), lambda scale: tfd.Normal(0., scale, name='x')]),
       scale=1.)  # Pins the `scale` part.

    JointDistributionPinned(
       tfd.JointDistributionSequential([
           tfd.Gamma(1., 1.), lambda scale: tfd.Normal(0., scale, name='x')]),
       x=1.)  # Pins the `x` part.
    ```

    Args:
      distribution: A `tfp.distributions.JointDistribution`.
      *pins: A single object like the `value` argument that may be passed into
        `JointDistribution.sample` (some parts may be `None`), or a sequence of
        objects similar to such sequence as might be passed to
        `JointDistribution.log_prob`, but with the difference that some parts
        may be `None` (`log_prob` would require all parts be specified).
        More precisely, the user may pass (A) a single argument specifiying pins
        of one or more of the parts of the underlying `distribution` either by
        name (i.e. a `dict`, `namedtuple`) or by sequence ordering (`tuple`,
        `list`), or (B) a sequence of arguments which align with the model of
        the underlying distribution (which must be ordered). It is an error to
        use an unordered sequence of pins with an unordered model, e.g. a
        `tfp.distributions.JointDistributionNamed` constructed with a `dict`
        model (`collections.OrderedDict` is allowed).
      name: Python `str` name for this distribution. If `None`, defaults to
        'Pinned{distribution.name}'.
        Default value: `None`.
      **named_pins: Named elements to pin. The names given must align with the
        part names defined by `distribution._flat_resolve_names()`, i.e. either
        the explicitly named parts of `tfp.distributions.JointDistributionNamed`
        or the `name` parameters passed to distributions constructed by the
        model given to `JointDistribution*`.
    """
    ...
  
  @property
  def distribution(self): # -> Any:
    """The underlying distribution being partially pinned."""
    ...
  
  @property
  def name(self): # -> str:
    """Name of this pinned distribution."""
    ...
  
  @property
  def pins(self): # -> dict[Any, Any] | dict[str | Any, Any | defaultdict[Any, Any] | list[Any] | None] | dict[Any, Any | defaultdict[Any, Any] | list[Any] | None]:
    """Dictionary of pins resolved to names."""
    ...
  
  @property
  def use_vectorized_map(self): # -> Any | bool:
    """Whether the underlying distribution relies on automatic vectorization."""
    ...
  
  @property
  def validate_args(self):
    ...
  
  @property
  def dtype(self):
    """DType of unpinned parts."""
    ...
  
  @property
  def reparameterization_type(self): # -> dict[Any, Any] | Any:
    ...
  
  @property
  def event_shape(self): # -> dict[Any, Any] | Any:
    """Statically resolvable event shapes of unpinned parts."""
    ...
  
  def event_shape_tensor(self): # -> dict[Any, Any] | Any:
    """Dynamic/graph Tensor event shapes of unpinned parts."""
    ...
  
  @property
  def batch_shape(self): # -> dict[Any, Any] | Any:
    ...
  
  def batch_shape_tensor(self): # -> dict[Any, Any] | Any:
    ...
  
  __str__ = ...
  __repr__ = ...
  def experimental_default_event_space_bijector(self, *args, **kwargs): # -> _DefaultJointBijectorAutoBatchedWithPins:
    """A bijector to pull back unpinned values to unconstrained reals."""
    ...
  
  def experimental_pin(self, *args, **kwargs): # -> JointDistributionPinned:
    """Logical equivalent of `JointDistribution.experimental_pin`.

    For example
    ```
    @tfd.JointDistributionCoroutine
    def model():
        x = yield tfd.Normal(0, 1, name='x'),
        y = yield tfd.Normal(0, 1, name='y'),
        yield tfd.Normal(0, 1, name='z')
    model.experimental_pin(z=1.).experimental_pin(y=.5).event_shape
    # => StructTuple(x=[])
    ```

    Args:
      *args: Positional arguments: a value structure or component values.
      **kwargs: Keyword arguments: a value structure or component values.
        May also include `name`, specifying a Python string name for ops
        generated by this method.

    Returns:
      pinned: a `tfp.experimental.distributions.JointDistributionPinned` with
        the given values pinned in addition to those pins already specified on
        `self`.

    """
    ...
  
  def unpin(self, *pinned_part_names): # -> Self | Any | JointDistributionPinned:
    """Unpins selected parts, returning a new instance.

    Args:
      *pinned_part_names: One or more `str` names of parts to unpin.

    Returns:
      jd: A joint distribution with the specified pins dropped. If all pins are
        dropped, the underlying joint distribution is returned.
    """
    ...
  
  def sample_unpinned(self, sample_shape=..., seed=...): # -> dict[Any, Any] | Any:
    """Draws unnormalized samples using ancestral sampling.

    This produces the same distribution on outputs as calling
    `underlying.sample(value=pins)`, then stripping away the pinned
    parts.

    In the probability literature, this operation corresponds to
    sampling from the do-calculus expression
    `underlying(Unpinned|do(Pinned = pins))`, where the assumed causal
    structure of `underlying` is given by its data dependence graph.

    Note that this is not the same measure as is represented by the
    `log_prob` method of this class, which is why this method is not
    called `sample`.

    Args:
      sample_shape: Shape prefix to use when sampling.
      seed: PRNG seed; see `tfp.random.sanitize_seed` for details.

    Returns:
      samples: unpinned parts sampled from the underlying distribution.
    """
    ...
  
  def experimental_sample_and_log_prob(self, sample_shape=..., seed=..., name=..., **kwargs): # -> tuple[dict[Any, Any] | Any, Any]:
    ...
  
  def sample_and_log_weight(self, sample_shape=..., seed=...): # -> tuple[dict[Any, Any] | Any, int]:
    """Draws unnormalized samples and their log-weights with ancestral sampling.

    This method provides the ancestral importance sampler for the
    non-probability measure on the unpinned space represented by this
    object.  To wit, we draw a sample with the same distribution as
    `sample_unpinned`, and return it together with a weight given by
    the log-probability of the pinned parts.

    Denoting the unpinned space by `Y`, the samples by `y`, their
    corresponding log-weights by `w`, and the measure defined by this
    object by `m`, the spec for this method is that, for any
    real-valued function `f : Y -> R`,

      int_Y f(y) dm = E [f(y) * exp(w)].

    Args:
      sample_shape: Shape prefix to use when sampling.
      seed: PRNG seed; see `tfp.random.sanitize_seed` for details.

    Returns:
      samples: unpinned parts ancestrally sampled from the pinned distribution.
      log_weights: log-weight of the sample. (Log-probability of the pinned
        parts at the sampled location.)
    """
    ...
  
  @docstring_util.expand_docstring(calling_convention_description=CALLING_CONVENTION_DESCRIPTION.format(method='log_weight', method_abbr='log_wt'))
  def log_weight(self, *args, **kwargs): # -> int:
    """Computes the log relative weight of the given sample.

    This function computes the log-probability of the pinned parts at the given
    location, ignoring the probability of the unpinned parts.

    ${calling_convention_description}

    Returns:
      log_weights: log-weight of the given point, i.e. the log pinned evidence.
    """
    ...
  
  @docstring_util.expand_docstring(calling_convention_description=CALLING_CONVENTION_DESCRIPTION.format(method='log_prob', method_abbr='lp'))
  def log_prob(self, *args, **kwargs):
    """Computes the log-density of this measure at the given point.

    The measure on the space of unpinned values that is represented by
    this object is not a probability measure, so the values of
    `log_prob` will in general not integrate to 1.

    ${calling_convention_description}

    Returns:
      log_prob: The joint log-probability of `*xs` or `**kwargs`
        with the pinned parts. It is unnormalized with respect to `*xs` or
        `**kwargs`.
    """
    ...
  
  @docstring_util.expand_docstring(calling_convention_description=CALLING_CONVENTION_DESCRIPTION.format(method='unnormalized_log_prob', method_abbr='lp'))
  def unnormalized_log_prob(self, *args, **kwargs):
    """Computes the log-density of this measure at the given point.

    The measure on the space of unpinned values that is represented by
    this object is not a probability measure, so the values of
    `unnormalized_log_prob` will in general not integrate to 1.

    This method currently computes the same values as `log_prob`, but
    may be modified in the future to omit normalization constants.

    ${calling_convention_description}

    Returns:
      log_prob: The joint log-probability of `*xs` or `**kwargs`
        with the pinned parts. It is unnormalized with respect to `*xs` or
        `**kwargs`.
    """
    ...
  
  @docstring_util.expand_docstring(calling_convention_description=CALLING_CONVENTION_DESCRIPTION.format(method='log_prob_parts', method_abbr='lp_parts'))
  def log_prob_parts(self, *args, **kwargs): # -> UnnormalizedLogProbParts:
    """Computes the log-probability of each part.

    The measure on the space of unpinned values that is represented by
    this object is not a probability measure, so the values produced
    by `log_prob_parts` will in general not integrate to 1.

    ${calling_convention_description}

    Returns:
      pinned: partial log-prob of each pinned part
      unpinned: partial log-prob of each unpinned part
    """
    ...
  
  @docstring_util.expand_docstring(calling_convention_description=CALLING_CONVENTION_DESCRIPTION.format(method='unnormalized_log_prob_parts', method_abbr='lp_parts'))
  def unnormalized_log_prob_parts(self, *args, **kwargs): # -> UnnormalizedLogProbParts:
    """Computes the log-probability of each part.

    The measure on the space of unpinned values that is represented by
    this object is not a probability measure, so the values produced
    by `unnormalized_log_prob_parts` will in general not integrate to
    1.

    This method currently computes the same values as
    `log_prob_parts`, but may be modified in the future to omit
    normalization constants.

    ${calling_convention_description}

    Returns:
      pinned: partial log-prob of each pinned part
      unpinned: partial log-prob of each unpinned part
    """
    ...
  
  @property
  def experimental_shard_axis_names(self): # -> dict[Any, Any] | Any:
    ...
  


class _DefaultJointBijectorAutoBatchedWithPins(joint_distribution._DefaultJointBijectorAutoBatched):
  """Bijector that auto-batches over both its inputs *and* any pinned values."""
  ...


