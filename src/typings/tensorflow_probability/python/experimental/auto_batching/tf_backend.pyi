"""
This type stub file was generated by pyright.
"""

import collections

"""TensorFlow (graph) backend for auto-batching VM.

Implements VM variable stack and registers backed by TF `Tensor`s.
"""
__all__ = ['TensorFlowBackend']
class RegisterTensorFlowVariable(collections.namedtuple('RegisterTensorFlowVariable', ['value'])):
  """A register-only variable.

  Efficiently stores and updates values whose lifetime does not cross function
  calls (and therefore does not require a stack).  This is different from
  `TemporaryVariable` because it supports crossing basic block boundaries.  A
  `RegisterTensorFlowVariable` therefore needs to store its content persistently
  across the `while_loop` in `execute`, and to handle divergence (and
  re-convergence) of logical threads.
  """
  def update(self, value, mask): # -> Self:
    """Update with `value` at `mask`, propagate other positions."""
    ...
  
  def push(self, mask): # -> Self:
    ...
  
  def read(self):
    ...
  
  def pop(self, mask): # -> Self:
    ...
  
  def ensure_initialized(self): # -> Self:
    ...
  


class Stack(collections.namedtuple('Stack', ['stack', 'stack_index'])):
  """Immutable, internal container for a fixed size stack.

  The implementation is backed by a `Tensor` each for the stack and the
  (batched) stack pointer.

  As a namedtuple, it can be directly passed through TF's nest library for
  flattening and restructuring as an element passed to e.g. a TF while loop.
  """
  def pop(self, mask, name=...): # -> tuple[Self, Any]:
    """Pops each indicated batch member, returns the new top of the stack.

    Does not mutate `self`.

    Args:
      mask: Boolean `Tensor` of shape `[batch_size]`. The stack frames at `True`
        indices of `mask` are regressed; the others are unchanged.
      name: Optional name for this op.

    Returns:
      new_stack: A new stack whose frames have been regressed where indicated
          by `mask`.
      read: The batch of values at the newly-current stack frame.
    """
    ...
  
  def push(self, value, mask, name=...): # -> tuple[Self, Any]:
    """Pushes `value` onto the stack, advances frame of batch members in `mask`.

    In this impl, we update each thread's top-of-stack (regardless of `mask`) to
    the corresponding `value`, then advance the stack pointers of only those
    threads indicated by `mask`.

    Args:
      value: `Tensor` having the shape of a single batch of the variable.
      mask: Boolean `Tensor` of shape `[batch_size]`. Threads at `True` indices
          of `mask` have their stack frames advanced; the others remain.
      name: Optional name for this op.

    Returns:
      stack: Updated stack. Does not mutate `self`.
      asserted_value: A assertion-bound snapshot of the input `value`,
          assertions used to catch stack overflows.
    """
    ...
  


class UnsafeStack(Stack):
  """Stack with runtime assertions disabled."""
  ...


class FullTensorFlowVariable(collections.namedtuple('FullTensorFlowVariable', ['current', 'stack'])):
  """An immutable register + stack backed by batched TF `Tensor`s.

  All state-changing methods return new Variable instances.

  The register is used to make reads from and writes to the top of the stack
  cheaper than they would be otherwise, i.e. save slice updates.

  As a namedtuple, the variable can be passed through the TF nest library as
  part of the structure handed to/returned from the body of a while_loop, or
  even a Session.run call.
  """
  def read(self, name=...):
    """Returns the batch of top values."""
    ...
  
  def update(self, value, mask, name=...): # -> Self:
    """Updates the variable at the indicated places.

    Args:
      value: Array of shape `[batch_size, ...]` of data to update with.
        Indices in the first dimension corresponding to `False`
        entries in `mask` are ignored.
      mask: Boolean array of shape `[batch_size]`. The values at `True`
        indices of `mask` are updated; the others remain.
      name: Optional name for this op.

    Returns:
      var: Updated variable. Does not mutate `self`.
    """
    ...
  
  def push(self, mask, name=...): # -> Self:
    """Pushes each indicated batch member, making room for a new write.

    The new top value is the same as the old top value (this is a
    "duplicating push").

    Args:
      mask: Boolean array of shape `[batch_size]`. The values at `True`
        indices of `mask` are updated; the others remain.
      name: Optional name for this op.

    Returns:
      var: Updated variable. Does not mutate `self`.
    """
    ...
  
  def pop(self, mask, name=...): # -> Self:
    """Pops each indicated batch member, restoring a previous write.

    Args:
      mask: Boolean `Tensor` of shape `[batch_size]`. The values at `True`
        indices of `mask` are updated; the others are unchanged.
      name: Optional name for this op.

    Returns:
      var: Updated variable. Does not mutate `self`.
    """
    ...
  


class TensorFlowBackend:
  """Implements the TF backend ops for a PC auto-batching VM."""
  def __init__(self, safety_checks=..., while_parallel_iterations=..., while_maximum_iterations=..., basic_block_xla_device=...) -> None:
    """Construct a new backend instance.

    Args:
      safety_checks: Python `bool` indicating whether we should use runtime
        assertions to detect stack overflow/underflow.
      while_parallel_iterations: Python `int`, the argument to pass along to
        `tf.while_loop(..., parallel_iterations=while_parallel_iterations)`
      while_maximum_iterations: Python `int` or None, the argument to pass along
        to `tf.while_loop(..., maximum_iterations=while_maximum_iterations)`
      basic_block_xla_device: Python `str` indicating the device to which basic
        blocks should be targeted (i.e. 'CPU:0' or 'GPU:0'); if not None.
    """
    ...
  
  @property
  def variable_class(self): # -> tuple[type[NullVariable], type[TemporaryVariable], type[RegisterTensorFlowVariable], type[FullTensorFlowVariable]]:
    ...
  
  def type_of(self, t, dtype_hint=...): # -> TensorType:
    """Returns the `instructions.Type` of `t`.

    Args:
      t: `tf.Tensor` or a Python or numpy constant.
      dtype_hint: dtype to prefer, if `t` is a constant.

    Returns:
      vm_type: `instructions.TensorType` describing `t`.
    """
    ...
  
  def run_on_dummies(self, primitive_callable, input_types):
    """Runs the given `primitive_callable` with dummy input.

    This is useful for examining the outputs for the purpose of type inference.

    Args:
      primitive_callable: A python callable.
      input_types: `list` of `instructions.Type` type of each argument to the
        callable.  Note that the contained `TensorType` objects must match the
        dimensions with which the primitive is to be invoked at runtime, even
        though type inference conventionally does not store the batch dimension
        in the `TensorType`s.

    Returns:
      outputs: pattern of backend-specific objects whose types may be
        analyzed by the caller with `type_of`.
    """
    ...
  
  def merge_dtypes(self, dt1, dt2):
    """Merges two dtypes, returning a compatible dtype.

    In practice, TF implementation asserts that the two dtypes are identical.

    Args:
      dt1: A numpy dtype, or None.
      dt2: A numpy dtype, or None.

    Returns:
      dtype: The common numpy dtype.

    Raises:
      ValueError: If dt1 and dt2 are not equal and both are non-`None`.
    """
    ...
  
  def merge_shapes(self, s1, s2): # -> tuple[Any, ...] | None:
    """Merges two shapes, returning a broadcasted shape.

    Args:
      s1: A `list` of Python `int` or None.
      s2: A `list` of Python `int` or None.

    Returns:
      shape: A `list` of Python `int` or None.

    Raises:
      ValueError: If `s1` and `s2` are not broadcast compatible.
    """
    ...
  
  def assert_matching_dtype(self, expected_dtype, value, message=...): # -> None:
    """Asserts that the dtype of `value` matches `expected_dtype`.

    Args:
      expected_dtype: A numpy dtype
      value: `Tensor` or convertible.
      message: Optional diagnostic message.

    Raises:
      ValueError: If dtype does not match.
    """
    ...
  
  def batch_size(self, value, name=...): # -> Literal[1]:
    """Returns the first (batch) dimension of `value`."""
    ...
  
  def static_value(self, t): # -> None:
    """Gets the eager/immediate value of `t`, or `None` if `t` is a Tensor."""
    ...
  
  def fill(self, value, size, dtype, shape, name=...):
    """Fill a fresh batched Tensor of the given shape and dtype with `value`.

    Args:
      value: Scalar to fill with.
      size: Scalar `int` `Tensor` specifying the number of VM threads.
      dtype: `tf.DType` of the zeros to be returned.
      shape: Rank 1 `int` `Tensor`, the per-thread value shape.
      name: Optional name for the op.

    Returns:
      result: `Tensor` of `dtype` `value`s with shape `[size, *shape]`
    """
    ...
  
  def create_variable(self, name, alloc, type_, max_stack_depth, batch_size): # -> NullVariable | TemporaryVariable | NamedVariable:
    """Returns an intialized Variable.

    Args:
      name: Name for the variable.
      alloc: `VariableAllocation` for the variable.
      type_: `instructions.TensorType` describing the sub-batch shape and dtype
        of the variable being created.
      max_stack_depth: Scalar `int` `Tensor`, the maximum stack depth allocated.
      batch_size: Scalar `int` `Tensor`, the number of parallel threads being
        executed.

    Returns:
      var: A new, initialized Variable object.
    """
    ...
  
  def full_mask(self, size, name=...):
    """Returns an all-True mask `Tensor` with shape `[size]`."""
    ...
  
  def broadcast_to_shape_of(self, val, target, name=...):
    """Broadcasts val to the shape of target.

    Attempts to match the dtype of `broadcast_val` to the dtype of `target`, if
    `val` is not a `Tensor` and `target` has a dtype.

    Args:
      val: The value to be broadcast. Must be broadcast-compatible with
        `target`.
      target: `Tensor` whose shape we will broadcast `val` to match.
      name: Optional name for the op.

    Returns:
      broadcast_val: A `Tensor` with shape matching `val + target`. Provided
        that `val`'s dimension sizes are all smaller or equal to `target`'s, the
        returned value will be the shape of `target`.
    """
    ...
  
  def cond(self, pred, true_fn, false_fn, name=...):
    """Implements a conditional operation for the backend.

    Args:
      pred: A boolean scalar `Tensor` indicating the condition.
      true_fn: A callable accepting and returning nests of `Tensor`s having
        the same structure as `state`, to be executed when `pred` is True.
      false_fn: A callable accepting and returning nests of `Tensor`s having
        the same structure as `state`, to be executed when `pred` is False.
      name: Optional name for the op.

    Returns:
      state: Output state, matching nest structure of input argument `state`.
    """
    ...
  
  def prepare_for_cond(self, state):
    """Backend hook for preparing Tensors for `cond`.

    The TensorFlow backend uses this hook to apply `tf.convert_to_tensor` before
    entering the cond tree generated by `virtual_machine._staged_apply`.  One
    could do this inside `cond`, but when this API element was defined there
    seemed to be a performance reason (for Eager mode) to do it once per cond
    tree rather than once per cond.

    Args:
      state: A state to be prepared for use in conditionals.

    Returns:
      state: The prepared state.
    """
    ...
  
  def where(self, condition, x, y, name=...):
    """Implements a where selector for the TF backend.

    Attempts to match the dtypes of the value operands, if they are not yet both
    `Tensor`s.

    Args:
      condition: A boolean `Tensor`, either a vector having length
        `(x + y).shape[0]` or matching the full shape of `x + y`.
      x: `Tensor` of values to take when `condition` is `True`. Shape must match
        that of `y`.
      y: `Tensor` of values to take when `condition` is `False`. Shape must
        match that of `x`.
      name: Optional name for the op.

    Returns:
      masked: A broadcast-shaped `Tensor` where elements corresponding to `True`
        values of `condition` come from `x`, and others come from `y`.
    """
    ...
  
  def reduce_min(self, t, name=...):
    """Implements reduce_min for TF backend."""
    ...
  
  def while_loop(self, cond, body, loop_vars, name=...):
    """Implements while loops for TF backend."""
    ...
  
  def switch_case(self, branch_selector, branch_callables, name=...):
    """Implements a switch (branch_selector) { case ... } construct."""
    ...
  
  def equal(self, t1, t2, name=...):
    """Implements equality comparison for TF backend."""
    ...
  
  def not_equal(self, t1, t2, name=...):
    """Implements inequality comparison for TF backend."""
    ...
  
  def any(self, t, name=...):
    ...
  
  def wrap_straightline_callable(self, f): # -> Callable[..., Any]:
    """Method exists solely to be stubbed, i.e. for defun + XLA compile."""
    ...
  


