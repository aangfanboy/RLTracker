"""
This type stub file was generated by pyright.
"""

import collections

"""Instruction language for auto-batching virtual machine."""
__all__ = ['Program', 'TensorType', 'Type', 'VariableAllocation', 'ControlFlowGraph', 'Block', 'Function', 'FunctionCallOp', 'PrimOp', 'PopOp', 'BranchOp', 'GotoOp', 'PushGotoOp', 'IndirectGotoOp', 'pc_var', 'push_op', 'halt_op', 'interpret', 'extract_referenced_variables']
class Program:
  """An auto-batchable program.

  The primary ingredient of a Program is the control flow graph of
  operations to perform.  The operation language is a union that
  serves two purposes: one subset is designed to be convenient to run
  in Single Instruction Multiple Thread style, and the other to
  generate from an upstream Python-embedded DSL.

  As such, there are operations for explicit control transfers and
  stack management, as well as for interpreted function calls (pending
  lowering to explicit control transfers).  The primitive computations
  are encapsulated from the interpreter as Python functions.  It is
  not expected that one would author programs in this operation
  language directly, but rather generate them with an appropriate
  compiler.

  Lowering consists of eliminating `FunctionCallOp` in favor of a
  specific sequence of lower-level instructions.  A few choices for
  the lowered language may register as somewhat nonstandard:
  - The variable name space is global; the upstream compiler is
    expected to generate unique variable names.
  - There is no one stack; instead, every variable has its own stack.
    This reduces push traffic, because only the variables that are
    actually written to are ever pushed.
  - Return addresses for pending lowered function calls are stored in
    a reserved variable, that is otherwise in the same environment as
    the user variables.
  - There are no designated registers for function arguments or return
    values.  This is because all runtime storage is in Tensors, which
    need to have fixed types.  Instead, it is the (post-lowering)
    caller's responsibility to write the arguments into the formal
    parameters and to retrieve the returned value(s) from the
    variable(s) in which the callee left them.

  The post-lowering function call sequence is
  - Push the arguments to the formal parameters;
  - Pop any argument variables that are no longer used;
  - Store the desired return address and jump to the beginning of the function's
    body (with a single `PushGotoOp`);
  - When the function returns by executing `IndirectGotoOp`, assign the
    returned values to the variables that should receive them; and
  - Pop the variables holding the returned values.

  Note that this sequence requires that all calls in the source
  language be to statically known functions, and for every function to
  leave its results in the same variable(s) on every call (regardless
  of internal control flow).
  """
  def __init__(self, graph, functions, var_defs, vars_in, vars_out, var_alloc=...) -> None:
    """Initialize a new `Program`.

    Args:
      graph: A `ControlFlowGraph`.  This is the graph of basic blocks
        to execute.
      functions: A list of `Function`s giving the definitions of all
        the auto-batchable functions this `Program` may (recursively)
        call.
      var_defs: A dict mapping variable names to `Type` objects
        giving their pattern of held Tensors.  Each leaf of the pattern
        is a `TensorType` object giving the dtype and shape of that leaf.
        The shape excludes the batch dimension.
      vars_in: A list of the names of the variables in which to store
        the inputs when starting.
      vars_out: A pattern of the names of the variables from which to
        read the outputs when finished.
      var_alloc: A dict mapping variable names to allocation strategies (see
        `VariableAllocation`).  The semantics of an entry are "A proof has been
        found that this strategy suffices for this variable."
    """
    ...
  
  def replace(self, var_defs=..., var_alloc=...): # -> Program:
    """Return a copy of `self` with `var_defs` and/or `var_alloc` replaced."""
    ...
  
  def main_function(self): # -> Function:
    """Return a representation of the main program as a `Function`."""
    ...
  
  def __str__(self, indent=..., width=...) -> str:
    ...
  


class TensorType(collections.namedtuple('TensorType', ['dtype', 'shape'])):
  __slots__ = ...
  def __str__(self) -> str:
    ...
  
  def __repr__(self): # -> str:
    ...
  


Type = ...
def single_type(dtype, shape): # -> Type:
  ...

class Module:
  """A module of related auto-batchable functions.

  A `Module` stores multiple `Function`s that may call one another (possibly
  recursively).  A `Module` differs from a `Program` in that it has no
  pre-specified entry point.
  """
  def __init__(self, functions, var_defs) -> None:
    ...
  
  def lookup(self, func_name): # -> None:
    ...
  
  def program(self, main): # -> Program:
    """Returns a `Program` corresponding to entering this `Module` at `main`."""
    ...
  


class VariableAllocation:
  """A token indicating how to allocate memory for an autobatched variable.

  In general, a variable holding data with a longer or more complex lifetime
  will need a more expensive storage strategy.

  Specifically, the four variable allocation strategies are:
  - `NULL`: Holds nothing.  Drops writes, raises on reads.  Useful for
    representing dummy variables that the user program never reads.
  - `TEMPORARY`: Holds one value per thread, but not across basic block
    boundaries.  Only usable for temporaries that live in a single basic block,
    and thus never experience joins (or vm execution loop crossings).  For such
    a variable, `push` just overwrites the whole Tensor; `pop` nulls the whole
    Tensor out.
  - `REGISTER`: Holds one value per thread, with no associated stack.  Useful
    for representing temporaries that do not cross (recursive) function calls,
    but do span multiple basic blocks.  For such a variable, `push` amounts to a
    `where`, with an optional runtime safety check for overwriting a defined
    value.
  - `FULL`: Holds a complete stack for each thread.  Used as a last resort, when
    a stack is unavoidable.

  The difference between `register` and `temporary` is that `register` is a
  `[batch_size] + event_shape` Tensor in the loop state of the toplevel
  `while_loop`, whereas `temporary` is represented as an empty tuple in the loop
  state, and only holds a Tensor during the execution of the
  `virtual_machine._run_block` call that uses it.  Consequently, `register`
  updating involves a `where`, but writing to a `temporary` produces 0 TF ops.
  Also, in the (envisioned) gather-scatter batching mode, the `temporary` Tensor
  will automatically only hold data for the live threads, whereas reading and
  writing a `register` will still require the gathers and scatters.
  """
  def __init__(self, name) -> None:
    ...
  
  def __str__(self) -> str:
    ...
  
  def __repr__(self): # -> str:
    ...
  


class ControlFlowGraph:
  """A control flow graph (CFG)."""
  def __init__(self, blocks) -> None:
    """A control flow graph (CFG).

    A CFG is a set of basic `Block`s available in a program.  In this
    system, the `Block`s are ordered and indexed to support VM
    instruction selection, so the CFG also keeps the reverse map from
    `Block`s to their indexes.

    Args:
      blocks: Python list of `Block` objects, the content of the CFG.
        Any terminator instructions of said `Block` objects should
        refer to other `Block`s in the same CFG.  Otherwise,
        downstream passes or staging may fail.
    """
    ...
  
  @property
  def blocks(self): # -> Any:
    ...
  
  def block(self, index):
    """Returns the `Block` given by the input `index`.

    Args:
      index: A Python `int`.

    Returns:
      block: The `Block` at that location in the block list.
    """
    ...
  
  def block_index(self, block): # -> int:
    """Returns the `int` index of the given `Block`.

    Args:
      block: The block to look up. If `None`, returns the exit index.

    Returns:
      index: Python `int`, the index of the requested block.
    """
    ...
  
  def exit_index(self): # -> int:
    """Returns the `int` index denoting "exit this CFG"."""
    ...
  
  def enter_block(self):
    """Returns the entry `Block`."""
    ...
  
  def __str__(self, indent=..., width=...) -> str:
    ...
  


class Block:
  """A basic block."""
  def __init__(self, instructions=..., terminator=..., name=...) -> None:
    """Initialize a `Block`.

    A basic block is a sequence of instructions that are always executed
    in order from first to last.  After the last instruction is executed,
    the block transfers control as indicated by the control transfer
    instruction in the `terminator` field.

    Pre-lowering, `FunctionCallOp` is admissible as an internal
    instruction in a `Block`, on the grounds that it returns to a fixed
    place, and the block is guaranteed to continue executing.

    Args:
      instructions: A list of `PrimOp`, `PopOp`, and `FunctionCallOp`
        instructions to execute in order.  Control transfer instructions (that
        do not return) are not permitted in this list.
      terminator: A single `BranchOp`, `GotoOp`, `PushGotoOp` or
        `IndirectGotoOp`, indicating how to transfer control out of this basic
        block.
      name: An object serving as the name of this `Block`, for display.
    """
    ...
  
  def assign_instructions(self, instructions): # -> None:
    """Assigns the body `instructions` and the `terminator` at once.

    This is a convenience method, to set a `Block`'s program content
    in one invocation instead of having to assign the `instructions`
    and the `terminator` fields separately.

    Args:
      instructions: A non-empty Python list of `Op` objects.  The last one must
        be a `BranchOp`, `GotoOp`, `PushGotoOp`, or `IndirectGotoOp`, and
        becomes the `terminator`.  The others, if any, must be `PrimOp`,
        `PopOp`, or `FunctionCallOp`, and become the `instructions`, in order.
    """
    ...
  
  @property
  def label_str(self): # -> str:
    """A string suitable for referring to this `Block` in printed output."""
    ...
  
  def __str__(self, indent=..., width=...) -> str:
    ...
  


class Function:
  """A function subject to auto-batching, callable with `FunctionCallOp`."""
  def __init__(self, graph, vars_in, vars_out, type_inference, name=...) -> None:
    """A `Function` is a control flow graph with input and output variables.

    Args:
      graph: A `ControlFlowGraph` comprising the function's body.
      vars_in: List of `string` giving the names of the formal parameters
        of the function.
      vars_out: Pattern of `string` giving the name(s) of the variables
        the function returns.  Ergo, functions must be canonicalized to
        place the return value(s) in the same-named variable(s) along
        every path to the exit.
      type_inference: A callable which takes a list of patterns of `TensorType`s
        corresponding to the data types of `vars_in`.  This callable must
        return a pattern of `TensorType`s corresponding to the structure
        assembled by the `return_vars`.
      name: Optional string denoting this `Function` in printed output.
    """
    ...
  
  @property
  def name(self): # -> str | None:
    ...
  
  def __str__(self, indent=..., width=...) -> str:
    ...
  


class FunctionCallOp(collections.namedtuple('FunctionCallOp', ['function', 'vars_in', 'vars_out'])):
  """Call a `Function`.

  This is a higher-level instruction, what in LLVM jargon is called an
  "intrinsic".  An upstream compiler may construct such instructions;
  there is a pass that lowers these to sequences of instructions the
  downstream VM can stage directly.

  This differs from `PrimOp` in that the function being called is
  itself implemented in this instruction language, and is subject to
  auto-batching by the downstream VM.

  A `FunctionCallOp` is required to statically know the identity of the
  `Function` being called.  This is because we want to copy the return
  values to their destinations at the caller side of the return
  sequence.  Why do we want that?  Because threads may diverge at
  function returns, thus needing to write the returned values to
  different caller variables.  Doing that on the callee side would
  require per-thread information about where to write the variables,
  which, in this design, is encoded in the program counter stack.
  Why, in turn, may threads diverge at function returns?  Because part
  of the point is to allow them to converge when calling the same
  function, even if from different points.

  Args:
    function: A `Function` object describing the function to call.
      This requires all call targets to be known statically.
    vars_in: list of strings.  The names of the VM variables whose
      current values to pass to the `function`.
    vars_out: pattern of strings.  The names of the VM variables
      where to save the results returned from `function`.
  """
  __slots__ = ...
  def __str__(self, indent=..., width=...) -> str:
    ...
  
  def replace(self, vars_out=...): # -> FunctionCallOp:
    """Return a copy of `self` with `vars_out` replaced."""
    ...
  


class PrimOp(collections.namedtuple('PrimOp', ['vars_in', 'vars_out', 'function', 'skip_push_mask'])):
  """An arbitrary already-batched computation, a 'primitive operation'.

  These are the items of work on which auto-batching is applied.  The
  `function` must accept and produce Tensors with a batch dimension,
  and is free to stage any (batched) computation it wants.
  Restriction: the `function` must use the same computation substrate
  as the VM backend.  That is, if the VM is staging to XLA, the
  `function` will see XLA Tensor handles; if the VM is staging to
  graph-mode TensorFlow, the `function` will see TensorFlow Tensors;
  etc.

  The current values of the `vars_out` are saved on their respective
  stacks, and the results written to the new top.

  The exact contract for `function` is as follows:
  - It will be invoked with a list of positional (only) arguments,
    parallel to `vars_in`.
  - Each argument will be a pattern of Tensors (meaning, either one
    Tensor or a (potentially nested) list or tuple of Tensors),
    corresponding to the `Type` of that variable.
  - Each Tensor in the argument will have the `dtype` and `shape`
    given in the corresponding `TensorType`, and an additional leading
    batch dimension.
  - Some indices in the batch dimension may contain junk data, if the
    corresponding threads are not executing this instruction [this is
    subject to change based on the batch execution strategy].
  - The `function` must return a pattern of Tensors, or objects
    convertible to Tensors.
  - The returned pattern must be compatible with the `Type`s of
    `vars_out`.
  - The Tensors in the returned pattern must have `dtype` and `shape`
    compatible with the corresponding `TensorType`s of `vars_out`.
  - The returned Tensors will be broadcast into their respective
    positions if necessary.  The broadcasting _includes the batch
    dimension_: Thus, a returned Tensor of insufficient rank (e.g., a
    constant) will be broadcast across batch members.  In particular,
    a Tensor that carries the indended batch size but whose sub-batch
    shape is too low rank will broadcast incorrectly, and will result
    in an error.
  - If the `function` raises an exception, it will propagate and abort
    the entire computation.
  - Even in the TensorFlow backend, the `function` will be staged
    several times: at least twice during type inference (to ascertain
    the shapes of the Tensors it likes to return, as a function of the
    shapes of the Tensors it is given), and exactly once during
    executable graph construction.

  Args:
    vars_in: list of strings.  The names of the VM variables whose
      current values to pass to the `function`.
    vars_out: Pattern of strings.  The names of the VM variables
      where to save the results returned from `function`.
    function: Python callable implementing the computation.
    skip_push_mask: Set of strings, a subset of `vars_out`.  These VM variables
      will be updated in place rather than pushed.
  """
  __slots__ = ...
  def __str__(self, indent=..., width=...) -> str:
    ...
  
  def replace(self, vars_out=...): # -> PrimOp:
    """Return a copy of `self` with `vars_out` replaced."""
    ...
  


class PopOp(collections.namedtuple('PopOp', ['vars'])):
  """Restore the given variables from their stacks.

  The current top value of each popped variable is lost.

  Args:
    vars: list of strings: The names of the VM variables to restore.
  """
  __slots__ = ...
  def __str__(self, indent=..., width=...) -> str:
    ...
  


class BranchOp(collections.namedtuple('BranchOp', ['cond_var', 'true_block', 'false_block'])):
  """A conditional.

  Args:
    cond_var: The string name of the VM variable holding the condition.
      This variable must have boolean dtype and scalar data shape.
    true_block: The `Block` where to transfer logical threads whose
      condition is `True`.
    false_block: The `Block` where to transfer logical threads whose
      condition is `False`.
  """
  __slots__ = ...
  def __str__(self, indent=..., width=...) -> str:
    ...
  


class GotoOp(collections.namedtuple('GotoOp', ['block'])):
  """An unconditional jump.

  Use for skipping non-taken `if` branches, and for transferring
  control during the function call sequence.

  Args:
    block: The `Block` to jump to.
  """
  __slots__ = ...
  def __str__(self, indent=..., width=...) -> str:
    ...
  


class PushGotoOp(collections.namedtuple('PushGotoOp', ['push_block', 'goto_block'])):
  """Save an address for `IndirectGotoOp` and unconditionally jump to another.

  Use in the function call sequence.  The address is saved on the
  reserved "program counter" variable.

  Args:
    push_block: The `Block` that the matching `IndirectGotoOp` will jump to.
    goto_block: The `Block` to jump to.
  """
  __slots__ = ...
  def __str__(self, indent=..., width=...) -> str:
    ...
  


class IndirectGotoOp(collections.namedtuple('IndirectGotoOp', [])):
  """Jump to the address in the reserved "program counter" variable.

  Use to return from a function call.

  Also restores the previous saved program counter (under the one
  jumped to), enforcing proper nesting with invocations of
  `PushGotoOp`.
  """
  __slots__ = ...
  def __str__(self, indent=..., width=...) -> str:
    ...
  


pc_var = ...
def prim_op(vars_in, vars_out, function, skip_push_mask=...): # -> PrimOp:
  ...

def push_op(vars_in, vars_out): # -> PrimOp:
  """Returns an `Op` that pushes values from `vars_in` into `vars_out`.

  Args:
    vars_in: Python pattern of `string`, the variables to read.
    vars_out: Python pattern of `string`, matching with `vars_in`; the
      variables to write to.

  Returns:
    op: An `Op` that accomplishes the push.
  """
  ...

def halt(): # -> None:
  """Returns `None`, acting as a sentinel `Block` meaning "exit this graph"."""
  ...

def halt_op(): # -> GotoOp:
  """Returns a control transfer `Op` that means "exit this graph"."""
  ...

def is_return_op(op): # -> bool:
  ...

class NullVariable(collections.namedtuple('NullVariable', [])):
  """A Variable that contains no storage and drops writes.

  Efficiently represents user variables that are never read.

  This is a namedtuple so it can be stored in an environment
  that passes through TensorFlow `while_loop`.
  """
  def update(self, value, mask): # -> Self:
    ...
  
  def push(self, mask): # -> Self:
    ...
  
  def read(self):
    ...
  
  def pop(self, mask): # -> Self:
    ...
  


class TemporaryVariable(collections.namedtuple('TemporaryVariable', ['value'])):
  """A temporary Variable.

  Efficiently stores values whose lifetime is inside one basic block.
  """
  @staticmethod
  def empty(): # -> TemporaryVariable:
    ...
  
  def update(self, value, mask): # -> TemporaryVariable:
    ...
  
  def push(self, mask): # -> Self:
    ...
  
  def read(self):
    ...
  
  def pop(self, mask): # -> TemporaryVariable:
    ...
  


class PythonVMVariable:
  """A pure-python implementation of the interface for VM variables.

  This `PythonVMVariable` class used only by the semantics-defining interpreter
  `interpret`, and as such is meant to be simple rather than high-performance.
  In particular, each variable is given exactly one value in each of its stack
  levels---there is no support for batching.

  To wit, the actual variable is a Python list giving the stack of values of
  that variable.  Reads and writes interact with the -1-indexed element.  Note
  that this architecture supports different variables having different stack
  heights, like the staging VM of which this is a model.

  The methods are all written consistently with functional style to support the
  `Environment` being written in functional style itself.
  """
  def __init__(self, name) -> None:
    ...
  
  def push(self): # -> Self:
    ...
  
  def update(self, value): # -> Self:
    ...
  
  def pop(self): # -> Self:
    ...
  
  def read(self): # -> None:
    ...
  
  def __repr__(self): # -> str:
    ...
  


class PythonBackend:
  """A factory for `PythonVMVariable` with the interface of a VM backend.

  Used in the definitional interpreter to form a simple model of non-batched
  computations.
  """
  def create_variable(self, name, alloc, type_): # -> PythonVMVariable:
    ...
  
  @property
  def variable_class(self): # -> type[PythonVMVariable]:
    ...
  


class Environment:
  """An environment, giving values for the extant variables.

  The actual environment is a dictionary mapping each variable name to the
  backend-determined `Variable` object holding the stack of values of that
  variable.  Reads and writes pass through to the `Variable`s; this class
  just manages name resolution.

  The methods are all written in functional style to support using the same
  `Environment` to stage multiple different instructions.
  """
  def __init__(self, env_dict, backend, update=...) -> None:
    """Creates an `Environment` from the dictionary giving its content.

    Defensively copies the input dictionary.

    Args:
      env_dict: A dictionary mapping variable names to their values;
        e.g., from the `env_dict` method of a (different) `Environment`
        instance.
      backend: A factory for `Variable`s supporting a `create_variable(name,
        type)` method and a `variable_class` field.
      update: An optional dictionary mapping (some) variable names
        to their values.  If supplied, those variables' values are
        overwritten.
    """
    ...
  
  @staticmethod
  def initialize(backend, var_alloc, var_defs, *args, **kwargs): # -> Environment:
    """Initializes an `Environment` from type definitions.

    Args:
      backend: A factory for `Variable`s supporting a `create_variable(name,
        type)` method and a `variable_class` field.
      var_alloc: A Python dict mapping variable to `VariableAllocation`
        objects.
      var_defs: A Python dict mapping variable names to `Type`
        objects, as the `var_defs` field of `Program`.
      *args: Additional arguments to pass to the backend's `create_variable`
        method, if any.
      **kwargs: Additional keyword arguments to pass to the backend's
        `create_variable` method, if any.

    Returns:
      env: A properly initialized `Environment`.
    """
    ...
  
  def push(self, name, value, *args, **kwargs): # -> Any:
    """Pushes value to the variable of the given name.

    Does not mutate the `Environment`.

    Args:
      name: A Python `string` giving the name of the
        variable to push to.
      value: A object giving the value to push.
      *args: Additional arguments to pass to the `Variable`'s
        methods, if any.
      **kwargs: Additional keyword arguments to pass to the `Variable`'s
        methods, if any.

    Raises:
      ValueError: If `name` refers to a variable that was not part of
        the `var_defs` with which this Environment was created.

    Returns:
      new_var: An updated `Variable` incorporating the push.
    """
    ...
  
  def update(self, name, value, *args, **kwargs): # -> Any:
    """Writes value to the variable of the given name without pushing.

    Does not mutate the `Environment`.

    Args:
      name: A Python `string` giving the name of the
        variable to update.
      value: A object giving the value to update with.
      *args: Additional arguments to pass to the `Variable`'s
        methods, if any.
      **kwargs: Additional keyword arguments to pass to the `Variable`'s
        methods, if any.

    Raises:
      ValueError: If `name` refers to a variable that was not part of
        the `var_defs` with which this Environment was created.

    Returns:
      new_var: An updated `Variable` incorporating the update.
    """
    ...
  
  def pop(self, name, *args, **kwargs): # -> Any:
    """Pops a value off the stack of the given variable.

    Does not mutate the `Environment`.

    Args:
      name: A Python `string` giving the name of the variable to pop.
      *args: Additional arguments to pass to the `Variable`'s
        `pop` method, if any.
      **kwargs: Additional keyword arguments to pass to the `Variable`'s
        `pop` method, if any.

    Raises:
      ValueError: If `name` refers to a variable that was not part of
        the `var_defs` with which this Environment was created.

    Returns:
      new_var: An updated `Variable` incorporating the pop.
    """
    ...
  
  def read(self, name, *args, **kwargs): # -> Any:
    """Reads the current (top) value of the given variable.

    Args:
      name: A Python `string`, naming the variable to read.
      *args: Additional arguments to pass to the `Variable`'s
        `read` method, if any.
      **kwargs: Additional keyword arguments to pass to the `Variable`'s
        `read` method, if any.

    Returns:
      value: The value of that variable.

    Raises:
      ValueError: If `name` refers to a variable that was not part of
        the `var_defs` with which this Environment was created, or if
        the read stack is empty.
    """
    ...
  
  def __setitem__(self, key, val): # -> None:
    ...
  
  @property
  def env_dict(self): # -> dict[Any, Any]:
    ...
  


def interpret(program, *inputs): # -> Any:
  """Interprets a program in this instruction language and returns the result.

  This is a definitional interpreter; its purpose is to define the
  semantics of the instruction language.  As such, it does no
  auto-batching, and generally strives to be as simple as possible.
  It also does not stage graph computations, so will only work in
  Eager mode TensorFlow.

  Args:
    program: The Program tuple to interpret.
    *inputs: Values to pass to the program.  The length of `inputs` must be
      the same as the length of `program.vars_in`.

  Returns:
    results: A tuple of results, which are the values of the variables listed
      in `program.out_vars` at termination.

  Raises:
    ValueError: If an internal invariant is violated, or an error is
      detected in the program being interpreted.
  """
  ...

def extract_referenced_variables(node): # -> set[Any | tuple[Any, ...] | list[Any]] | set[Any | tuple[Any, ...] | list[Any] | str] | set[Any] | set[Any | str] | set[str]:
  """Extracts a set of the variable names referenced by the node in question.

  Args:
    node: Most structures from the VM, including ops, sequences of ops, blocks.

  Returns:
    varnames: `set` of variable names referenced by the node in question.
  """
  ...

def pattern_map(f, pattern, leaf_type=...): # -> Any:
  """Applies a function elementwise to a pattern.

  Args:
    f: Function to apply
    pattern: Objects
    leaf_type: Optional list of Python types to treat as leaves even though
      they may inherit from `list` or `tuple`.

  Returns:
    result: A structure of the same shape as objects, with every
      item `x` replaced by `f(x)`
  """
  ...

def pattern_traverse(pattern, leaf_type=...): # -> Generator[Any | tuple[Any, ...] | list[Any], Any, None]:
  """Yields every terminal object in a pattern."""
  ...

def pattern_zip(pattern1, pattern2, leaf_type=...): # -> Generator[tuple[Any | list[Any] | tuple[Any, ...], Any], Any, None]:
  """Yields every corresponding pair of objects in the given patterns.

  Args:
    pattern1: A pattern.
    pattern2: A pattern of matching shape.  Pattern2 may have lists or
      tuples where pattern1 has terminals; those pairs will be yielded.
    leaf_type: Optional list of Python types to treat as leaves even though
      they may inherit from `list` or `tuple`.

  Yields:
    item1: A terminal item from pattern1
    item2: The (potentially terminal) sub-pattern from pattern2 in the
      corresponding place.

  Raises:
    ValueError: If the patterns do not match.
  """
  ...

def pattern_map2(f, pattern1, pattern2, leaf_type=...): # -> Any:
  """Applies f to every corresponding pair of objects in the given patterns.

  Args:
    f: Binary function to map.
    pattern1: A pattern.
    pattern2: A pattern of matching shape.  Pattern2 may have lists or
      tuples where pattern1 has terminals; those pairs will be matched.
    leaf_type: Optional list of Python types to treat as leaves even though
      they may inherit from `list` or `tuple`.

  Returns:
    results: A pattern of the same shape as pattern1.  Each terminal is the
      result of applying `f` to the corresponding pair of items from pattern1
      and pattern2.

  Raises:
    ValueError: If the patterns do not match.
  """
  ...

def pattern_flatten(pattern, leaf_type=...): # -> list[Any | tuple[Any, ...] | list[Any]]:
  """Returns all the terminals in `pattern` as a list."""
  ...

def detect_batch_size(var_defs, init_vals, backend): # -> Literal[1]:
  """Returns the batch size implied by the top dimensions of the inputs."""
  ...

