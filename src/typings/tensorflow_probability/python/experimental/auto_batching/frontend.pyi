"""
This type stub file was generated by pyright.
"""

from tensorflow.python.autograph.pyct import transformer

"""AutoGraph-based auto-batching frontend."""
TF_BACKEND = ...
class _AutoBatchingTransformer(transformer.Base):
  """A subclass of `pyct.transformer.Base` implementing auto-batching.

  Specifically, converts some Python source P into a Python function that will
  invoke methods on a `dsl.ProgramBuilder` to register an auto-batchable version
  of P.  This class is private: use through the `transform` wrapper.

  The input is expected to be in A-normal form, so that all intermediate values
  are explicitly assigned.  As such, this class need only intercept function
  definitions, assignment statements, and control constructs.  At the moment,
  only supporting if, function call, and return.  TODO(axch): Expand the subset.
  """
  def __init__(self, known_functions, enclosing_names, ctx) -> None:
    """Initializes an _AutoBatchingTransformer.

    Args:
      known_functions: List of Python strings.  These are the names of
        auto-batched functions.  Calls to the functions on this list (when
        recognized) are transformed into `ProgramBuilder.call` methods.  Other
        function calls become `ProgramBuilder.primop`.
      enclosing_names: List of Python variables.  These are names to draw from
        the transformee's enclosing scope (without turning them into autobatch
        variables).
      ctx: An AutoGraph `autograph.converter.EntityContext` object describing
        the node to be transformed, as returned by `_parse_and_analyze`.
        Inherits from `autograph.transformer.Context`, see
        `autograph.transformer.Base`.
    """
    ...
  
  def visit_FunctionDef(self, node): # -> Any:
    """Intercepts function definitions.

    Converts function definitions to the corresponding `ProgramBuilder.function`
    construction.

    Args:
      node: An `ast.AST` node representing the function to convert.

    Returns:
      node: An updated node, representing the result.

    Raises:
      ValueError: If the input node does not adhere to the restrictions,
        e.g., failing to have a `return` statement at the end.
    """
    ...
  
  def visit_Assign(self, node): # -> list[Any]:
    """Intercepts assignment statements.

    The input is expected to be in A-normal form, so every assignment represents
    at most one interesting computation, which is the AST node serving as the
    assignment's right-hand side.

    Args:
      node: An `ast.AST` node representing the assignment to convert.

    Returns:
      node: An updated node, representing the result.

    Raises:
      ValueError: If the node is a call to a function whose name cannot be
        determined.
    """
    ...
  
  def visit_If(self, node): # -> list[Any] | Any:
    """Intercepts if statements.

    Converts each `if` to up to two separate `with` statements,
    `ProgramBuilder.if_(condition_variable)` and `ProgramBuilder.else_()`.  If
    the incoming `if` had one arm, returns the transformed AST node; if it had
    two, returns two nodes in a list.

    Args:
      node: An `ast.AST` node representing the `if` statement to convert.

    Returns:
      then_node: A node representing the `with`-guarded consequent branch.
      else_node: A node representing the `with`-guarded alternate branch,
        if present.
    """
    ...
  
  def visit_Return(self, node):
    """Intercepts return statements.

    Args:
      node: An `ast.AST` node representing the `return` statement to convert.

    Returns:
      node: A node representing the result.
    """
    ...
  


class Context:
  """Context object for auto-batching multiple Python functions together.

  Warning: This is alpha-grade software.  The exact subset of Python that can be
  successfully auto-batched is ill-specified and subject to change.
  Furthermore, the errors elicited by stepping outside this subset are likely to
  be confusing and misleading.  Use at your own risk.

  Usage:
  ```python
  ctx = frontend.Context()

  @ctx.batch(type_inference=lambda ...)
  def my_single_example_function_1(args):
    ...

  @ctx.batch(type_inference=lambda ...)
  def my_single_example_function_2(args):
    ...

  # etc
  ```

  Then calling any of the decorated functions will execute a batch computation.
  The decorated functions may call each other, including mutually recursively.
  See also the `batch` method.

  Limitations:
  - You must explicitly decorate every function to be auto-batched.
  - All calls to them must call them by name (no higher-order auto-batching).
  - Auto-batched functions must be defined with `def`, not `lambda`.
  """
  def __init__(self) -> None:
    """Initializes a `Context` object."""
    ...
  
  def batch(self, type_inference): # -> partial[Callable[..., Any]]:
    """Decorates one function to auto-batch.

    The decorated function will run in batch.  It accepts all the same
    arguments, except:
    - All arguments must have an additional leading dimension for the batch.
      (By special dispensation, scalar inputs are promoted to shape [1], which
      then leads to broadcasting.)
    - All the arguments' sizes in the batch dimension must be the same, or 1.
      The latter are broadcast.
    - The returned value will also have a leading batch dimension, and
      will have the same size.
    - The batched function accepts an additional `bool` keyword argument
      `dry_run`.  If present and `True`, just calls the unbatched version,
      circumventing the auto-batching system.  This can be useful for
      debugging the program subject to auto-batching.
    - The batched function accepts an additional `bool` keyword argument
      `stackless`.  If present and `True`, invokes the stackless version of the
      auto-batching system.  This can be useful for avoiding stack maintenance
      overhead; but in general, it will recover less batching, and not work in
      graph-mode TensorFlow.
    - The batched function accepts an additional `int` keyword argument
      `max_stack_depth` specifying the maximum stack depth (default 15).
      Ignored in stackless execution.
    - The batched function accepts an additional keyword argument `backend`
      specifying the backend to use.  Must be an instance of
      `auto_batching.TensorFlowBackend` (default) or
      `auto_batching.NumpyBackend`.
    - The batched function accepts an additional keyword argument
      `block_code_cache`, a dict which allows the caching of basic block
      rewrites (i.e. `tf.function` + XLA) to live across calls to the
      autobatched function. The default value of `None` results in caching only
      within a given call to the batched function. Currently, stackless
      autobatching ignores the cache completely.

    Note: All functions called by the decorated function that need auto-batching
    must be decorated in the same `Context` before invoking any of them.

    Args:
      type_inference: A Python callable giving the type signature of the
        function being auto-batched.  The callable will be invoked with a single
        argument giving the list of `instructions.Type` objects describing the
        arguments at a particular call site, and must return a list of
        `instructions.Type` objects describing the values that call site will
        return.

    Returns:
      dec: A decorator that may be applied to a function with the given
        type signature to auto-batch it.

    Raises:
      ValueError: If the decorated function predictably cannot be auto-batched,
        e.g., name-clashing with another function already decorated in this
        `Context`.
    """
    ...
  
  def batch_uncurried(self, function, type_inference): # -> Callable[..., Any]:
    """A non-decorator version of `batch`, which see."""
    ...
  
  def function_names(self): # -> list[Any]:
    ...
  
  def module(self): # -> Module:
    """Constructs an `instructions.Module` for this `Context`.

    Returns:
      module: An `instructions.Module` representing the batched computation
        defined by all the functions decorated with `batch` in this `Context` so
        far.
    """
    ...
  
  def program(self, main): # -> Program:
    """Constructs an `instructions.Program` for this `Context`.

    This is a helper method, equivalent to `self.module().program(main)`.

    Args:
      main: Python string name of the function that should be the entry point.

    Returns:
      prog: An `instructions.Program` representing the batched computation
        defined by all the functions decorated with `batch` in this `Context` so
        far.  Suitable for downstream compilation with other passes in
        `auto_batching`.

    Raises:
      ValueError: If the intended `main` function was not decorated with
        `batch`.
    """
    ...
  
  def program_compiled(self, main, sig=..., backend=...): # -> Program:
    """Constructs a compiled `instructions.Program` for this `Context`.

    This constructs the program with `self.program(main)`, and the performs type
    inference and optimization, to emit a result that can be executed by the
    stackless auto-batching VM.

    The point of having this as a method in its own right is that it caches the
    compilation on the types of the arguments.

    If either `sig` or `backend` are omitted or `None`, type inference is
    skipped.  The result is not executable, but it can be enlightening to
    inspect.

    Args:
      main: Python string name of the function that should be the entry point.
      sig: A `list` of (patterns of) `instructions.TensorType` aligned with
        the formal parameters to `main`.
      backend: Backend implementation.

    Returns:
      prog: An `instructions.Program` representing the batched computation
        defined by all the functions decorated with `batch` in this `Context` so
        far.  Suitable for execution or staging on real data by the
        auto-batching VM.
    """
    ...
  
  def program_lowered(self, main, sig=..., backend=...): # -> Program:
    """Constructs a lowered `instructions.Program` for this `Context`.

    This constructs the program with `self.program(main)`, and the performs type
    inference, optimization, and lowering, to emit a result that can be executed
    (or staged) by the auto-batching VM.

    The point of having this as a method in its own right is that it caches the
    compilation on the types of the arguments.

    If either `sig` or `backend` are omitted or `None`, type inference is
    skipped.  The result is not executable, but it can be enlightening to
    inspect.

    Args:
      main: Python string name of the function that should be the entry point.
      sig: A `list` of (patterns of) `instructions.TensorType` aligned with
        the formal parameters to `main`.
      backend: Backend implementation.

    Returns:
      prog: An `instructions.Program` representing the batched computation
        defined by all the functions decorated with `batch` in this `Context` so
        far.  Suitable for execution or staging on real data by the
        auto-batching VM.
    """
    ...
  
  def lowered_for_args(self, name, args, backend): # -> Program:
    """Helper for calling program_lowered that computes the type signature."""
    ...
  


def truthy(x):
  """Normalizes Tensor ranks for use in `if` conditions.

  This enables dry-runs of programs with control flow.  Usage: Program the
  conditions of `if` statements and `while` loops to have a batch dimension, and
  then wrap them with this function.  Example:
  ```python
  ctx = frontend.Context
  truthy = frontend.truthy

  @ctx.batch(type_inference=...)
  def my_abs(x):
    if truthy(x > 0):
      return x
    else:
      return -x

  my_abs([-5], dry_run=True)
  # returns [5] in Eager mode
  ```

  This is necessary because auto-batched programs still have a leading batch
  dimension (of size 1) even in dry-run mode, and a Tensor of shape [1] is not
  acceptable as the condition to an `if` or `while`.  However, the leading
  dimension is critical during batched execution; so conditions of ifs need to
  have rank 1 if running batched and rank 0 if running unbatched (i.e.,
  dry-run).  The `truthy` function arranges for this be happen (by detecting
  whether it is in dry-run mode or not).

  If you missed a spot where you should have used `truthy`, the error message
  will say `Non-scalar tensor <Tensor ...> cannot be converted to boolean.`

  Args:
    x: A Tensor.

  Returns:
    x: The Tensor `x` if we are in batch mode, or if the shape of `x` is
      anything other than `[1]`.  Otherwise returns the single scalar in `x` as
      a Tensor of scalar shape (which is acceptable in the conditions of `if`
      and `while` statements.
  """
  ...

