"""
This type stub file was generated by pyright.
"""

import collections

"""Numpy backend for auto-batching VM.

It can be faster than TF for tiny examples and prototyping, and moderately
simpler due to immediate as opposed to deferred result computation.

All operations take and ignore name= arguments to allow for useful op names in
the TensorFlow backend.
"""
__all__ = ['NumpyBackend']
class RegisterNumpyVariable(collections.namedtuple('RegisterNumpyVariable', ['value'])):
  """A register-only variable.

  Efficiently stores and updates values whose lifetime does not cross function
  calls (and therefore does not require a stack).  This is different from
  `TemporaryVariable` because it supports crossing basic block boundaries.  A
  `RegisterNumpyVariable` therefore needs to store its content persistently
  across the `while_loop` in `execute`, and to handle divergence (and
  re-convergence) of logical threads.
  """
  def update(self, value, mask): # -> Self:
    ...
  
  def push(self, mask): # -> Self:
    ...
  
  def read(self):
    ...
  
  def pop(self, mask): # -> Self:
    ...
  


class Stack(collections.namedtuple('Stack', ['stack', 'stack_index'])):
  """Internal container for a batched stack.

  The implementation is a preallocated array and a (batched) stack
  pointer.

  The namedtuple structure exposes the full state of the stack, and is useful
  for testing, passing through flatten/unflatten operations, and general
  symmetry with the TensorFlow backend.
  """
  def pop(self, mask): # -> tuple[Stack, Any]:
    """Pops each indicated batch member, returning a previous write.

    Args:
      mask: Boolean array of shape `[batch_size]`. The threads at `True`
        indices of `mask` will have their frame pointers regressed by 1.

    Returns:
      stack: Updated variable. Does not mutate `self`.
      read: The new top of the stack, after regressing the frame pointers
        indicated by `mask`.

    Raises:
      ValueError: On an attempt to pop the last value off a batch member.
    """
    ...
  
  def push(self, value, mask): # -> Stack:
    """Writes new value to all threads, updates frame of those in `mask`.

    Args:
      value: Value to write into all threads top frame before updating `mask`
        frame pointers.
      mask: Boolean array of shape `[batch_size]`. The values at `True`
        indices of `mask` are updated; the others remain.

    Returns:
      stack: Updated stack. Does not mutate `self`.

    Raises:
      ValueError: If a push exceeds the maximum stack depth.
    """
    ...
  


class FullNumpyVariable(collections.namedtuple('FullNumpyVariable', ['current', 'stack'])):
  """A variable backed by a batched numpy "stack" with a cache for the top.

  The purpose of the cache is to make reads from and writes to the top
  of the stack cheaper than they would be otherwise.

  The namedtuple structure exposes the full state of the variable, and is useful
  for testing, passing through flatten/unflatten operations, and general
  symmetry with the TensorFlow backend.
  """
  def read(self, name=...):
    """Returns the batch of top values.

    Args:
      name: Optional name for the op.

    Return:
      val: Read of the current variable value.
    """
    ...
  
  def update(self, value, mask, name=...): # -> FullNumpyVariable:
    """Updates this variable at the indicated places.

    Args:
      value: Array of shape `[batch_size, e1, ..., eE]` of data to update with.
        Indices in the first dimension corresponding to `False`
        entries in `mask` are ignored.
      mask: Boolean array of shape `[batch_size]`. The values at `True`
        indices of `mask` are updated; the others remain.
      name: Optional name for the op.

    Returns:
      var: Updated variable. Does not mutate `self`.
    """
    ...
  
  def push(self, mask, name=...): # -> FullNumpyVariable:
    """Pushes each indicated batch member, making room for a new write.

    The new top value is the same as the old top value (this is a
    "duplicating push").

    Args:
      mask: Boolean array of shape `[batch_size]`. The values at `True`
        indices of `mask` are updated; the others remain.
      name: Optional name for the op.

    Returns:
      var: Updated variable. Does not mutate `self`.

    Raises:
      ValueError: If a push exceeds the maximum stack depth.
    """
    ...
  
  def pop(self, mask, name=...): # -> FullNumpyVariable:
    """Pops each indicated batch member, restoring a previous write.

    Args:
      mask: Boolean array of shape `[batch_size]`. The values at `True`
        indices of `mask` are updated; the others remain.
      name: Optional name for the op.

    Returns:
      var: Updated variable. Does not mutate `self`.

    Raises:
      ValueError: On an attempt to pop the last value off a batch member.
    """
    ...
  


class NumpyBackend:
  """Implements the Numpy backend ops for a PC auto-batching VM."""
  @property
  def variable_class(self): # -> tuple[type[NullVariable], type[TemporaryVariable], type[RegisterNumpyVariable], type[FullNumpyVariable]]:
    ...
  
  def type_of(self, t, dtype_hint=...): # -> TensorType:
    """Returns the `instructions.Type` of `t`.

    Args:
      t: `np.ndarray` or a Python constant.
      dtype_hint: dtype to prefer, if `t` is a constant.

    Returns:
      vm_type: `instructions.TensorType` describing `t`
    """
    ...
  
  def run_on_dummies(self, primitive_callable, input_types):
    """Runs the given `primitive_callable` with dummy input.

    This is useful for examining the outputs for the purpose of type inference.

    Args:
      primitive_callable: A python callable.
      input_types: `list` of `instructions.Type` type of each argument to the
        callable.  Note that the contained `TensorType` objects must match the
        dimensions with which the primitive is to be invoked at runtime, even
        though type inference conventionally does not store the batch dimension
        in the `TensorType`s.

    Returns:
      outputs: pattern of backend-specific objects whose types may be
        analyzed by the caller with `type_of`.
    """
    ...
  
  def merge_dtypes(self, dt1, dt2): # -> dtype[float64]:
    """Merges two dtypes, returning a compatible dtype.

    Args:
      dt1: A numpy dtype, or None.
      dt2: A numpy dtype, or None.

    Returns:
      dtype: The more precise numpy dtype (e.g. prefers int64 over int32).
    """
    ...
  
  def merge_shapes(self, s1, s2): # -> _Shape:
    """Merges two shapes, returning a broadcasted shape.

    Args:
      s1: A `list` of Python `int` or None.
      s2: A `list` of Python `int` or None.

    Returns:
      shape: A `list` of Python `int` or None.

    Raises:
      ValueError: If `s1` and `s2` are not broadcast compatible.
    """
    ...
  
  def assert_matching_dtype(self, expected_dtype, val, message=...): # -> None:
    """Asserts that the dtype of `val` matches `expected_dtype`.

    Args:
      expected_dtype: A numpy dtype
      val: An object convertible to `np.array`
      message: Optional diagnostic message.

    Raises:
      ValueError: If dtype does not match.
    """
    ...
  
  def batch_size(self, val, name=...): # -> int:
    """Returns the first (batch) dimension of `val`."""
    ...
  
  def static_value(self, t):
    """Gets the eager/immediate value of `t`."""
    ...
  
  def fill(self, value, size, dtype, shape, name=...):
    """Fill a fresh batched Tensor of the given shape and dtype with `value`.

    Args:
      value: Scalar to fill with.
      size: Scalar `int` `Tensor` specifying the number of VM threads.
      dtype: `tf.DType` of the zeros to be returned.
      shape: Rank 1 `int` `Tensor`, the per-thread value shape.
      name: Optional name for the op.

    Returns:
      result: `Tensor` of `dtype` `value`s with shape `[size, *shape]`
    """
    ...
  
  def create_variable(self, name, alloc, type_, max_stack_depth, batch_size): # -> NullVariable | TemporaryVariable | RegisterNumpyVariable | FullNumpyVariable:
    """Returns an intialized Variable.

    Args:
      name: Name for the variable.
      alloc: `VariableAllocation` for the variable.
      type_: `instructions.TensorType` describing the sub-batch shape and dtype
        of the variable being created.
      max_stack_depth: Python `int`, the maximum stack depth to enforce.
      batch_size: Python `int`, the number of parallel threads being executed.

    Returns:
      var: A new, initialized Variable object.
    """
    ...
  
  def full_mask(self, size, name=...):
    """Returns an all-True mask `np.ndarray` with shape `[size]`."""
    ...
  
  def broadcast_to_shape_of(self, val, target, name=...): # -> NDArray[Any]:
    """Broadcasts val to the shape of target.

    Args:
      val: Python or Numpy array to be broadcast. Must be `np.array` compatible
        and broadcast-compatible with `target`.
      target: Python or Numpy array whose shape we broadcast `val` to match.
      name: Optional name for the op.

    Returns:
      broadcast_val: A `np.ndarray` with shape matching `val + target`. Provided
        that `val`'s dimension sizes are all smaller or equal to `target`'s, the
        returned value will be the shape of `target`.
    """
    ...
  
  def cond(self, pred, true_fn, false_fn, name=...):
    """Implements a conditional operation for the backend.

    Args:
      pred: A Python or Numpy `bool` scalar indicating the condition.
      true_fn: A callable accepting and returning nests of `np.ndarray`s
        with the same structure as `state`, to be executed when `pred` is True.
      false_fn: A callable accepting and returning nests of `np.ndarray`s with
        the same structure as `state`, to be executed when `pred` is False.
      name: Optional name for the op.

    Returns:
      state: Output state, matching nest structure of input argument `state`.
    """
    ...
  
  def prepare_for_cond(self, state):
    """Backend hook for preparing Tensors for `cond`.

    Does nothing in the numpy backend (needed by the TensorFlow backend).

    Args:
      state: A state to be prepared for use in conditionals.

    Returns:
      state: The prepared state.
    """
    ...
  
  def where(self, condition, x, y, name=...): # -> NDArray[Any]:
    """Implements a where selector for the Numpy backend.

    Extends `tf.where` to support broadcasting of `on_false`.

    Args:
      condition: A `bool` `np.ndarray`, either a vector having length
        `y.shape[0]` or matching the full shape of `y`.
      x: `np.ndarray` of values to take when `condition` is `True`.
      y: `np.ndarray` of values to take when `condition` is `False`. May
        be smaller than `x`, as long as it is broadcast-compatible.
      name: Optional name for the op.

    Returns:
      masked: A `np.ndarray` where indices corresponding to `True` values in
        `condition` come from the corresponding value in `x`, and others come
        from `y`.
    """
    ...
  
  def reduce_min(self, t, name=...):
    """Implements reduce_min for Numpy backend."""
    ...
  
  def while_loop(self, cond, body, loop_vars, name=...):
    """Implements while loops for Numpy backend."""
    ...
  
  def switch_case(self, branch_selector, branch_callables, name=...):
    """Implements a switch (branch_selector) { case ... } construct."""
    ...
  
  def equal(self, t1, t2, name=...): # -> Any:
    """Implements equality comparison for Numpy backend."""
    ...
  
  def not_equal(self, t1, t2, name=...): # -> Any:
    """Implements inequality comparison for Numpy backend."""
    ...
  
  def any(self, t, name=...): # -> numpy.bool[builtins.bool]:
    ...
  
  def wrap_straightline_callable(self, f):
    """Method exists solely to be stubbed, i.e. for defun or XLA compile."""
    ...
  


