"""
This type stub file was generated by pyright.
"""

import collections
from tensorflow_probability.python.experimental.mcmc import reducer as reducer_lib
from tensorflow_probability.python.mcmc import kernel as kernel_base
from tensorflow_probability.python.mcmc.internal import util as mcmc_util

"""SNAPER-HMC[1] TransitionKernel.

#### References

[1]: Sountsov, P. & Hoffman, M. (2021). Focusing on Difficult Directions for
     Learning HMC Trajectory Lengths. <https://arxiv.org/abs/2110.11576>
"""
__all__ = ['SNAPERHamiltonianMonteCarlo', 'SNAPERHamiltonianMonteCarloResults', 'SampleSNAPERHamiltonianMonteCarloResults', 'sample_snaper_hmc']
class SNAPERHamiltonianMonteCarloResults(mcmc_util.PrettyNamedTupleMixin, collections.namedtuple('GradientBasedTrajectoryLengthAdaptationResults', ['inner_results', 'ema_mean', 'ema_variance', 'max_ema_variance', 'state_ema_points', 'ema_principal_component', 'principal_component_ema_points', 'seed'])):
  """Internal state of SNAPERHamiltonianMonteCarlo.

  Attributes:
    inner_results: Results of the inner kernel. This is
      `PreconditionedHamiltonianMonteCarloResults` wrapped in
      `GradientBasedTrajectoryLengthAdaptationResults`.
    ema_mean: Exponential moving average cross-chain state mean.
    ema_variance: Exponential moving average cross-chain state variance.
    max_ema_variance: Maximum of `ema_variance`.
    state_ema_points: Approximate number of points used to compute the
      exponential moving averages.
    ema_principal_component: Exponential moving average cross-chain state
      covariance matrix principal component.
    principal_component__ema_points: Approximate number of points used to
      compute the exponential moving average of the principal component.
    seed: PRNG seed; see `tfp.random.sanitize_seed` for details. The random seed
      used by the kernel in the previous step.
  """
  __slots__ = ...


class SNAPERHamiltonianMonteCarlo(kernel_base.TransitionKernel):
  """SNAPER-HMC without step size adaptation.

  This implements the SNAPER-HMC algorithm from [1], without the step size
  adaptation. This kernel learns a diagonal mass matrix and the trajectory
  length parameters of the Hamiltonian Monte Carlo (HMC) sampler using the
  Adaptive MCMC framework [2]. As with all adaptive MCMC algorithms, this kernel
  does not produce samples from the target distribution while adaptation is
  engaged, so be sure to set `num_adaptation_steps` parameter smaller than the
  number of burnin steps.

  This kernel uses the SNAPER criterion (see
  `tfp.experimental.mcmc.snaper_criterion` for details) which has a principal-
  component parameter. This kernel learns it using a batched Oja's algorithm
  with a learning rate of `principal_component_ema_factor / step` where `step`
  is the iteration number.

  The mass matrix is learned using a variant of the Welford's
  algorithm/Exponential Moving Average, with a decay rate set to `step //
  state_ema_factor / (step // state_ema_factor + 1)`.

  Learning the step size is a necessary component of a good HMC sampler, but it
  is not handled by this kernel. That adaptation can be provided by, for
  example, `tfp.mcmc.SimpleStepSizeAdaptation` or
  `tfp.mcmc.DualAveragingSizeAdaptation`.

  To aid algorithm stability, the first few steps are taken with the number of
  leapfrog steps set to 1, turning the algorithm into Metropolis Adjusted
  Langevin Algorithm (MALA). This is controlled by the `num_mala_steps`
  argument.

  Unlike some classical MCMC algorithms, this algorithm behaves best when the
  chains are initialized with very low variance. Initializing them all at one
  point is recommended.

  SNAPER-HMC requires at least two chains to function.

  #### Examples

  Here we apply this kernel to a target with a known covariance structure and
  show that it recovers the principal component and the variances.

  ```python
  num_dims = 8
  num_burnin_steps = 1000
  num_adaptation_steps = int(num_burnin_steps * 0.8)
  num_results = 500
  num_chains = 64
  step_size = 1e-2
  num_mala_steps = 100

  eigenvalues = np.exp(np.linspace(0., 3., num_dims))
  q, r = np.linalg.qr(np.random.randn(num_dims, num_dims))
  q *= np.sign(np.diag(r))
  covariance = (q * eigenvalues).dot(q.T)

  _, eigs = np.linalg.eigh(covariance)
  principal_component = eigs[:, -1]

  gaussian = tfd.MultivariateNormalTriL(
      loc=tf.zeros(num_dims),
      scale_tril=tf.linalg.cholesky(covariance),
  )

  kernel = tfp.experimental.mcmc.SNAPERHamiltonianMonteCarlo(
      gaussian.log_prob,
      step_size=step_size,
      num_adaptation_steps=num_adaptation_steps,
      num_mala_steps=num_mala_steps,
  )
  kernel = tfp.mcmc.DualAveragingStepSizeAdaptation(
      kernel, num_adaptation_steps=num_adaptation_steps)

  def trace_fn(_, pkr):
    return {
        'principal_component':
            unnest.get_innermost(pkr, 'ema_principal_component'),
        'variance':
            unnest.get_innermost(pkr, 'ema_variance'),
    }

  init_x = tf.zeros([num_chains, num_dims])

  chain, trace = tfp.mcmc.sample_chain(
              num_results=num_results,
              num_burnin_steps=num_burnin_steps,
              current_state=init_x,
              kernel=kernel,
              trace_fn=trace_fn)

  # Close to `np.diag(covariance)`
  trace['variance'][-1]
  # Close to `principal_component`, up to a sign.
  trace['principal_component'][-1]

  # Compute sampler diagnostics.
  tfp.mcmc.effective_sample_size(chain, cross_chain_dims=1)
  tfp.mcmc.potential_scale_reduction(chain)

  # Compute downstream statistics.
  tf.reduce_mean(chain, [0, 1])
  ```


  #### References

  [1]: Sountsov, P. & Hoffman, M. (2021). Focusing on Difficult Directions for
       Learning HMC Trajectory Lengths. <https://arxiv.org/abs/2110.11576>

  [2]: Andrieu, Christophe, Thoms, Johannes. A tutorial on adaptive MCMC.
       Statistics and Computing, 2008.
       <https://people.eecs.berkeley.edu/~jordan/sail/readings/andrieu-thoms.pdf>.
  """
  def __init__(self, target_log_prob_fn, step_size, num_adaptation_steps, num_mala_steps=..., max_leapfrog_steps=..., trajectory_length_adaptation_rate=..., principal_component_ema_factor=..., state_ema_factor=..., experimental_shard_axis_names=..., experimental_reduce_chain_axis_names=..., preconditioned_hamiltonian_monte_carlo_kwargs=..., gradient_based_trajectory_length_adaptation_kwargs=..., validate_args=..., name=...) -> None:
    """Constructs the `SNAPERHamiltonianMonteCarlo` kernel.

    Args:
      target_log_prob_fn: Python callable which takes an argument like
        `current_state` (or `*current_state` if it's a list) and returns its
        (possibly unnormalized) log-density under the target distribution.
      step_size: Scalar `float` `Tensor` representing the step size for the
        leapfrog integrator.
      num_adaptation_steps: Scalar `int` `Tensor` number of initial steps during
        which to adjust the hyperparameters.
      num_mala_steps: Scalar `int` `Tensor` number of initial steps during which
        the number of leapfrog steps is clamped to 1, for stability.
      max_leapfrog_steps: Scalar `int` `Tensor`. Clips the number of leapfrog
        steps to this value.
      trajectory_length_adaptation_rate: Scalar `float` `Tensor`. How
        rapidly to adapt the trajectory length.
      principal_component_ema_factor: Scalar `int` `Tensor`. Factor controlling
        the principal component adaptation. Larger number corresponds to faster
        adaptation.
      state_ema_factor: Scalar `int` `Tensor`. Factor controlling
        the mass matrix adaptation. Larger number corresponds to faster
        adaptation.
      experimental_shard_axis_names: A structure of string names indicating how
        members of the state are sharded.
      experimental_reduce_chain_axis_names: A string or list of string names
        indicating which named axes to average cross-chain statistics over.
      preconditioned_hamiltonian_monte_carlo_kwargs: Additional keyword
        arguments to pass to `PreconditionedHamiltonianMonteCarlo` kernel.
      gradient_based_trajectory_length_adaptation_kwargs: Additional keyword
        arguments to pass to `GradientBasedTrajectoryLengthAdaptation` kernel.
      validate_args: Python `bool`. When `True`, kernel parameters are checked
        for validity. When `False`, invalid inputs may silently render incorrect
        outputs.
      name: Python `str` name prefixed to Ops created by this class. Default:
        'snaper_hamiltonian_monte_carlo'.
    """
    ...
  
  @property
  def target_log_prob_fn(self): # -> None:
    ...
  
  @property
  def step_size(self): # -> None:
    ...
  
  @property
  def num_adaptation_steps(self): # -> None:
    ...
  
  @property
  def num_mala_steps(self): # -> None:
    ...
  
  @property
  def max_leapfrog_steps(self): # -> None:
    ...
  
  @property
  def trajectory_length_adaptation_rate(self): # -> None:
    ...
  
  @property
  def state_ema_factor(self): # -> None:
    ...
  
  @property
  def principal_component_ema_factor(self): # -> None:
    ...
  
  @property
  def experimental_reduce_chain_axis_names(self): # -> None:
    ...
  
  @property
  def experimental_shard_axis_names(self): # -> None:
    ...
  
  @property
  def preconditioned_hamiltonian_monte_carlo_kwargs(self): # -> None:
    ...
  
  @property
  def gradient_based_trajectory_length_adaptation_kwargs(self): # -> None:
    ...
  
  @property
  def validate_args(self): # -> None:
    ...
  
  @property
  def name(self): # -> None:
    ...
  
  @property
  def parameters(self): # -> dict[str, Any | None]:
    ...
  
  def one_step(self, current_state, previous_kernel_results, seed=...): # -> tuple[Any, Any]:
    ...
  
  def bootstrap_results(self, init_state): # -> SNAPERHamiltonianMonteCarloResults:
    ...
  
  @property
  def is_calibrated(self): # -> Literal[True]:
    ...
  
  def experimental_with_shard_axes(self, shard_axis_names):
    ...
  


def default_snaper_trace_fn(state, is_burnin, kernel_results, reducer, reducer_state): # -> tuple[Any, dict[str, Any]]:
  """Default trace function for SNAPER."""
  ...

class SampleSNAPERHamiltonianMonteCarloResults(mcmc_util.PrettyNamedTupleMixin, collections.namedtuple('SampleSNAPERHamiltonianMonteCarloResults', ['trace', 'reduction_results', 'final_state', 'final_kernel_results'])):
  """Results of `sample_snaper_hmc`.

  Attributes:
    trace: Traced quantities defined by `trace_fn`.
    reduction_results: Finalized reducer results.
    final_state: Final state of the MCMC chain.
    final_kernel_results: The final results of `DualAveragingStepSizeAdaptation`
      wrapping `SNAPERHamiltonianMonteCarlo` kernels.
  """
  __slots__ = ...


def sample_snaper_hmc(model, num_results, reducer=..., trace_fn=..., num_burnin_steps=..., num_adaptation_steps=..., num_chains=..., discard_burnin_steps=..., num_steps_between_results=..., init_state=..., init_step_size=..., event_space_bijector=..., event_dtype=..., event_shape=..., experimental_shard_axis_names=..., experimental_reduce_chain_axis_names=..., dual_averaging_kwargs=..., snaper_kwargs=..., seed=..., validate_args=..., name=...): # -> SampleSNAPERHamiltonianMonteCarloResults:
  """Generates samples using SNAPER HMC [1] with step size adaptation.

  This utility function generates samples from a probabilistic model using
  `SNAPERHamiltonianMonteCarlo` kernel combined with
  `DualAveragingStepSizeAdaptation` kernel. The `model` argument can either be
  an instance of `tfp.distributions.Distribution` or a callable that computes
  the target log-density. In the latter case, it is also necessary to specify
  `event_space_bijector`, `event_dtype` and `event_shape` (these are inferred if
  `model` is a distribution instance).

  This function can accept a structure of `tfp.experimental.mcmc.Reducer`s,
  which allow computing streaming statitics with minimal memory usage. The
  reducers only incorporate samples after the burnin period.

  By default, this function traces the following quantities:

  - The chain state.
  - A dict of auxiliary information, using keys from ArviZ [2].
    - step_size: Float scalar `Tensor`. HMC step size.
    - n_steps: Int `Tensor`. Number of HMC leapfrog steps.
    - tune: Bool `Tensor`. Whether this step is part of the burnin.
    - max_trajectory_length: Float `Tensor`. Maximum HMC trajectory length.
    - variance_scaling: List of float `Tensor`s. The diagonal variance of the
      unconstrained state, used as the mass matrix.
    - diverging: Bool `Tensor`. Whether the sampler is divering.
    - accept_ratio: Float `Tensor`. Probability of acceptance of the proposal
      for this step.
    - is_accepted: Bool `Tensor. Whether this step is a result of an accepted
      proposal.

  It is possible to trace nothing at all, and rely on the reducers to compute
  the necessary statitiscs.

  Args:
    model: Either an instance of `tfp.distributions.Distribution` or a callable
      that evaluates the target log-density at a batch of chain states.
    num_results: Number of MCMC results to return after burnin.
    reducer: A structure of reducers.
    trace_fn: A callable with signature: `(state, is_burnin, kernel_results,
      reducer, reducer_state) -> structure` which defines what quantities to
      trace.
    num_burnin_steps: Python `int`. Number of burnin steps.
    num_adaptation_steps: Python `int`. Number of adaptation steps. Default:
      `0.9 * num_burnin_steps`.
    num_chains: Python `int`. Number of chains. This can be inferred from
      `init_state`. Otherwise, this is 64 by default.
    discard_burnin_steps: Python `bool`. Whether to discard the burnin steps
      when returning the trace. Burning steps are never used for the reducers.
    num_steps_between_results: Python `int`. Number of steps to take between
      MCMC results. This acts as a multiplier on the total number of steps taken
      by the MCMC (burnin included). The size of the output trace tensors is not
      affected, but each element is produced by this many sub-steps.
    init_state: Structure of `Tensor`s. Initial state of the chain. Default:
      `num_chains` worth of zeros in unconstrained space.
    init_step_size: Scalar float `Tensor`. Initial step size. Default: `1e-2 *
      total_num_dims ** -0.25`,
    event_space_bijector: Bijector or a list of bijectors used to go from
      unconstrained to constrained space to improve MCMC mixing. Default: Either
      inferred from `model` or an identity.
    event_dtype: Structure of dtypes. The event dtype. Default: Inferred from
      `model` or `init_state`.
    event_shape: Structure of tuples. The event shape. Default: Inferred from
      `model` or `init_state`.
    experimental_shard_axis_names: A structure of string names indicating how
      members of the state are sharded.
    experimental_reduce_chain_axis_names: A string or list of string names
      indicating which named axes to average cross-chain statistics over.
    dual_averaging_kwargs: Keyword arguments passed into
      `DualAveragingStepSizeAdaptation` kernel. Default: `{'target_accept_prob':
      0.8}`.
    snaper_kwargs: Keyword arguments passed into `SNAPERHamiltonianMonteCarlo`
      kernel. Default: `{}`.
    seed: PRNG seed; see `tfp.random.sanitize_seed` for details.
    validate_args: Python `bool`. When `True`, kernel parameters are checked
      for validity. When `False`, invalid inputs may silently render incorrect
      outputs.
    name: Python `str` name prefixed to Ops created by this class.

  Returns:
    results: `SampleSNAPERHamiltonianMonteCarloResults`.

  #### Tuning

  The defaults for this function should function well for many models, but it
  does provide a number of arguments for verifying sampler behavior. If there's
  a question of efficiency, the first thing to do is to set
  `discard_burnin_steps=False` and examine the `step_size` and
  `max_trajectory_length` and `variance_scaling` traces. A well-functioning
  sampler will have these quantities converge before sampling begins. If they
  are not converged, consider increasing `num_burnin_steps`, or adjusting the
  `snaper_kwargs` to tune SNAPER more.

  #### Examples

  Here we sample from a simple model while performing a reduction.

  ```
  num_dims = 8

  eigenvalues = np.exp(np.linspace(0., 3., num_dims))
  q, r = np.linalg.qr(np.random.randn(num_dims, num_dims))
  q *= np.sign(np.diag(r))
  covariance = (q * eigenvalues).dot(q.T).astype(self.dtype)

  gaussian = tfd.MultivariateNormalTriL(
      loc=tf.zeros(num_dims, self.dtype),
      scale_tril=tf.linalg.cholesky(covariance),
  )

  @tf.function(jit_compile=True)
  def run():
    results = tfp.experimental.mcmc.sample_snaper_hmc(
        model=gaussian,
        num_results=500,
        reducer=tfp.experimental.mcmc.PotentialScaleReductionReducer(),
    )

    return results.trace, results.reduction_results

  (chain, trace), potential_scale_reduction = run(tfp.random.sanitize_seed(0))

  # Compute sampler diagnostics.

  # Should be high (at least 100-1000).
  tfp.mcmc.effective_sample_size(chain, cross_chain_dims=1)
  # Should be close to 1.
  potential_scale_reduction

  # Compute downstream statistics.

  # Should be close to np.diag(covariance)
  tf.math.reduce_variance(chain, [0, 1])
  ```

  #### References

  [1]: Sountsov, P. & Hoffman, M. (2021). Focusing on Difficult Directions for
       Learning HMC Trajectory Lengths. <https://arxiv.org/abs/2110.11576>

  [2]: Kumar, R., Carroll, C., Hartikainen, A., & Martin, O. (2019). ArviZ a
       unified library for exploratory analysis of Bayesian models in Python.
       Journal of Open Source Software, 4(33), 1143.

  """
  ...

class _SNAPERReducer(reducer_lib.Reducer):
  """A Reducer utility wrapper for `snaper_hmc`.

  This does two things:
  - Pre-transforms the chain state.
  - Prevents reduction before num_burnin_steps.
  """
  def __init__(self, reducer, num_burnin_steps, flat_event_space_bijector) -> None:
    ...
  
  def initialize(self, initial_chain_state, initial_inner_kernel_results):
    ...
  
  def one_step(self, new_chain_state, current_reducer_state, previous_kernel_results): # -> Any:
    ...
  
  def finalize(self, final_reducer_state): # -> defaultdict[Any, Any] | Any | list[Any] | None:
    ...
  


