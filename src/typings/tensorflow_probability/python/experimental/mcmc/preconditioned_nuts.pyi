"""
This type stub file was generated by pyright.
"""

import collections
from tensorflow_probability.python.mcmc.internal import util as mcmc_util
from tensorflow_probability.python.mcmc.kernel import TransitionKernel

"""No U-Turn Sampler.

The implementation closely follows [1; Algorithm 3], with Multinomial sampling
on the tree (instead of slice sampling) and a generalized No-U-Turn termination
criterion [2; Appendix A].

Achieves batch execution across chains by precomputing the recursive tree
doubling data access patterns and then executes this "unrolled" data pattern via
a `tf.while_loop`.

#### References

[1]: Matthew D. Hoffman, Andrew Gelman. The No-U-Turn Sampler: Adaptively
     Setting Path Lengths in Hamiltonian Monte Carlo.
     In _Journal of Machine Learning Research_, 15(1):1593-1623, 2014.
     http://jmlr.org/papers/volume15/hoffman14a/hoffman14a.pdf

[2]: Michael Betancourt. A Conceptual Introduction to Hamiltonian Monte Carlo.
     _arXiv preprint arXiv:1701.02434_, 2018. https://arxiv.org/abs/1701.02434
"""
JAX_MODE = ...
TREE_COUNT_DTYPE = ...
MULTINOMIAL_SAMPLE = ...
GENERALIZED_UTURN = ...
__all__ = ['PreconditionedNoUTurnSampler']
class PreconditionedNUTSKernelResults(mcmc_util.PrettyNamedTupleMixin, collections.namedtuple('PreconditionedNUTSKernelResults', ['target_log_prob', 'grads_target_log_prob', 'step_size', 'log_accept_ratio', 'leapfrogs_taken', 'is_accepted', 'reach_max_depth', 'has_divergence', 'energy', 'momentum_distribution', 'seed'])):
  """Internal state and diagnostics for No-U-Turn Sampler."""
  __slots__ = ...


class VelocityStateSwap(mcmc_util.PrettyNamedTupleMixin, collections.namedtuple('VelocityStateSwap', ['velocity_swap', 'state_swap'])):
  """Internal state and diagnostics for No-U-Turn Sampler."""
  __slots__ = ...


class OneStepMetaInfo(mcmc_util.PrettyNamedTupleMixin, collections.namedtuple('OneStepMetaInfo', ['log_slice_sample', 'init_energy', 'write_instruction', 'read_instruction'])):
  """Internal state and diagnostics for No-U-Turn Sampler."""
  __slots__ = ...


class TreeDoublingState(mcmc_util.PrettyNamedTupleMixin, collections.namedtuple('TreeDoublingState', ['momentum', 'velocity', 'state', 'target', 'target_grad_parts'])):
  """Internal state and diagnostics for No-U-Turn Sampler."""
  __slots__ = ...


class TreeDoublingStateCandidate(mcmc_util.PrettyNamedTupleMixin, collections.namedtuple('TreeDoublingStateCandidate', ['state', 'target', 'target_grad_parts', 'energy', 'weight'])):
  """Internal state and diagnostics for No-U-Turn Sampler."""
  __slots__ = ...


class TreeDoublingMetaState(mcmc_util.PrettyNamedTupleMixin, collections.namedtuple('TreeDoublingMetaState', ['candidate_state', 'is_accepted', 'momentum_sum', 'energy_diff_sum', 'leapfrog_count', 'continue_tree', 'not_divergence'])):
  """Internal state and diagnostics for No-U-Turn Sampler."""
  __slots__ = ...


class PreconditionedNoUTurnSampler(TransitionKernel):
  """Runs one step of the No U-Turn Sampler.

  The No U-Turn Sampler (NUTS) is an adaptive variant of the Hamiltonian Monte
  Carlo (HMC) method for MCMC. NUTS adapts the distance traveled in response to
  the curvature of the target density. Conceptually, one proposal consists of
  reversibly evolving a trajectory through the sample space, continuing until
  that trajectory turns back on itself (hence the name, 'No U-Turn'). This class
  implements one random NUTS step from a given `current_state`.
  Mathematical details and derivations can be found in
  [Hoffman, Gelman (2011)][1] and [Betancourt (2018)][2].

  The `one_step` function can update multiple chains in parallel. It assumes
  that a prefix of leftmost dimensions of `current_state` index independent
  chain states (and are therefore updated independently).  The output of
  `target_log_prob_fn(*current_state)` should sum log-probabilities across all
  event dimensions.  Slices along the rightmost dimensions may have different
  target distributions; for example, `current_state[0][0, ...]` could have a
  different target distribution from `current_state[0][1, ...]`.  These
  semantics are governed by `target_log_prob_fn(*current_state)`. (The number of
  independent chains is `tf.size(target_log_prob_fn(*current_state))`.)

  #### References

  [1]: Matthew D. Hoffman, Andrew Gelman.  The No-U-Turn Sampler: Adaptively
  Setting Path Lengths in Hamiltonian Monte Carlo.  2011.
  https://arxiv.org/pdf/1111.4246.pdf.

  [2]: Michael Betancourt. A Conceptual Introduction to Hamiltonian Monte Carlo.
  _arXiv preprint arXiv:1701.02434_, 2018. https://arxiv.org/abs/1701.02434
  """
  def __init__(self, target_log_prob_fn, step_size, momentum_distribution=..., max_tree_depth=..., max_energy_diff=..., unrolled_leapfrog_steps=..., parallel_iterations=..., experimental_shard_axis_names=..., name=...) -> None:
    """Initializes this transition kernel.

    Args:
      target_log_prob_fn: Python callable which takes an argument like
        `current_state` (or `*current_state` if it's a list) and returns its
        (possibly unnormalized) log-density under the target distribution.
      step_size: `Tensor` or Python `list` of `Tensor`s representing the step
        size for the leapfrog integrator. Must broadcast with the shape of
        `current_state`. Larger step sizes lead to faster progress, but
        too-large step sizes make rejection exponentially more likely. When
        possible, it's often helpful to match per-variable step sizes to the
        standard deviations of the target distribution in each variable.
      momentum_distribution: A `tfp.distributions.Distribution` instance to draw
        momentum from. Defaults to normal distributions with identity
        covariance.
      max_tree_depth: Maximum depth of the tree implicitly built by NUTS. The
        maximum number of leapfrog steps is bounded by `2**max_tree_depth` i.e.
        the number of nodes in a binary tree `max_tree_depth` nodes deep. The
        default setting of 10 takes up to 1024 leapfrog steps.
      max_energy_diff: Scaler threshold of energy differences at each leapfrog,
        divergence samples are defined as leapfrog steps that exceed this
        threshold. Default to 1000.
      unrolled_leapfrog_steps: The number of leapfrogs to unroll per tree
        expansion step. Applies a direct linear multipler to the maximum
        trajectory length implied by max_tree_depth. Defaults to 1.
      parallel_iterations: The number of iterations allowed to run in parallel.
        It must be a positive integer. See `tf.while_loop` for more details.
      experimental_shard_axis_names: A structure of string names indicating how
        members of the state are sharded.
      name: Python `str` name prefixed to Ops created by this function.
        Default value: `None` (i.e., 'PreconditionedNoUTurnSampler').
    """
    ...
  
  @property
  def target_log_prob_fn(self): # -> Any:
    ...
  
  @property
  def step_size(self): # -> Any:
    ...
  
  @property
  def max_tree_depth(self):
    ...
  
  @property
  def max_energy_diff(self): # -> float:
    ...
  
  @property
  def unrolled_leapfrog_steps(self): # -> int:
    ...
  
  @property
  def name(self):
    ...
  
  @property
  def parallel_iterations(self): # -> int:
    ...
  
  @property
  def write_instruction(self): # -> Any:
    ...
  
  @property
  def read_instruction(self): # -> NDArray[Any]:
    ...
  
  @property
  def parameters(self): # -> dict[str, Any | None]:
    ...
  
  @property
  def is_calibrated(self): # -> Literal[True]:
    ...
  
  def one_step(self, current_state, previous_kernel_results, seed=...): # -> tuple[Any, PreconditionedNUTSKernelResults]:
    ...
  
  def init_velocity_state_memory(self, input_tensors): # -> list[Any]:
    """Allocate TensorArray for storing state and momentum."""
    ...
  
  def bootstrap_results(self, init_state): # -> PreconditionedNUTSKernelResults:
    """Creates initial `previous_kernel_results` using a supplied `state`."""
    ...
  
  @property
  def experimental_shard_axis_names(self): # -> None:
    ...
  
  def experimental_with_shard_axes(self, shard_axis_names): # -> Self:
    ...
  


def has_not_u_turn_at_all_index(read_indexes, direction, velocity_state_memory, velocity_right, state_right, no_u_turns_within_tree, log_prob_rank, shard_axis_names=...):
  """Check u turn for early stopping."""
  ...

def has_not_u_turn(state_diff, velocity_left, velocity_right, log_prob_rank, shard_axis_names=...): # -> bool:
  """If the trajectory does not exhibit a U-turn pattern."""
  ...

def build_tree_uturn_instruction(max_depth, init_memory=...): # -> NDArray[signedinteger[_32Bit]]:
  """Run build tree and output the u turn checking input instruction."""
  ...

def generate_efficient_write_read_instruction(instruction_array): # -> tuple[Any, NDArray[Any]]:
  """Statically generate a memory efficient write/read instruction."""
  ...

def compute_hamiltonian(target_log_prob, momentum_parts, momentum_distribution):
  """Compute the Hamiltonian of the current system."""
  ...

def get_kinetic_energy_fn(momentum_distribution): # -> Callable[..., Any]:
  """Convert a momentum distribution to a kinetic energy function."""
  ...

