"""
This type stub file was generated by pyright.
"""

import collections
from tensorflow_probability.python.mcmc.internal import util as mcmc_util

"""Experimental MCMC driver, `sample_sequential_monte_carlo`."""
__all__ = ['default_make_hmc_kernel_fn', 'gen_make_hmc_kernel_fn', 'gen_make_transform_hmc_kernel_fn', 'make_rwmh_kernel_fn', 'sample_sequential_monte_carlo', 'simple_heuristic_tuning']
PRINT_DEBUG = ...
class ParticleInfo(mcmc_util.PrettyNamedTupleMixin, collections.namedtuple('ParticleInfo', ['log_accept_prob', 'log_scalings', 'tempered_log_prob', 'likelihood_log_prob'])):
  """Internal particle state for Sequential Monte Carlos."""
  __slots__ = ...


class SMCResults(mcmc_util.PrettyNamedTupleMixin, collections.namedtuple('SMCResults', ['num_steps', 'inverse_temperature', 'log_marginal_likelihood', 'particle_info'])):
  """Result state for Sequential Monte Carlos."""
  __slots__ = ...


def gather_mh_like_result(results): # -> tuple[Any, Any]:
  """Gather log_accept_ratio and target_log_prob from kernel result."""
  ...

def default_make_tempered_target_log_prob_fn(prior_log_prob_fn, likelihood_log_prob_fn, inverse_temperatures): # -> Callable[..., Any]:
  """Helper which creates inner kernel target_log_prob_fn."""
  ...

def make_rwmh_kernel_fn(target_log_prob_fn, init_state, scalings):
  """Generate a Random Walk MH kernel."""
  ...

def compute_hmc_step_size(scalings, state_std, num_leapfrog_steps): # -> list[Any]:
  ...

def gen_make_transform_hmc_kernel_fn(unconstraining_bijectors, num_leapfrog_steps=...): # -> Callable[..., Any]:
  """Generate a transformed hmc kernel."""
  ...

def gen_make_hmc_kernel_fn(num_leapfrog_steps=...): # -> Callable[..., Any]:
  """Generate a transformed hmc kernel."""
  ...

default_make_hmc_kernel_fn = ...
def simple_heuristic_tuning(num_steps, log_scalings, log_accept_prob, optimal_accept=..., target_accept_prob=..., name=...): # -> tuple[Any, Any]:
  """Tune the number of steps and scaling of one mutation.

  # TODO(b/152412213): Better explanation of the heuristic used here.

  This is a simple heuristic for tuning the number of steps of the next
  mutation, as well as the scaling of a transition kernel (e.g., step size in
  HMC, scale of a Normal proposal in RWMH) using the acceptance probability from
  the previous mutation stage in SMC.

  Args:
    num_steps: The initial number of steps for the next mutation, to be tune.
    log_scalings: The log of the scale of the proposal kernel
    log_accept_prob: The log of the acceptance ratio from the last mutation.
    optimal_accept: Optimal acceptance ratio for a Transitional Kernel. Default
      value is 0.234 (Optimal for Random Walk Metropolis kernel).
    target_accept_prob: Target acceptance probability at the end of one mutation
      step. Default value: 0.99
    name: Python `str` name prefixed to Ops created by this function.
      Default value: `None`.

  Returns:
    num_steps: The number of steps for the next mutation.
    new_log_scalings: The log of the scale of the proposal kernel for the next
      mutation.

  """
  ...

def sample_sequential_monte_carlo(prior_log_prob_fn, likelihood_log_prob_fn, current_state, min_num_steps=..., max_num_steps=..., max_stage=..., make_kernel_fn=..., tuning_fn=..., make_tempered_target_log_prob_fn=..., resample_fn=..., ess_threshold_ratio=..., parallel_iterations=..., seed=..., name=...): # -> tuple[Any, Any, Any]:
  """Runs Sequential Monte Carlo to sample from the posterior distribution.

  This function uses an MCMC transition operator (e.g., Hamiltonian Monte Carlo)
  to sample from a series of distributions that slowly interpolates between
  an initial 'prior' distribution:

    `exp(prior_log_prob_fn(x))`

  and the target 'posterior' distribution:

    `exp(prior_log_prob_fn(x) + target_log_prob_fn(x))`,

  by mutating a collection of MC samples (i.e., particles). The approach is also
  known as Particle Filter in some literature. The current implemenetation is
  largely based on Del Moral et al [1], which adapts the tempering sequence
  adaptively (base on the effective sample size) and the scaling of the mutation
  kernel (base on the sample covariance of the particles) at each stage.

  Args:
    prior_log_prob_fn: Python callable that returns the log density of the
      prior distribution.
    likelihood_log_prob_fn: Python callable which takes an argument like
      `current_state` (or `*current_state` if it's a list) and returns its
      (possibly unnormalized) log-density under the likelihood distribution.
    current_state: Nested structure of `Tensor`s, each of shape
      `concat([[num_particles, b1, ..., bN], latent_part_event_shape])`, where
      `b1, ..., bN` are optional batch dimensions. Each batch represents an
      independent SMC run.
    min_num_steps: The minimal number of kernel transition steps in one mutation
      of the MC samples.
    max_num_steps: The maximum number of kernel transition steps in one mutation
      of the MC samples. Note that the actual number of steps in one mutation is
      tuned during sampling and likely lower than the max_num_step.
    max_stage: Integer number of the stage for increasing the temperature
      from 0 to 1.
    make_kernel_fn: Python `callable` which returns a `TransitionKernel`-like
      object. Must take one argument representing the `TransitionKernel`'s
      `target_log_prob_fn`. The `target_log_prob_fn` argument represents the
      `TransitionKernel`'s target log distribution.  Note:
        `sample_sequential_monte_carlo` creates a new `target_log_prob_fn` which
        is an interpolation between the supplied `target_log_prob_fn` and
        `proposal_log_prob_fn`; it is this interpolated function which is used
        as an argument to `make_kernel_fn`.
    tuning_fn: Python `callable` which takes the number of steps, the log
      scaling, and the log acceptance ratio from the last mutation and output
      the number of steps and log scaling for the next mutation.
    make_tempered_target_log_prob_fn: Python `callable` that takes the
      `prior_log_prob_fn`, `likelihood_log_prob_fn`, and `inverse_temperatures`
      and creates a `target_log_prob_fn` `callable` that pass to
      `make_kernel_fn`.
    resample_fn: Python `callable` to generate the indices of resampled
      particles, given their weights. Generally, one of
      `tfp.experimental.mcmc.resample_independent` or
      `tfp.experimental.mcmc.resample_systematic`, or any function with the same
      signature.
      Default value: `tfp.experimental.mcmc.resample_systematic`.
    ess_threshold_ratio: Target ratio for effective sample size.
    parallel_iterations: The number of iterations allowed to run in parallel. It
      must be a positive integer. See `tf.while_loop` for more details.
    seed: Python integer or TFP seedstream to seed the random number generator.
    name: Python `str` name prefixed to Ops created by this function.
      Default value: `None` (i.e., 'sample_sequential_monte_carlo').

  Returns:
    n_stage: Number of the mutation stage SMC ran.
    final_state: `Tensor` or Python `list` of `Tensor`s representing the
      final state(s) of the Markov chain(s). The output are the posterior
      samples.
    final_kernel_results: `collections.namedtuple` of internal calculations used
      to advance the chain.

  #### References

  [1] Del Moral, Pierre, Arnaud Doucet, and Ajay Jasra. An adaptive sequential
      Monte Carlo method for approximate Bayesian computation.
      _Statistics and Computing_, 22.5(1009-1020), 2012.

  """
  ...

