"""
This type stub file was generated by pyright.
"""

"""Utility functions for building neural networks."""
__all__ = ['batchify_op', 'display_imgs', 'expand_dims', 'flatten_rightmost', 'halflife_decay', 'make_fit_op', 'tfcompile', 'trace', 'tune_dataset', 'variables_load', 'variables_save', 'variables_summary']
def display_imgs(x, title=..., fignum=...): # -> tuple[Any, Any]:
  """Display images as a grid."""
  ...

def tune_dataset(dataset, batch_size=..., shuffle_size=..., preprocess_fn=..., repeat_count=...):
  """Sets generally recommended parameters for a `tf.data.Dataset`.

  Args:
    dataset: `tf.data.Dataset`-like instance to be tuned according to this
      functions arguments.
    batch_size: Python `int` representing the number of elements in each
      minibatch.
    shuffle_size: Python `int` representing the number of elements to shuffle
      (at a time).
    preprocess_fn: Python `callable` applied to each item in `dataset`.
    repeat_count: Python `int`, representing the number of times the dataset
      should be repeated. The default behavior (`repeat_count = -1`) is for the
      dataset to be repeated indefinitely. If `repeat_count is None` repeat is
      "off;" note that this is a deviation from `tf.data.Dataset.repeat` which
      interprets `None` as "repeat indefinitely".
      Default value: `-1` (i.e., repeat indefinitely).

  Returns:
    tuned_dataset: `tf.data.Dataset` instance tuned according to this functions
      arguments.

  #### Example

  ```python
  [train_dataset, eval_dataset], datasets_info = tfds.load(
       name='mnist',
       split=['train', 'test'],
       with_info=True,
       as_supervised=True,
       shuffle_files=True)

  def _preprocess(image, label):
    image = tf.cast(image, dtype=tf.int32)
    u = tf.random.uniform(shape=tf.shape(image), maxval=256, dtype=image.dtype)
    image = tf.cast(u < image, dtype=tf.float32)   # Randomly binarize.
    return image, label

  # TODO(b/144500779): Cant use `jit_compile=True`.
  @tf.function(autograph=False)
  def one_step(iter):
    x, y = next(iter)
    return tf.reduce_mean(x)

  ds = tune_dataset(
      train_dataset,
      batch_size=32,
      shuffle_size=int(datasets_info.splits['train'].num_examples / 7),
      preprocess_fn=_preprocess)
  it = iter(ds)
  [one_step(it)]*3  # Build graph / burn-in.
  %time one_step(it)
  ```

  """
  ...

def tfcompile(func=..., tf_function=..., xla_best_effort=..., xla_compile_all=...): # -> Callable[..., Any] | Callable[..., _Wrapped[Callable[..., Any], Any, Callable[..., Any], Any]] | _Wrapped[Callable[..., Any], Any, Callable[..., Any], Any]:
  """Centralizes TF compilation related options.

  Args:
    func: Python `callable` to wrapped with the specified TF compilation
      directives.
      Default value: `None`.
    tf_function: `bool` representing whether the resulting function should be
      `tf.function` decoreated.
      Default value: `True`.
    xla_best_effort: `bool` representing whether XLA auto-clustering compilation
      should be performed. (This argument is ignored if the function is executed
      eagerly.)
      Default value: `True`.
    xla_compile_all: `bool` representing whether XLA compilation should be
      performed. (This argument overrides both `tf_function` and
      `xla_best_effort`.
      Default value: `False`.

  Returns:
    wrapped_func: A Python `callable` with the specified compilation directives
      embedded.

  ### Example Usage

  ```python
  tfn = tfp.experimental.nn

  # Use style #1.
  @tfn.util.tfcompile(xla_compile_all=True)
  def foo(...):
       ...

  # Use style #2.
  def foo(...):
    ...
  foo = tfn.util.tfcompile(xla_compile_all=True)(foo)
  ```

  """
  ...

def make_fit_op(loss_fn, optimizer, trainable_variables, grad_summary_fn=..., tf_function=..., xla_compile=...): # -> Callable[..., tuple[Any, Any, Any] | tuple[Any, Any]]:
  """One training step.

  Args:
    loss_fn: Python `callable` which returns the pair `loss` (`tf.Tensor`) and
      any other second result such that
      `tf.nest.map_structure(tf.convert_to_tensor, other)` will succeed.
    optimizer: `tf_keras.optimizers.Optimizer`-like instance which has members
      `gradient` and `apply_gradients`.
    trainable_variables: `tf.nest.flatten`-able structure of `tf.Variable`
      instances.
    grad_summary_fn: Python `callable` which takes a `trainable_variables`-like
      structure of `tf.Tensor`s representing the gradient of the result of
      `loss_fn` with respect to `trainable_variables`. For example,
      `lambda grads: tf.nest.map_structure(
         lambda x: 0. if x is None else tf.norm(x), grads)`.
      Default value: `None` (i.e., no summarization is made).
    tf_function: `bool` representing whether the resulting function should be
      `tf.function` decoreated.
      Default value: `True`.
    xla_compile: `bool` representing whether XLA compilation should be
      performed. (This argument is ignored if the function is executed eagerly.)
      Default value: `True`.

  Returns:
    fit_op: A Python `callable` taking args which are forwarded to `loss_fn` and
      such that when called `trainable_variables` are updated per the logic of
      `optimizer.apply_gradients`.
  """
  ...

def flatten_rightmost(ndims=...): # -> Callable[..., Any]:
  """Flatten rightmost dims."""
  ...

def trace(name=...): # -> Callable[..., Any]:
  """Returns a function which prints info related to input."""
  ...

def expand_dims(axis, name=...): # -> Callable[..., Any]:
  """Like `tf.expand_dims` but accepts a vector of axes to expand."""
  ...

def variables_save(filename, variables): # -> None:
  """Saves structure of `tf.Variable`s to `filename`."""
  ...

def variables_load(filename, variables):
  """Assigns values to structure of `tf.Variable`s from `filename`."""
  ...

def variables_summary(variables, name=...): # -> LiteralString:
  """Returns a list of summarizing `str`s."""
  ...

def halflife_decay(time_step, half_life, initial, final=..., dtype=..., name=...):
  """Interpolates `initial` to `final` using halflife (exponential) decay."""
  ...

def batchify_op(op, op_min_input_ndims, x, *other_op_args):
  """Reshape `op` input `x` to be a vec of `op_min_input_ndims`-rank tensors."""
  ...

