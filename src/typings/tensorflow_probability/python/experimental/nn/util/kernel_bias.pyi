"""
This type stub file was generated by pyright.
"""

"""Utility functions for building layer kernels and biases."""
__all__ = ['make_kernel_bias', 'make_kernel_bias_posterior_mvn_diag', 'make_kernel_bias_prior_spike_and_slab']
def make_kernel_bias(kernel_shape, bias_shape, kernel_initializer=..., bias_initializer=..., kernel_batch_ndims=..., bias_batch_ndims=..., dtype=..., kernel_name=..., bias_name=...): # -> tuple[Any, Any]:
  """Creates kernel and bias as `tf.Variable`s.

  Args:
    kernel_shape: ...
    bias_shape: ...
    kernel_initializer: ...
      Default value: `None` (i.e., `tf_keras.initializers.glorot_uniform()`).
    bias_initializer: ...
      Default value: `None` (i.e., `tf_keras.initializers.zeros()`).
    kernel_batch_ndims: ...
      Default value: `0`.
    bias_batch_ndims: ...
      Default value: `0`.
    dtype: ...
      Default value: `tf.float32`.
    kernel_name: ...
      Default value: `"kernel"`.
    bias_name: ...
      Default value: `"bias"`.

  Returns:
    kernel: ...
    bias: ...

  #### Recommendations:

  ```python
  #   tf.nn.relu    ==> tf_keras.initializers.he_*
  #   tf.nn.elu     ==> tf_keras.initializers.he_*
  #   tf.nn.selu    ==> tf_keras.initializers.lecun_*
  #   tf.nn.tanh    ==> tf_keras.initializers.glorot_*
  #   tf.nn.sigmoid ==> tf_keras.initializers.glorot_*
  #   tf.nn.softmax ==> tf_keras.initializers.glorot_*
  #   None          ==> tf_keras.initializers.glorot_*
  # https://towardsdatascience.com/hyper-parameters-in-action-part-ii-weight-initializers-35aee1a28404
  # https://stats.stackexchange.com/a/393012/1835

  def make_uniform(size):
    s = tf.math.rsqrt(size / 3.)
    return tfd.Uniform(low=-s, high=s)

  def make_normal(size):
    # Constant is: `scipy.stats.truncnorm.std(loc=0., scale=1., a=-2., b=2.)`.
    s = tf.math.rsqrt(size) / 0.87962566103423978
    return tfd.TruncatedNormal(loc=0, scale=s, low=-2., high=2.)

  # He.  https://arxiv.org/abs/1502.01852
  he_uniform = make_uniform(fan_in / 2.)
  he_normal  = make_normal (fan_in / 2.)

  # Glorot (aka Xavier). http://proceedings.mlr.press/v9/glorot10a.html
  glorot_uniform = make_uniform((fan_in + fan_out) / 2.)
  glorot_normal  = make_normal ((fan_in + fan_out) / 2.)
  ```

  """
  ...

def make_kernel_bias_prior_spike_and_slab(kernel_shape, bias_shape, kernel_initializer=..., bias_initializer=..., kernel_batch_ndims=..., bias_batch_ndims=..., dtype=..., kernel_name=..., bias_name=...): # -> JointDistributionSequential:
  """Create prior for Variational layers with kernel and bias.

  Note: Distribution scale is inversely related to regularization strength.
  Consider a "Normal" prior; bigger scale corresponds to less L2 regularization.
  I.e.,
  ```python
  scale    = (2. * l2weight)**-0.5
  l2weight = scale**-2. / 2.
  ```
  have a similar regularizing effect.

  The std. deviation of each of the component distributions returned by this
  function is approximately `1415` (or approximately `l2weight = 25e-6`). In
  other words this prior is extremely "weak".

  Args:
    kernel_shape: ...
    bias_shape: ...
    kernel_initializer: Ignored.
      Default value: `None` (i.e., `tf_keras.initializers.glorot_uniform()`).
    bias_initializer: Ignored.
      Default value: `None` (i.e., `tf_keras.initializers.zeros()`).
    kernel_batch_ndims: ...
      Default value: `0`.
    bias_batch_ndims: ...
      Default value: `0`.
    dtype: ...
      Default value: `tf.float32`.
    kernel_name: ...
      Default value: `"prior_kernel"`.
    bias_name: ...
      Default value: `"prior_bias"`.

  Returns:
    kernel_and_bias_distribution: ...
  """
  ...

def make_kernel_bias_posterior_mvn_diag(kernel_shape, bias_shape, kernel_initializer=..., bias_initializer=..., kernel_batch_ndims=..., bias_batch_ndims=..., dtype=..., kernel_name=..., bias_name=...): # -> JointDistributionSequential:
  """Create learnable posterior for Variational layers with kernel and bias.

  Args:
    kernel_shape: ...
    bias_shape: ...
    kernel_initializer: ...
      Default value: `None` (i.e., `tf_keras.initializers.glorot_uniform()`).
    bias_initializer: ...
      Default value: `None` (i.e., `tf_keras.initializers.zeros()`).
    kernel_batch_ndims: ...
      Default value: `0`.
    bias_batch_ndims: ...
      Default value: `0`.
    dtype: ...
      Default value: `tf.float32`.
    kernel_name: ...
      Default value: `"posterior_kernel"`.
    bias_name: ...
      Default value: `"posterior_bias"`.

  Returns:
    kernel_and_bias_distribution: ...
  """
  ...

