"""
This type stub file was generated by pyright.
"""

import abc
import functools
import six
import tensorflow.compat.v2 as tf
from tensorflow_probability.python.internal import auto_composite_tensor
from tensorflow.python.framework import tensor_spec, type_spec

"""Class definitions for declaritive (vs imperative) `Tensors` & `Variables`."""
__all__ = ['DeferredTensor', 'TransformedVariable']
JAX_MODE = ...
NUMPY_MODE = ...
_identity = ...
class TensorMetaClass(abc.ABCMeta):
  """A type of class which will make objects which act like Tensors."""
  def __new__(mcs, name, bases, attrs): # -> Self:
    ...
  


NONE_SPECIFIED = ...
class DeferredTensor(six.with_metaclass(TensorMetaClass, tf.Module, tf.__internal__.CompositeTensor)):
  """Variable tracking object which applies function upon `convert_to_tensor`.

  #### Example

  ```python
  import tensorflow.compat.v2 as tf
  import tensorflow_probability as tfp
  tfb = tfp.bijectors
  tfd = tfp.distributions

  # Note: it'd be better to use `tfp.util.TransformedVariable`;
  #       this example is for illustration only.
  trainable_normal = tfd.Normal(
      loc=tf.Variable(0.),
      scale=tfp.util.DeferredTensor(tf.Variable(0.), tf.math.exp))

  trainable_normal.loc
  # ==> <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.0>

  trainable_normal.scale
  # ==> <DeferredTensor: dtype=float32, shape=[], fn=exp>

  # Operators work with `DeferredTensor`.
  trainable_normal.scale + 1.
  # ==> 2.

  with tf.GradientTape() as tape:
    negloglik = -trainable_normal.log_prob(0.5)
  g = tape.gradient(negloglik, trainable_normal.trainable_variables)
  # ==> (-0.5, 0.75)
  ```

  Which we could then fit as:

  ```python
  opt = tf_keras.optimizers.Adam(learning_rate=0.05)
  loss = tf.function(lambda: -trainable_normal.log_prob(0.5), autograph=True)
  for _ in range(int(1e3)):
    opt.minimize(loss, trainable_normal.trainable_variables)
  trainable_normal.mean()
  # ==> 0.5
  trainable_normal.stddev()
  # ==> (approximately) 0.0075
  ```

  It is also possible to parameterize a `DeferredTensor` with a bijector, e.g.:

  ```python
  # Note: it'd be better to use `tfp.util.TransformedVariable`;
  #       this example is for illustration only.
  d = tfd.Normal(loc=0.,
                 scale=tfp.util.DeferredTensor(tf.Variable([0.54, 1.85]),
                                               tfb.Softplus()))
  d.stddev()
  # ==> [1., 2.]
  tf.convert_to_tensor(d.scale)
  # ==> [1., 2.]
  ```

  """
  def __init__(self, pretransformed_input, transform_fn, dtype=..., shape=..., also_track=..., name=...) -> None:
    """Creates the `DeferredTensor` object.

    Args:
      pretransformed_input: object with `shape`, `dtype` properties (typically a
        `tf.Variable`) passed into `transform_fn` when this object is acted upon
        in a `Tensor` context, eg, `tf.convert_to_tensor`, `+`, `tf.math.exp`,
        etc.
      transform_fn: Python `callable` or `tfp.bijectors.Bijector`-like instance.
        When `callable`, should take `pretransformed_input` and
        return a `Tensor` (representing by this object).
      dtype: Equivalent to what would otherwise be
      `transform_fn(pretransformed_input).dtype`.
         Default value: `None` (i.e.,
         `getattr(transform_fn, 'dtype', None) or pretransformed_input.dtype`).
      shape: Equivalent to what would otherwise be
        `transform_fn(pretransformed_input).shape`.
         Default value: `'None'` (i.e.,
         `getattr(transform_fn, 'forward_event_shape', lambda x: x)(
              pretransformed_input.shape)`).
      also_track: Optional instance or structure of instances of `tf.Variable`
        and/or `tf.Module`, containing any additional trainable variables that
        the `transform_fn` may access beyond the given
        `pretransformed_input`. This ensures that such variables
        will be correctly tracked in `self.trainable_variables`.
        Default value: `None`.
      name: Python `str` representing this object's `name`; used only in graph
        mode.
        Default value: `None` (i.e.,
        `(getattr(transform_fn, 'name', None) or
          transform_fn.__name__ + '_' + pretransformed_input.name)`).

    Raises:
      TypeError: if `transform_fn` is not `callable`.
      TypeError: if `pretransformed_input` lacks `dtype` and/or `shape`
        properties (and `dtype` and/or `shape` arguments are unspecified).
    """
    ...
  
  @property
  def transform_fn(self): # -> Any | Callable[..., object]:
    """Function which characterizes the `Tensor`ization of this object."""
    ...
  
  @property
  def pretransformed_input(self):
    """Input to `transform_fn`."""
    ...
  
  @property
  def dtype(self): # -> None:
    """Represents the type of the elements in a `Tensor`."""
    ...
  
  @property
  def shape(self):
    """Represents the shape of a `Tensor`."""
    ...
  
  @property
  def also_track(self): # -> None:
    """Additional variables tracked by tf.Module in self.trainable_variables."""
    ...
  
  @property
  def name(self):
    """The string name of this object."""
    ...
  
  def numpy(self): # -> NDArray[Any]:
    """Returns (copy of) deferred values as a NumPy array or scalar."""
    ...
  
  def set_shape(self, shape): # -> None:
    """Updates the shape of this pretransformed_input.

    This method can be called multiple times, and will merge the given `shape`
    with the current shape of this object. It can be used to provide additional
    information about the shape of this object that cannot be inferred from the
    graph alone.

    Args:
      shape: A `TensorShape` representing the shape of this
        `pretransformed_input`, a `TensorShapeProto`, a list, a tuple, or None.

    Raises:
      ValueError: If `shape` is not compatible with the current shape of this
        `pretransformed_input`.
    """
    ...
  
  def __repr__(self): # -> str:
    ...
  
  def __getitem__(self, i):
    ...
  
  def __array__(self, dtype=...): # -> NDArray[Any]:
    ...
  


class TransformedVariable(DeferredTensor):
  """Variable tracking object which applies a bijector upon `convert_to_tensor`.

  #### Example

  ```python
  import tensorflow.compat.v2 as tf
  import tensorflow_probability as tfp
  tfb = tfp.bijectors

  positive_variable = tfp.util.TransformedVariable(1., bijector=tfb.Exp())

  positive_variable
  # ==> <TransformedVariable: dtype=float32, shape=[], fn=exp>

  # Note that the initial value corresponds to the transformed output.
  tf.convert_to_tensor(positive_variable)
  # ==> 1.

  positive_variable.pretransformed_input
  # ==> <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.0>

  # Operators work with `TransformedVariable`.
  positive_variable + 1.
  # ==> 2.

  # It is also possible to assign values to a TransformedVariable
  with tf.control_dependencies([positive_variable.assign_add(2.)]):
    positive_variable
  # ==> 3.

  A common use case for the `TransformedVariable` is to fit constrained
  parameters. E.g.:

  ```python
  import tensorflow.compat.v2 as tf
  import tensorflow_probability as tfp
  tfb = tfp.bijectors
  tfd = tfp.distributions

  trainable_normal = tfd.Normal(
      loc=tf.Variable(0.),
      scale=tfp.util.TransformedVariable(1., bijector=tfb.Exp()))

  trainable_normal.loc
  # ==> <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.0>

  trainable_normal.scale
  # ==> <TransformedVariable: dtype=float32, shape=[], fn=exp>

  with tf.GradientTape() as tape:
    negloglik = -trainable_normal.log_prob(0.5)
  g = tape.gradient(negloglik, trainable_normal.trainable_variables)
  # ==> (-0.5, 0.75)

  opt = tf_keras.optimizers.Adam(learning_rate=0.05)
  loss = tf.function(lambda: -trainable_normal.log_prob(0.5))
  for _ in range(int(1e3)):
    opt.minimize(loss, trainable_normal.trainable_variables)
  trainable_normal.mean()
  # ==> 0.5
  trainable_normal.stddev()
  # ==> (approximately) 0.0075
  ```

  """
  def __init__(self, initial_value, bijector, dtype=..., name=..., **kwargs) -> None:
    """Creates the `TransformedVariable` object.

    Args:
      initial_value: A `Tensor`, or Python object convertible to a `Tensor`,
        which is the initial value for the `TransformedVariable`. The underlying
        untransformed `tf.Variable` will be initialized with
        `bijector.inverse(initial_value)`. Can also be a callable with no
        argument that returns the initial value when called.
      bijector: A `Bijector`-like instance which defines the transformations
        applied to the underlying `tf.Variable`.
      dtype: `tf.dtype.DType` instance or otherwise valid `dtype` value to
        `tf.convert_to_tensor(..., dtype)`.
         Default value: `None` (i.e., `bijector.dtype`).
      name: Python `str` representing the underlying `tf.Variable`'s name.
         Default value: `None`.
      **kwargs: Keyword arguments forward to `tf.Variable`.
    """
    ...
  
  @property
  def bijector(self): # -> Any:
    ...
  
  @property
  def initializer(self):
    """The initializer operation for the underlying variable."""
    ...
  
  @functools.wraps(tf.Variable.assign)
  def assign(self, value, use_locking=..., name=..., read_value=...):
    ...
  
  @functools.wraps(tf.Variable.assign_add)
  def assign_add(self, value, use_locking=..., name=..., read_value=...):
    ...
  
  @functools.wraps(tf.Variable.assign_sub)
  def assign_sub(self, value, use_locking=..., name=..., read_value=...):
    ...
  


class _DeferredTensorTransformedVariableSpecBase:
  """Common methods for '_DeferredTensorSpec' and '_TransformedVariableSpec."""
  @property
  def name(self):
    ...
  
  @property
  def dtype(self):
    ...
  
  @property
  def transform_or_spec(self):
    ...
  
  def is_subtype_of(self, other): # -> bool:
    """Returns True if `self` is subtype of `other`.

    Args:
      other: A `TypeSpec`.
    """
    ...
  
  def most_specific_common_supertype(self, others): # -> Self | None:
    """Returns the most specific supertype of `self` and `others`.

    Args:
      others: A Sequence of `TypeSpec`.

    Returns `None` if a supertype does not exist.
    """
    ...
  
  def is_compatible_with(self, spec_or_value): # -> Literal[False]:
    """Returns True if `spec_or_value` is compatible with this TypeSpec."""
    ...
  
  def __repr__(self): # -> str:
    ...
  
  def __reduce__(self): # -> None:
    ...
  
  def __eq__(self, other) -> bool:
    ...
  
  def __ne__(self, other) -> bool:
    ...
  
  def __hash__(self) -> int:
    ...
  


class _DeferredTensorSpecBase(_DeferredTensorTransformedVariableSpecBase):
  """Base for batchable/non-batchable `tfp.util.DeferredTensor`."""
  def __init__(self, input_spec, transform_or_spec, dtype, shape, name, also_track_spec=...) -> None:
    """Initializes a new `_DeferredTensorSpec`.

    Args:
      input_spec: `tf.TypeSpec` instance describing the `DeferredTensor`s
        `pretransformed_input` attribute.
      transform_or_spec: The `transform_fn` passed to the `DeferredTensor`'s
        constructor, or `transform_fn._type_spec` if `transform_fn` is a
        `CompositeTensor`.
      dtype: `tf.DType`, `dtype` property of the `DeferredTensor`.
      shape: `tf.TensorShape`, `shape` property of the `DeferredTensor`.
      name: `str`, name of the `DeferredTensor`.
      also_track_spec: Python `list` of `VariableSpec` instances describing the
        additional variables tracked by the `DeferredTensor`.
    """
    ...
  
  @property
  def value_type(self): # -> type[DeferredTensor]:
    ...
  
  @property
  def shape(self): # -> Any:
    ...
  


@auto_composite_tensor.type_spec_register('tfp.util.DeferredTensorSpec')
class _DeferredTensorSpec(_DeferredTensorSpecBase, tensor_spec.DenseSpec):
  """`tf.TypeSpec` for `tfp.util.DeferredTensor`."""
  __slots__ = ...


@auto_composite_tensor.type_spec_register('tfp.util.DeferredTensorBatchableSpec')
class _DeferredTensorBatchableSpec(_DeferredTensorSpecBase, type_spec.BatchableTypeSpec):
  """`tf.TypeSpec` for `tfp.util.DeferredTensor`."""
  __slots__ = ...


@auto_composite_tensor.type_spec_register('tfp.util.TransformedVariableSpec')
class _TransformedVariableSpec(_DeferredTensorTransformedVariableSpecBase, tensor_spec.DenseSpec):
  """`tf.TypeSpec` for `tfp.util.TransformedVariable`."""
  __slots__ = ...
  def __init__(self, input_spec, transform_or_spec, dtype, name) -> None:
    """Initializes a new `_TransformedVariableSpec`.

    Args:
      input_spec: `tf.TypeSpec` instance describing the `TransformedVariable`s
        `pretransformed_input` attribute.
      transform_or_spec: The `bijector` passed to the `TransformedVariable`'s
        constructor, or `bijector._type_spec` if `bijector` is a
        `CompositeTensor`.
      dtype: `tf.DType`, `dtype` property of the `TransformedVariable`.
      name: `str`, name of the `TransformedVariable`.
    """
    ...
  
  @property
  def value_type(self): # -> type[TransformedVariable]:
    ...
  


