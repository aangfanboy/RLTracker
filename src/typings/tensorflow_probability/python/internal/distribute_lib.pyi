"""
This type stub file was generated by pyright.
"""

from tensorflow_probability.python.internal import custom_gradient as tfp_custom_gradient

"""Utilities for writing distributed log prob functions."""
JAX_MODE = ...
if JAX_MODE:
  ...
def canonicalize_named_axis(named_axes): # -> list[Any] | list[str | Any]:
  """Converts an input into a list of named axis `str`s."""
  ...

def psum(x, named_axis=...):
  ...

reduce_sum = ...
def pbroadcast(x, named_axis=...):
  ...

def pmean(x, named_axis=...):
  ...

reduce_mean = ...
def pmax(x, named_axis=..., allow_all_gather=...):
  """Generic `pmax` implementation."""
  ...

reduce_max = ...
def pmin(x, named_axis=..., allow_all_gather=...):
  """Generic `pmin` implementation."""
  ...

reduce_min = ...
def reduce_logsumexp(x, axis=..., named_axis=..., allow_all_gather=..., **kwargs):
  """`logsumexp` wrapper."""
  ...

def get_axis_index(axis_name=...):
  ...

def get_axis_size(axis_name=...):
  ...

def fold_in_axis_index(seed, axis_name=...): # -> defaultdict[Any, Any] | Any | list[Any] | None:
  """Folds the active axis index into a seed according to its axis name."""
  ...

@tfp_custom_gradient.custom_gradient(vjp_fwd=_rwb_psum_fwd, vjp_bwd=_rwb_psum_bwd, nondiff_argnums=(1, ))
def rwb_psum(x, axis_name):
  """Applies a psum with reduce-without-broadcast (RWB) semantics.

  RWB semantics allow taking gradients w.r.t. unmapped variables of functions
  with psums in them.

  Args:
    x: a `Tensor` target for the psum.
    axis_name: A string axis name for the psum.

  Returns:
    A `Tensor` that is the result of applying a psum to an input `Tensor`.
  """
  ...

@tfp_custom_gradient.custom_gradient(vjp_fwd=_rwb_pbroadcast_fwd, vjp_bwd=_rwb_pbroadcast_bwd, nondiff_argnums=(1, ))
def rwb_pbroadcast(x, axis_name):
  """Applies a pbroadcast with reduce-without-broadcast (RWB) semantics."""
  ...

def make_pbroadcast_function(fn, in_axes, out_axes, out_dtype): # -> Callable[..., Any]:
  """Constructs a function that broadcasts inputs over named axes.

  Given a function `fn`, `make_pbroadcast_function` returns a new one that
  applies `pbroadcast` to input terms according to axis names provided in
  `in_axes` and `out_axes`. For each output axis in each term out the output of
  `fn`, inputs that do not have the output axes present are pbroadcasted before
  that term is computed.

  Args:
    fn: a callable to be transformed to have proadcasts at its inputs.
    in_axes: A structure of axis names that should match the structure of the
      input to `fn`. If the set of input axes for an input value does not match
      the output axes of a particular output value, the gradient of that output
      value w.r.t. the input value will be psum-ed over the axes present in the
      output but not the input.
    out_axes: A structure of axis names that should match the structure of the
      output of `fn`. The inputs to `fn` will be pbroadcast-ed before computing
      output terms according to their output axes.
    out_dtype: A structure of dtypes that matches the output of `fn`.

  Returns:
    A new function that applies pbroadcasts to the inputs of the original
    function.
  """
  ...

def make_psum_function(fn, in_axes, out_axes, out_dtype): # -> Callable[..., Any | defaultdict[Any, Any] | list[Any] | None]:
  """Constructs a function that broadcasts inputs over named axes.

  Given a function `fn`, `make_psum_function` returns a new one that
  includes psums over terms according to axis names provided in `out_axes`. It
  also adds psums for the vector-Jacobian product of the outputs of `fn` w.r.t.
  its inputs according to `in_axes` if there are axes in the outputs that are
  not present in an input.

  Args:
    fn: a callable to be transformed to have psums at its outputs and on the
      gradients to its inputs.
    in_axes: A structure of axis names that should match the structure of the
      input to `fn`. If the set of input axes for an input value does not match
      the output axes of a particular output value, the gradient of that output
      value w.r.t. the input value will be psum-ed over the axes present in the
      output but not the input.
    out_axes: A structure of axis names that should match the structure of the
      output of `fn`. The outputs of `fn` will be psum-med according to their
      respective output axes.
    out_dtype: A structure of dtypes that matches the output of `fn`.

  Returns:
    A new function that applies psums on to the output of the original
    function and corrects the gradient with respect to its inputs.
  """
  ...

def make_sharded_log_prob_parts(log_prob_parts_fn, axis_names): # -> Callable[..., Any | defaultdict[Any, Any] | list[Any] | None]:
  """Constructs a log prob parts function that all-reduces over terms.

  Given a log_prob_parts function, this function will return a new one that
  includes all-reduce sums over terms according to the `is_sharded` property. It
  will also add all-reduce sums for the gradient of sharded terms w.r.t.
  unsharded terms.

  Args:
    log_prob_parts_fn: a callable that takes in a structured value and returns a
      structure of log densities for each of the terms, that when summed returns
      a locally correct log-density.
    axis_names: a structure of values that matches the input and output of
      `log_prob_parts_fn`. Each value in `axis_names` is either `None, a string
      name of a mapped axis in the JAX backend or any non-`None` value in TF
      backend, or an iterable thereof corresponding to multiple sharding axes.
      If the `axis_name` is not `None`, the returned function will add
      all-reduce sum(s) for its term in the log prob calculation. If it is
      `None`, the returned function will have an all-reduce sum over the
      gradient of sharded terms w.r.t. to the unsharded value.

  Returns:
    A new log prob parts function that can be run inside of a strategy.
  """
  ...

