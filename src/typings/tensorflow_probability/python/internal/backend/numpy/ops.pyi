"""
This type stub file was generated by pyright.
"""

import numpy as np
import six
import wrapt

"""Numpy implementations of TensorFlow functions."""
__all__ = ['bitcast', 'broadcast_dynamic_shape', 'broadcast_static_shape', 'broadcast_to', 'cast', 'clip_by_value', 'constant', 'control_dependencies', 'convert_to_tensor', 'custom_gradient', 'device', 'enable_v2_behavior', 'ensure_shape', 'executing_eagerly', 'get_static_value', 'identity', 'init_scope', 'is_tensor', 'name_scope', 'newaxis', 'recompute_grad', 'register_tensor_conversion_function', 'stop_gradient', 'GradientTape', 'Module', 'SparseTensor', 'Tensor', 'Variable']
JAX_MODE = ...
if JAX_MODE:
  ...
class _NullContext:
  def __init__(self, *args, **kwargs) -> None:
    ...
  
  def __enter__(self): # -> None:
    ...
  
  def __exit__(self, type_arg, value_arg, traceback_arg): # -> Literal[False]:
    ...
  


broadcast_shape = ...
tensor_conversion_registry = ...
def register_tensor_conversion_function(base_type, conversion_func): # -> None:
  ...

class _Int64ToInt32Error(TypeError):
  """Error thrown when trying to convert an int64 to int32."""
  def __init__(self, int_value) -> None:
    ...
  


class _FloatToIntError(TypeError):
  """Error thrown when trying to convert a float to an int."""
  ...


class TypeConversionError(TypeError):
  def __init__(self, value, dtype) -> None:
    ...
  


class MixedTypesError(ValueError):
  def __init__(self) -> None:
    ...
  


class GradientTape:
  """tf.GradientTape stub."""
  def __init__(self, persistent=..., watch_accessed_variables=...) -> None:
    ...
  
  def __enter__(self): # -> Self:
    ...
  
  def __exit__(self, typ, value, traceback): # -> None:
    ...
  
  def watch(self, tensor): # -> None:
    ...
  
  def gradient(self, target, sources, output_gradients=..., unconnected_gradients=...):
    ...
  
  def batch_jacobian(self, target, source, unconnected_gradients=..., parallel_iterations=..., experimental_use_pfor=...):
    ...
  


class SparseTensor:
  """tf.SparseTensor stub."""
  def __init__(self, *args, **kwargs) -> None:
    ...
  


bitcast = ...
broadcast_dynamic_shape = ...
broadcast_static_shape = ...
broadcast_to = ...
cast = ...
clip_by_value = ...
constant = ...
control_dependencies = ...
convert_to_tensor = ...
custom_gradient = ...
device = ...
ensure_shape = ...
executing_eagerly = ...
get_static_value = ...
identity = ...
is_tensor = ...
init_scope = ...
class name_scope:
  """A context manager for use when defining a Python op.

  This context manager pushes a name scope, which will make the name of all
  operations added within it have a prefix.

  For example, to define a new Python op called `my_op`:

  ```python
  def my_op(a, b, c, name=None):
    with tf.name_scope("MyOp") as scope:
      a = tf.convert_to_tensor(a, name="a")
      b = tf.convert_to_tensor(b, name="b")
      c = tf.convert_to_tensor(c, name="c")
      # Define some computation that uses `a`, `b`, and `c`.
      return foo_op(..., name=scope)
  ```

  When executed, the Tensors `a`, `b`, `c`, will have names `MyOp/a`, `MyOp/b`,
  and `MyOp/c`.

  If the scope name already exists, the name will be made unique by appending
  `_n`. For example, calling `my_op` the second time will generate `MyOp_1/a`,
  etc.
  """
  @property
  def name(self): # -> Any:
    ...
  
  def __init__(self, name, *args, **kwargs) -> None:
    ...
  
  def __enter__(self): # -> Any:
    ...
  
  def __exit__(self, type_arg, value_arg, traceback_arg): # -> Literal[False]:
    ...
  


newaxis = ...
if JAX_MODE:
  recompute_grad = ...
else:
  recompute_grad = ...
if JAX_MODE:
  stop_gradient = ...
else:
  stop_gradient = ...
class NumpyVariable(getattr(wrapt, 'ObjectProxy', object)):
  """Stand-in for tf.Variable."""
  __slots__ = ...
  def __init__(self, initial_value=..., trainable=..., validate_shape=..., caching_device=..., name=..., variable_def=..., dtype=..., import_scope=..., constraint=..., shape=...) -> None:
    ...
  
  @property
  def name(self): # -> str:
    ...
  
  @property
  def trainable(self): # -> bool:
    ...
  
  def __array__(self, dtype=...):
    ...
  
  def assign(self, value, **_): # -> Self:
    ...
  
  def assign_add(self, value, **_): # -> Self:
    ...
  
  def assign_sub(self, value, **_): # -> Self:
    ...
  


if JAX_MODE:
  ...
Variable = NumpyVariable
class _TensorMeta(type(np.ndarray)):
  @classmethod
  def __instancecheck__(cls, instance): # -> bool:
    ...
  


class Tensor(six.with_metaclass(_TensorMeta)):
  OVERLOADABLE_OPERATORS = ...


class Module:
  """tf.Module."""
  _TF_MODULE_IGNORED_PROPERTIES = ...
  def __init__(self, name) -> None:
    ...
  
  @property
  def name(self): # -> Any:
    ...
  
  @property
  def trainable_variables(self): # -> list[Any]:
    ...
  
  @property
  def variables(self): # -> list[Any]:
    ...
  


enable_v2_behavior = ...
